{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/louiezzang/next-gpt/blob/main/examples/huggingface_sft_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o5OIi7jDa9K"
      },
      "source": [
        "# Supervised Fine-tuning with huggingface\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TMz3aiLECTk"
      },
      "source": [
        "Build a Supervised Fine-tuning model to answer well to the question.\n",
        "\n",
        "- SFT(Supervised Fine Tuning)\n",
        "- Fine-tune a pretrained LLM on a specific domain or corpus of instructions and human demonstrations\n",
        "\n",
        "- Dataset example\n",
        "```json\n",
        "[\n",
        "    {\n",
        "        \"prompt\": \"\",\n",
        "        \"completion\": \"\"        \n",
        "    }, ...\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YG_5Fvkx9fH0"
      },
      "source": [
        "# Environment setup\n",
        "\n",
        "#### Installation (python>=3.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdbMmQE5Zsjw"
      },
      "outputs": [],
      "source": [
        "# Install next-gpt lib.\n",
        "!rm -rf ./next-gpt/\n",
        "!git clone https://github.com/louiezzang/next-gpt.git\n",
        "%cd next-gpt/\n",
        "!pip install .\n",
        "%cd ../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpoPqaBfAkqW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "import json\n",
        "import yaml\n",
        "import argparse\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "import transformers\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoConfig, AutoModelForCausalLM, pipeline, \n",
        "    TrainingArguments, AutoModelWithLMHead,\n",
        "    ProgressCallback\n",
        ")\n",
        "from nextgpt.dataset import (\n",
        "    SupervisedDataset, DataCollatorForSupervisedDataset\n",
        ")\n",
        "from nextgpt.finetuning import (\n",
        "    SupervisedTrainer, LoggingCallback\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define arguments.\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--model\", type=str, default=\"gpt2\", choices=[\"gpt2\", \"bloom\", \"opt\"])\n",
        "parser.add_argument(\"--max_epochs\", type=int, default=1)\n",
        "parser.add_argument(\"--train_batch_size\", type=int, default=4)\n",
        "parser.add_argument(\"--output_dir\", type=str, default=\"./output_1_sft\")\n",
        "\n",
        "args = parser.parse_args(args=[])\n",
        "print(args)"
      ],
      "metadata": {
        "id": "NLJdNVzEsdJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9M1fZh_A80h"
      },
      "outputs": [],
      "source": [
        "# Get the tokenizer.\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(args.model, \n",
        "                                        #   bos_token=\"<|startoftext|>\",\n",
        "                                        #   eos_token=\"<|endoftext|>\", \n",
        "                                        #   pad_token=\"<|pad|>\"\n",
        "                                          )\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "print(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vji8AkHflB0b"
      },
      "outputs": [],
      "source": [
        "dataset_webgpt_comp = load_dataset(\"openai/webgpt_comparisons\", split=\"train[:20%]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azsnYbQtmQ3j"
      },
      "outputs": [],
      "source": [
        "data_list = []\n",
        "for row in dataset_webgpt_comp:\n",
        "  question = row[\"question\"][\"full_text\"]\n",
        "  answer_0 = row[\"answer_0\"]\n",
        "  data_list.append({\n",
        "      \"instruction\": question,\n",
        "      \"completion\": answer_0\n",
        "  })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4bLuGNRlh9-"
      },
      "outputs": [],
      "source": [
        "PROMPT_TEMPLATE = (\n",
        "  \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "  \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "  \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NhIhWL0lAVp"
      },
      "outputs": [],
      "source": [
        "dataset = SupervisedDataset(\n",
        "    dataset=data_list,\n",
        "    tokenizer=tokenizer, \n",
        "    prompt_template=PROMPT_TEMPLATE,\n",
        "    completion_field=\"completion\",\n",
        "    verbose=True)\n",
        "\n",
        "# Split train and val dataset.\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, eval_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Data collator.\n",
        "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_9-GV97sf9O"
      },
      "outputs": [],
      "source": [
        "# Load the pretrained model.\n",
        "model = AutoModelForCausalLM.from_pretrained(args.model)\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRbskFS7sq84"
      },
      "outputs": [],
      "source": [
        "# Train arguments.\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./checkpoint_1_sft\", # the output directory\n",
        "    overwrite_output_dir=True, # overwrite the content of the output directory\n",
        "    num_train_epochs=args.max_epochs, # number of training epochs\n",
        "    per_device_train_batch_size=args.train_batch_size, # batch size for training\n",
        "    per_device_eval_batch_size=4, # batch size for evaluation\n",
        "    eval_steps=3, # number of update steps between two evaluations.\n",
        "    save_steps=100, # after # steps model is saved \n",
        "    warmup_steps=5, # number of warmup steps for learning rate scheduler\n",
        "    prediction_loss_only=True,\n",
        ")\n",
        "\n",
        "# Train the model.\n",
        "trainer = SupervisedTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=None,\n",
        "    # callbacks=[ProgressCallback, LoggingCallback(logger=None)],\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_state()\n",
        "trainer.safe_save_model(output_dir=args.output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhp5IFF0vQ-m"
      },
      "outputs": [],
      "source": [
        "# Inference test.\n",
        "generator = pipeline(\"text-generation\", model=args.output_dir, tokenizer=tokenizer)\n",
        "\n",
        "generation_args = dict(\n",
        "    num_beams=4,\n",
        "    repetition_penalty=2.0,\n",
        "    no_repeat_ngram_size=4,\n",
        "    # bos_token=\"<|startoftext|>\",\n",
        "    # eos_token=\"<|endoftext|>\", \n",
        "    # pad_token=\"<|pad|>\",\n",
        "    max_new_tokens=64,\n",
        "    do_sample=True,\n",
        "    top_k=30,\n",
        "    top_p=0.95,\n",
        "    temperature=1.9, \n",
        "    #max_length=300, \n",
        "    #num_return_sequences=20\n",
        "    early_stopping=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rnWI0iWwLb1"
      },
      "outputs": [],
      "source": [
        "test_list = data_list[-5:]\n",
        "\n",
        "test_prompt_list = []\n",
        "actual_completion_list = []\n",
        "for row in test_list:\n",
        "    text_input = row\n",
        "    prompt = PROMPT_TEMPLATE.format_map(text_input)\n",
        "    test_prompt_list.append(prompt)\n",
        "    actual_completion_list.append(text_input[\"completion\"])\n",
        "\n",
        "result_list = generator(test_prompt_list, **generation_args)\n",
        "for prompt, result, actual_response in zip(test_prompt_list, result_list, actual_completion_list):\n",
        "    print(\"\")\n",
        "    print(\"-\" * 70)\n",
        "    print((\"completion: %s\" % (result[0][\"generated_text\"])))\n",
        "    print(f\"\\n### Actual answer:\\n{actual_response}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}