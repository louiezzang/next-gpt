{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/louiezzang/next-gpt/blob/main/examples/chatgpt_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o5OIi7jDa9K"
      },
      "source": [
        "# Overview\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TMz3aiLECTk"
      },
      "source": [
        "What is RLHF? <br>\n",
        "See [this link](https://gist.github.com/JoaoLages/c6f2dfd13d2484aa8bb0b2d567fbf093).\n",
        "\n",
        "<br>\n",
        "\n",
        "**Example of RLHF dataset**:\n",
        "\n",
        "Total 3 datasets are needed for training the 3 steps(SFT, RM and PPO)\n",
        "- [Example of dataset](https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/chatllama#dataset-preparation)\n",
        "- [Example of dataset 1](https://huggingface.co/datasets/stanfordnlp/SHP)\n",
        "- [Example of dataset 2](https://huggingface.co/datasets/Anthropic/hh-rlhf)\n",
        "\n",
        "step1) Dataset for SFT(Supervised Fine-tuning training)\n",
        "```json\n",
        "[\n",
        "    {\n",
        "        \"prompt\": \"\",\n",
        "        \"completion\": \"\"        \n",
        "    }, ...\n",
        "]\n",
        "```\n",
        "\n",
        "step2) Dataset for RM(Reward Model) training: There are multiple completetions with human rated ranking score for one prompt.\n",
        "```json\n",
        "[\n",
        "    {\n",
        "        \"prompt\": \"\",\n",
        "        \"completion_1\": \"\",\n",
        "        \"completion_2\": \"\",\n",
        "        \"completion_3\": \"\",            \n",
        "        \"ranking\": [1, 0, 2]\n",
        "    }, ...\n",
        "]\n",
        "```\n",
        "    \n",
        "step3) Dataset for PPO(RLHF) training: It only consists of prompt.\n",
        "```json\n",
        "[\n",
        "    {\n",
        "        \"prompt\": \"\"\n",
        "    }, ...\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YG_5Fvkx9fH0"
      },
      "source": [
        "# Environment setup\n",
        "\n",
        "#### Installation (python>=3.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdbMmQE5Zsjw",
        "outputId": "03b78ef4-05db-4d81-f02e-1266625a75c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'next-gpt'...\n",
            "remote: Enumerating objects: 938, done.\u001b[K\n",
            "remote: Counting objects: 100% (193/193), done.\u001b[K\n",
            "remote: Compressing objects: 100% (111/111), done.\u001b[K\n",
            "Receiving objects: 100% (938/938), 308.15 KiB | 14.01 MiB/s, done.\n",
            "remote: Total 938 (delta 101), reused 124 (delta 61), pack-reused 745\u001b[K\n",
            "Resolving deltas: 100% (542/542), done.\n",
            "/content/next-gpt\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/next-gpt\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nextgpt==0.0.3) (4.65.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (from nextgpt==0.0.3) (4.27.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (from nextgpt==0.0.3) (2.11.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.9/dist-packages (from nextgpt==0.0.3) (0.3.3)\n",
            "Requirement already satisfied: loralib in /usr/local/lib/python3.9/dist-packages (from nextgpt==0.0.3) (0.1.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.9/dist-packages (from nextgpt==0.0.3) (0.0.138)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets->nextgpt==0.0.3) (0.3.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets->nextgpt==0.0.3) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets->nextgpt==0.0.3) (23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from datasets->nextgpt==0.0.3) (0.13.4)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets->nextgpt==0.0.3) (0.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets->nextgpt==0.0.3) (6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets->nextgpt==0.0.3) (3.8.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets->nextgpt==0.0.3) (1.5.3)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets->nextgpt==0.0.3) (0.70.14)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets->nextgpt==0.0.3) (3.2.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets->nextgpt==0.0.3) (2023.3.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets->nextgpt==0.0.3) (9.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets->nextgpt==0.0.3) (2.27.1)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.9/dist-packages (from langchain->nextgpt==0.0.3) (1.2.4)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.9/dist-packages (from langchain->nextgpt==0.0.3) (0.5.7)\n",
            "Requirement already satisfied: SQLAlchemy<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain->nextgpt==0.0.3) (1.4.47)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain->nextgpt==0.0.3) (1.10.7)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.9/dist-packages (from langchain->nextgpt==0.0.3) (8.2.2)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from langchain->nextgpt==0.0.3) (4.0.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.9/dist-packages (from tiktoken->nextgpt==0.0.3) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers->nextgpt==0.0.3) (3.11.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers->nextgpt==0.0.3) (0.13.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->nextgpt==0.0.3) (1.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->nextgpt==0.0.3) (2.0.12)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->nextgpt==0.0.3) (1.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->nextgpt==0.0.3) (22.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->nextgpt==0.0.3) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->nextgpt==0.0.3) (6.0.4)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->nextgpt==0.0.3) (0.8.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->nextgpt==0.0.3) (3.19.0)\n",
            "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->nextgpt==0.0.3) (1.5.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets->nextgpt==0.0.3) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets->nextgpt==0.0.3) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets->nextgpt==0.0.3) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets->nextgpt==0.0.3) (1.26.15)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from SQLAlchemy<2,>=1->langchain->nextgpt==0.0.3) (2.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets->nextgpt==0.0.3) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets->nextgpt==0.0.3) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->nextgpt==0.0.3) (1.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain->nextgpt==0.0.3) (1.0.0)\n",
            "Building wheels for collected packages: nextgpt\n",
            "  Building wheel for nextgpt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nextgpt: filename=nextgpt-0.0.3-py3-none-any.whl size=53890 sha256=a885cb96c92717eca37549c1af2ee8958471957dcd30286c464e3d46f1abf6bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/f4/0d/a2/307ec9a6214260bfd63facee827d7bbef5c1ba9618277ea1b5\n",
            "Successfully built nextgpt\n",
            "Installing collected packages: nextgpt\n",
            "  Attempting uninstall: nextgpt\n",
            "    Found existing installation: nextgpt 0.0.3\n",
            "    Uninstalling nextgpt-0.0.3:\n",
            "      Successfully uninstalled nextgpt-0.0.3\n",
            "Successfully installed nextgpt-0.0.3\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# Install next-gpt lib.\n",
        "!rm -rf ./next-gpt/\n",
        "!git clone https://github.com/louiezzang/next-gpt.git\n",
        "%cd next-gpt/\n",
        "!pip install .\n",
        "%cd ../"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JezJ8wz3_A7B"
      },
      "source": [
        "# Step 1) SFT: Surpervised Fine-tuning\n",
        "Build a Supervised Fine-tuning model to answer well to the question.\n",
        "\n",
        "- Refereneces\n",
        "  - [fine tuning code_1](https://github.com/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb)\n",
        "  - [fine tuning code_2](https://github.com/Beomi/KoAlpaca/blob/main/train.py)\n",
        "\n",
        "\n",
        "- SFT(Supervised Fine Tuning)\n",
        "- Fine-tune a pretrained LLM on a specific domain or corpus of instructions and human demonstrations\n",
        "\n",
        "- Dataset example\n",
        "```json\n",
        "[\n",
        "    {\n",
        "        \"prompt\": \"\",\n",
        "        \"completion\": \"\"        \n",
        "    }, ...\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fpoPqaBfAkqW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "import json\n",
        "import yaml\n",
        "import argparse\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import loralib as lora\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, BloomTokenizerFast\n",
        "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "from nextgpt.dataset import SupervisedDataset, DataCollatorForSupervisedDataset\n",
        "from nextgpt.trainer import SFTTrainer\n",
        "from nextgpt.trainer.strategies import DDPStrategy, NaiveStrategy\n",
        "from nextgpt.models.bloom import BLOOMLM\n",
        "from nextgpt.models.gpt import GPTLM\n",
        "from nextgpt.models.opt import OPTLM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT_TEMPLATE = (\n",
        "  \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "  \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "  \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
        ")"
      ],
      "metadata": {
        "id": "ZhTIRVuOjjNr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLJdNVzEsdJT",
        "outputId": "91e72a42-c9b4-4d7c-c895-70b747fc1eaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(strategy='naive', model='gpt2', pretrain='gpt2', max_datasets_size=10000, need_optim_ckpt=False, max_epochs=3, batch_size=4, max_len=512, lora_rank=0, log_interval=100, lr=5e-06, gradient_accumulation_steps=8, output_dir='./output_1_sft')\n"
          ]
        }
      ],
      "source": [
        "# Define arguments.\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--strategy\",\n",
        "                    choices=[\"naive\", \"ddp\"],\n",
        "                    default=\"naive\")\n",
        "parser.add_argument(\"--model\", choices=[\"gpt2\", \"bloom\", \"opt\"], default=\"gpt2\")\n",
        "parser.add_argument(\"--pretrain\", type=str, default=None)\n",
        "parser.add_argument(\"--max_datasets_size\", type=int, default=None)\n",
        "parser.add_argument(\"--need_optim_ckpt\", type=bool, default=False)\n",
        "parser.add_argument(\"--max_epochs\", type=int, default=3)\n",
        "parser.add_argument(\"--batch_size\", type=int, default=4)\n",
        "parser.add_argument(\"--max_len\", type=int, default=512)\n",
        "parser.add_argument(\"--lora_rank\", type=int, default=0, help=\"low-rank adaptation matrices rank. 0 means LoRA is not applied.\")\n",
        "parser.add_argument(\"--log_interval\", type=int, default=100, help=\"how many steps to log\")\n",
        "parser.add_argument(\"--lr\", type=float, default=5e-6)\n",
        "parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=8)\n",
        "parser.add_argument(\"--output_dir\", type=str, default=\"./output_1_sft\")\n",
        "\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "# For testing.\n",
        "args.pretrain = \"gpt2\"\n",
        "args.max_datasets_size = 10000\n",
        "args.max_epochs = 1\n",
        "\n",
        "print(args)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure strategy.\n",
        "if args.strategy == \"naive\":\n",
        "    strategy = NaiveStrategy()\n",
        "elif args.strategy == \"ddp\":\n",
        "    strategy = DDPStrategy()\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported strategy: {args.strategy}\")\n",
        "\n",
        "# Configure model.\n",
        "with strategy.model_init_context():\n",
        "    if args.model == \"bloom\":\n",
        "        model = BLOOMLM(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
        "    elif args.model == \"opt\":\n",
        "        model = OPTLM(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
        "    elif args.model == \"gpt2\":\n",
        "        model = GPTLM(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported model: {args.model}\")\n",
        "\n",
        "# Configure tokenizer.\n",
        "if args.model == \"gpt2\":\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "elif args.model == \"bloom\":\n",
        "    tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-560m\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "elif args.model == \"opt\":\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")  \n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported model: {args.model}\")"
      ],
      "metadata": {
        "id": "jUPPbygUgtlY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure dataset.\n",
        "dataset_webgpt_comp = load_dataset(\"openai/webgpt_comparisons\", split=\"train[:20%]\")\n",
        "\n",
        "data_list = []\n",
        "for row in dataset_webgpt_comp:\n",
        "    question = row[\"question\"][\"full_text\"]\n",
        "    answer_0 = row[\"answer_0\"]\n",
        "    data_list.append({\n",
        "        \"instruction\": question,\n",
        "        \"completion\": answer_0\n",
        "    })\n",
        "\n",
        "dataset = SupervisedDataset(\n",
        "    dataset=data_list,\n",
        "    tokenizer=tokenizer, \n",
        "    prompt_template=PROMPT_TEMPLATE,\n",
        "    completion_field=\"completion\",\n",
        "    max_datasets_size=args.max_datasets_size,\n",
        "    max_length=args.max_len,\n",
        "    verbose=True)\n",
        "\n",
        "# Split train and eval dataset.\n",
        "train_size = int(0.8 * len(dataset))\n",
        "eval_size = len(dataset) - train_size\n",
        "train_dataset, eval_dataset = torch.utils.data.random_split(dataset, [train_size, eval_size])\n",
        "\n",
        "# Data collator.\n",
        "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62HB_iNujVAj",
        "outputId": "a9821fd4-148f-417b-d877-14e091eee945"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset webgpt_comparisons (/root/.cache/huggingface/datasets/openai___webgpt_comparisons/default/0.0.0/8b5d5879cdc98c4c0099af6053dffe8d504588d43d3b11f1b1ec223ab1e8db0a)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Limiting dataset to 10000 examples.\n",
            "Formatting inputs...\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Voiced by Harry Shearer, what Simpsons character was modeled after Ted Koppel?\n",
            "\n",
            "### Response:\n",
            "The Simpsons character that was possibly based on Ted Koppel is Kent Brockman.  He is a local news anchor in Springfield and is modeled after Ted Koppel. [1]<|endoftext|>\n",
            "Tokenizing inputs... This may take some time...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train!!!\n",
        "trainer = SFTTrainer(model=model,\n",
        "                     strategy=strategy,\n",
        "                     data_collator=data_collator,\n",
        "                     train_dataset=train_dataset,\n",
        "                     eval_dataset=eval_dataset,\n",
        "                     batch_size=args.batch_size,\n",
        "                     max_epochs=args.max_epochs,\n",
        "                     gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
        "                     lr=args.lr)\n",
        "\n",
        "trainer.fit()\n",
        "\n",
        "# Save model checkpoint after fitting on only rank0.\n",
        "trainer.save_model(path=args.output_dir, only_rank0=True, tokenizer=tokenizer)\n",
        "# Save optimizer checkpoint on all ranks.\n",
        "if args.need_optim_ckpt:\n",
        "    strategy.save_optimizer(trainer.optimizer,\n",
        "                            \"sft_optim_checkpoint_%d.pt\" % (torch.cuda.current_device()),\n",
        "                            only_rank0=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VutB4LDdkdRp",
        "outputId": "1fc35093-ab9e-4e37-bc3c-7404c1643300"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "steps: 294it [16:06,  3.29s/it, epoch=2, eval_loss=0.67]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la ./output_1_sft"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1HboVoaCdMi",
        "outputId": "a8c8add5-47ca-4f0e-f8bc-23715c3270d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 499884\n",
            "drwxr-xr-x 2 root root      4096 Apr 10 01:20 .\n",
            "drwxr-xr-x 1 root root      4096 Apr 10 01:20 ..\n",
            "-rw-r--r-- 1 root root       907 Apr 10 01:20 config.json\n",
            "-rw-r--r-- 1 root root       119 Apr 10 01:20 generation_config.json\n",
            "-rw-r--r-- 1 root root    456318 Apr 10 01:20 merges.txt\n",
            "-rw-r--r-- 1 root root 510398013 Apr 10 01:20 pytorch_model.bin\n",
            "-rw-r--r-- 1 root root       470 Apr 10 01:20 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root       722 Apr 10 01:20 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root    999186 Apr 10 01:20 vocab.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8rnWI0iWwLb1",
        "outputId": "15fb67a8-aaed-4d2c-e06e-0d725f3f2ed3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1201: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----------------------------------------------------------------------\n",
            "completion: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "I've noticed when scanning my files for malware/viruses, the \"number of files scanned\" that pops up is almost always greater than the number of files I selected to scan. What is actually being scanned and why is it considered different files?\n",
            "\n",
            "### Response:There are many factors that influence the amount of information that a software program has to send out to a user. The most important is what type of file or directory you have in there. You can download files from any web site, so your computer's internet service provider will usually provide them as well. For example, if\n",
            "\n",
            "### Actual answer:\n",
            "Microsoft Defender Antivirus has multiple layers of protection to catch malware and viruses. These include quick scans, full scans, and on-access protection with cloud-delivered protection [1,2,3]. A quick scan checks the processes, memory, profiles, and certain locations on the device [1]. Real-time protection reviews files when they are opened and closed and whenever a user navigates to a folder [1,2]. On-access protection with cloud-delivered protection helps ensure that all the files accessed on the system are being scanned with the latest security intelligence and cloud machine learning models [3]. A full scan detects malware that was not detected by other scans, but it can take a while and use valuable system resources to complete [3]. It can also take longer to complete if the device is offline for an extended period of time [3].\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "completion: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "How the Obama talk, which has 159,313 upvotes, is not higher all-time than \"Test Post Please Ignore\" with 26,753 upvotes.\n",
            "\n",
            "I'm assuming the difference is that the Reddit system accounts and tries to correct massive upvotes, but why is it being so much harsher to the Obama AMA? Shouldn't it level out over time, not decrease?\n",
            "\n",
            "### Response:The reddit system, which was built in 2009, uses a process known as \"redrawing\", where users post comments on a subreddit and see what others think about them. Users then rank these comments based on the number of replies they've received from those members who agree with them.[1] Redrawing works by\n",
            "\n",
            "### Actual answer:\n",
            "Reddit uses a story algorithm to rank posts. This means that the number of votes and the time a link has been posted have the biggest impact on where a story will rank. Additionally, Reddit also ranks items by the number of votes they accumulate, as well as the age of the post compared to others. [1, 3]\n",
            " sulphReddit also uses a logarithm function in its algorithm. This means that after the first few minutes of the post going live, the initial reactions are crucial to its future survival. If the post instantly achieves upvotes right after posting, more users will be likely to see the post. If it happens later, then the relevance of the post is diminished, so reactions hold weight. [2, 3] This means that newer posts generally rank higher than older posts. This keeps the front page fresh, and ensures that links with thousands of up-votes aren’t stuck on the front page for weeks or months at a time. Stories that get a more equal range of up-votes and down-votes will generally be ranked lower. [3]\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "completion: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "if micro- and macro-physics equations are incompatible, at roughly what size does each start being descriptive? Is there overlap, and if not, are there \"in-between\" sizes that could be studied?\n",
            "\n",
            "Also, aside from string theory, what efforts are currently being made to bridge the gap?\n",
            "\n",
            "### Response:There are several theories on the origins of cosmological constants [1]. Some say that the universe itself began out of thin air [2]. Others suggest that this was the place where the origin of light came into existence [3]. These theories have been widely debated since the 1980s [4]. One possibility is that\n",
            "\n",
            "### Actual answer:\n",
            "The laws of physics are considered fundamental, although many of them refer to idealized or theoretical systems that are hard to replicate in the real world [3]. The behavior of particles in the micro-physical world is so unpredictable from the known laws that operate in the macro-physical world that Niels Bohr reputedly said, “Anyone who is not shocked by quantum mechanics has not really understood it” [1]. Schrodinger – a pioneer in quantum mechanics - reputedly said, “I don’t like it and I wish I had had nothing to do with it” [1].\n",
            "\n",
            "String theory and quantum gravitation are attempts to reconcile the two – among them string theory and quantum gravitation [2]. In fact, the marriage of quantum mechanics – the physics of the very small – with general relativity – the physics of the very large – is believed by some to be the crucial step in formulating a general ‘theory of everything’ that will hopefully contain all the basic laws of nature in one package [5]. Quantum mechanics has had extraordinary successes in explaining everything from the state of the universe immediately after the big bang, to the structure of DNA, to the colour of your socks [6].\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "completion: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "- You know how you can flick a switch on a rearview mirror and it reduces glare while still being reflective? How does that work?\n",
            "\n",
            "### Response:The lens of the front f/1.8 aperture produces a very wide angle of view compared to its equivalent focal length of 2.4 inches. When viewed at infinity, this narrows the field of view available for each image by approximately 50%. This effect is best seen in bright sunlight, when the sun is about\n",
            "\n",
            "### Actual answer:\n",
            "Rearview mirrors have two reflective surfaces, a regular mirror on the back, and a glass wedge in front of it that reflects only about four percent of the incoming light [1]. When you flip the switch on the bottom of the mirror, the glass wedge moves, changing the way light passes through it and how it's reflected [2][3]. In daytime driving mode, the back surface of the mirror reflects light and images [3]. When you flip the switch and change the orientation of the mirror glass, the front section is responsible for what you see [3]. Because the light and images must first travel through the back side of the glass before hitting the front and bouncing back to you, the image is dimmer and the glare of headlights behind you is greatly reduced [3][4].\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "completion: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "How does negaive mass work in physics?\n",
            "\n",
            "I saw the article below and wondered how it could be possible to have a negative mass. \n",
            "\n",
            "https://www.reddit.com/r/space/comments/a3a33c/scientists_may_have_solved_one_of_the_biggest/?utm_source=reddit-android\n",
            "\n",
            "How will it behave in a classical situation? How would gravity interact with a negative mass? \n",
            "\n",
            "### Response:A negative mass of zero means you can't actually move much at all. But if you go through the motions, the gravitational force pulling you towards the destination causes you to float on the ground, thus moving faster than you should. There are three ways to describe this phenomenon: Gravity, inertia, and electromagnetism\n",
            "\n",
            "### Actual answer:\n",
            "In theoretical physics, negative mass is a type of matter whose mass is of opposite sign to the mass of normal matter [1]. The mass of normal matter is defined as positive mass [1] so therefore negative mass is mass that is actually negative. To create negative mass, the research team at Washington State University cooled rubidium atoms to just a hair above absolute zero, creating what is known as a Bose-Einstein condensate [3]. In this state, particles move extremely slowly and, following the principles of quantum mechanics, behave like waves [3]. They also synchronize and move in unison as what is known as a superfluid, which flows without losing energy [3]. To create the negative mass, the researchers applied a second set of lasers that kicked the atoms back and forth and changed the way they spin [3].\n"
          ]
        }
      ],
      "source": [
        "# Inference test.\n",
        "generator = pipeline(\"text-generation\", model=args.output_dir, tokenizer=tokenizer)\n",
        "\n",
        "generation_args = dict(\n",
        "    num_beams=4,\n",
        "    repetition_penalty=2.0,\n",
        "    no_repeat_ngram_size=4,\n",
        "    max_new_tokens=64,\n",
        "    do_sample=True,\n",
        "    top_k=30,\n",
        "    top_p=0.95,\n",
        "    temperature=1.9, \n",
        "    #max_length=300, \n",
        "    #num_return_sequences=20\n",
        "    early_stopping=True,\n",
        ")\n",
        "\n",
        "test_list = data_list[-5:]\n",
        "\n",
        "test_prompt_list = []\n",
        "actual_completion_list = []\n",
        "for row in test_list:\n",
        "    text_input = row\n",
        "    prompt = PROMPT_TEMPLATE.format_map(text_input)\n",
        "    test_prompt_list.append(prompt)\n",
        "    actual_completion_list.append(text_input[\"completion\"])\n",
        "\n",
        "result_list = generator(test_prompt_list, **generation_args)\n",
        "for prompt, result, actual_response in zip(test_prompt_list, result_list, actual_completion_list):\n",
        "    print(\"\")\n",
        "    print(\"-\" * 70)\n",
        "    print((\"completion: %s\" % (result[0][\"generated_text\"])))\n",
        "    print(f\"\\n### Actual answer:\\n{actual_response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JoJpv-wAvY6"
      },
      "source": [
        "# Step 2) RM: Reward Model\n",
        "Train Reward Model to generate the better answer by giving a reward to the better answer.\n",
        "- Dataset example\n",
        "```json\n",
        "[\n",
        "    {\n",
        "        \"prompt\": \"\",\n",
        "        \"completion_1\": \"\",\n",
        "        \"completion_2\": \"\",\n",
        "        \"completion_3\": \"\",            \n",
        "        \"ranking\": [1, 0, 2]\n",
        "    }, ...\n",
        "]\n",
        "```\n",
        "- Dataset sources\n",
        "  - [Dahoas/rm-static](https://huggingface.co/datasets/Dahoas/rm-static)\n",
        "  - [openai/webgpt_comparisons](https://huggingface.co/datasets/openai/webgpt_comparisons)\n",
        "  - [openai/summarize_from_feedback](https://huggingface.co/datasets/openai/summarize_from_feedback)\n",
        "  - [Dahoas/instruct-synthetic-prompt-responses](https://huggingface.co/datasets/Dahoas/synthetic-instruct-gptj-pairwise)\n",
        "\n",
        "- References\n",
        "    - [train_reward_model.py](https://github.com/hpcaitech/ColossalAI/blob/main/applications/Chat/examples/train_reward_model.py)\n",
        "    - [train_prompts.py](https://github.com/hpcaitech/ColossalAI/blob/main/applications/Chat/examples/train_prompts.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esYui6hPCuF-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, BloomTokenizerFast\n",
        "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
        "import loralib as lora\n",
        "\n",
        "from nextgpt.dataset import RewardDataset\n",
        "from nextgpt.models.base import RewardModel\n",
        "from nextgpt.models.bloom import BLOOMRM\n",
        "from nextgpt.models.gpt import GPTRM\n",
        "from nextgpt.models.opt import OPTRM\n",
        "from nextgpt.trainer import RewardModelTrainer\n",
        "from nextgpt.trainer.strategies import DDPStrategy, NaiveStrategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGnVyFvLSAjR",
        "outputId": "956acd20-1bb3-42f6-8665-33341117072a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(output_dir='./output_2_rm', strategy='naive', model='gpt2', pretrain='gpt2', model_path=None, need_optim_ckpt=False, max_epochs=3, batch_size=4, lora_rank=0, loss_fn='log_sig', max_len=512, verbose=True)\n"
          ]
        }
      ],
      "source": [
        "# Define arguments.\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--output_dir\", type=str, default=\"./output_2_rm\")\n",
        "parser.add_argument(\"--strategy\",\n",
        "                    type=str, \n",
        "                    choices=[\"naive\", \"ddp\"],\n",
        "                    default=\"naive\")\n",
        "parser.add_argument(\"--model\", \n",
        "                    type=str, \n",
        "                    choices=[\"gpt2\", \"bloom\", \"opt\"], \n",
        "                    default=\"gpt2\")\n",
        "parser.add_argument(\"--pretrain\", type=str, default=\"gpt2\")\n",
        "parser.add_argument(\"--model_path\", type=str, default=None)\n",
        "parser.add_argument(\"--need_optim_ckpt\", type=bool, default=False)\n",
        "parser.add_argument(\"--max_epochs\", type=int, default=10)\n",
        "parser.add_argument(\"--batch_size\", type=int, default=4)\n",
        "parser.add_argument(\"--lora_rank\", type=int, default=0, help=\"low-rank adaptation matrices rank\")\n",
        "parser.add_argument(\"--loss_fn\", \n",
        "                    type=str, \n",
        "                    choices=[\"log_sig\", \"log_exp\"],\n",
        "                    default=\"log_sig\")\n",
        "parser.add_argument(\"--lr\", type=float, default=5e-5)\n",
        "parser.add_argument(\"--max_len\", type=int, default=512)\n",
        "\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "# For testing.\n",
        "args.max_epochs = 3\n",
        "args.pretrain = \"gpt2\" # pretrained initial model.\n",
        "args.verbose = True\n",
        "\n",
        "print(args)\n",
        "if not os.path.exists(args.output_dir):\n",
        "    os.makedirs(args.output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHEdwfvCwZTi"
      },
      "outputs": [],
      "source": [
        "# Configure strategy.\n",
        "if args.strategy == \"naive\":\n",
        "    strategy = NaiveStrategy()\n",
        "elif args.strategy == \"ddp\":\n",
        "    strategy = DDPStrategy()\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported strategy: {args.strategy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "b3c0f8e79f5e49ee89ca7814ed67867d",
            "3766fafa089c4969a39e6137a8dfaef7",
            "76ed9ac281114e4894690cb459450d5b",
            "ca52f099b62f476fa97a69f01eed54f9",
            "6a99bea8b0bd42af9e329e5681a04e86",
            "89b3b0d550da4f6ea2739ccd82eaaba6",
            "da419feb0219474b9d4ab528a367f3d7",
            "2d64734e05c14085975cfb0afe0e7129",
            "9810ead6985147b184a04a4c9b458011",
            "7f3a6bd610ad4092b0de5b0519d6c7ad",
            "8b2c028b27bd4b599b7670cdbd7ad235"
          ]
        },
        "id": "jnR1YKD2Zuyo",
        "outputId": "f8cd9877-163e-48ea-c3eb-55522f9fb735"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b3c0f8e79f5e49ee89ca7814ed67867d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})\n"
          ]
        }
      ],
      "source": [
        "# Configure model.\n",
        "with strategy.model_init_context():\n",
        "    if args.model == \"gpt2\":\n",
        "        model = GPTRM(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
        "    elif args.model == \"bloom\":\n",
        "        model = BLOOMRM(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
        "    elif args.model == \"opt\":\n",
        "        model = OPTRM(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device()) \n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported model: {args.model}\")\n",
        "\n",
        "    # Load the supervised finetuning model state dict if it is specified.\n",
        "    # However, we will train the reward model from the initial language model instead of supervised finetuning model.\n",
        "    if args.model_path is not None:\n",
        "        state_dict = torch.load(args.model_path)\n",
        "        model.model.load_state_dict(state_dict)\n",
        "\n",
        "# This float16 or `model.half()` might cause loss NaN issue!!!\n",
        "# See:\n",
        "#   https://stackoverflow.com/questions/65332165/loss-is-nan-when-fine-tuning-huggingface-nli-model-both-roberta-bart\n",
        "#   https://github.com/huggingface/transformers/issues/9160\n",
        "# model = model.to(torch.float16)\n",
        "\n",
        "# Configure tokenizer.\n",
        "if args.model == \"gpt2\":\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        \"gpt2\", \n",
        "        # bos_token=\"<|startoftext|>\",\n",
        "        # eos_token=\"<|endoftext|>\", \n",
        "        # pad_token=\"<|pad|>\",\n",
        "        # padding_side=\"right\", \n",
        "        model_max_length=args.max_len,\n",
        "        )\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(tokenizer)\n",
        "    # model.resize_token_embeddings(len(tokenizer)) \n",
        "elif args.model == \"bloom\":\n",
        "    tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-560m\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "elif args.model == \"opt\":\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")  \n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported model: {args.model}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQU-3QqH65FE",
        "outputId": "7facce11-f7de-40d6-cfc3-4ac66b7760f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset webgpt_comparisons (/root/.cache/huggingface/datasets/openai___webgpt_comparisons/default/0.0.0/8b5d5879cdc98c4c0099af6053dffe8d504588d43d3b11f1b1ec223ab1e8db0a)\n"
          ]
        }
      ],
      "source": [
        "# Get the dataset.\n",
        "dataset_webgpt_comp = load_dataset(\"openai/webgpt_comparisons\", split=\"train[:20%]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2H18pYX7akp",
        "outputId": "3813a100-02c0-4ddd-f38a-8c847b2bc2b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'prompt': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nVoiced by Harry Shearer, what Simpsons character was modeled after Ted Koppel?\\n\\n### Response:',\n",
              "  'completion_0': 'The Simpsons character that was possibly based on Ted Koppel is Kent Brockman.  He is a local news anchor in Springfield and is modeled after Ted Koppel. [1]',\n",
              "  'completion_1': \"Apu Nahasapeemapetilon is a recurring character in the American animated television series The Simpsons. He is an Indian immigrant proprietor who runs the Kwik-E-Mart, a popular convenience store in Springfield. [1] He was based on Peter Seller's character in the film The Party. [2]\",\n",
              "  'ranking': [0, 1]},\n",
              " {'prompt': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nHeterophobia is the irrational fear of what\\n\\n### Response:',\n",
              "  'completion_0': ' Heterophobia is the irrational fear of the opposite sex, coined as Sexophobia [1]. This phobia can be caused by genetics, heredity, negative experiences with the opposite sex, or a combination of these [1].  Symptoms may result from encountering people of the opposite sex, including breathlessness, dizziness, excessive sweating, nausea, dry mouth, feeling sick, shaking, coronary heart palpitations, and anxiety [1].',\n",
              "  'completion_1': 'In modern times, there has been a rise in what is called heterophobia; the irrational fear of, discrimination against, or aversion to heterosexual people. [1][2] The word \"heterophobia\" is a play on the word \"homophobia,\" which describes the fear of homosexual people. [1] Like homophobia, heterophobia is promoted by those who wish to shame or bash heterosexuals, especially men who have sex with women. [2]',\n",
              "  'ranking': [0, 1]}]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# Convert data into ranking format.\n",
        "data_list_ranking = []\n",
        "for row in dataset_webgpt_comp:\n",
        "    question = row[\"question\"][\"full_text\"]\n",
        "    answer_0 = row[\"answer_0\"]\n",
        "    answer_1 = row[\"answer_1\"]\n",
        "    score_0 = row[\"score_0\"]\n",
        "    score_1 = row[\"score_1\"]\n",
        "    if answer_0 == \"\" or answer_1 == \"\" or (score_0 == score_1):\n",
        "        continue\n",
        "\n",
        "    ranking = [0 if score_0 > score_1 else 1, 0 if score_0 < score_1 else 1]\n",
        "    data_list_ranking.append({\n",
        "        \"prompt\": PROMPT_TEMPLATE.format_map({\"instruction\": question}),\n",
        "        \"completion_0\": answer_0,\n",
        "        \"completion_1\": answer_1,\n",
        "        \"ranking\": ranking\n",
        "    })\n",
        "\n",
        "data_list_ranking[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeTW6fIE_c2L",
        "outputId": "f32f47ca-536b-4855-af32-abdfcc528658"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before data num: 2747\n",
            "after data num: 2747\n",
            "data example: \n",
            "{'prompt': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nHeterophobia is the irrational fear of what\\n\\n### Response:', 'chosen': ' Heterophobia is the irrational fear of the opposite sex, coined as Sexophobia [1]. This phobia can be caused by genetics, heredity, negative experiences with the opposite sex, or a combination of these [1].  Symptoms may result from encountering people of the opposite sex, including breathlessness, dizziness, excessive sweating, nausea, dry mouth, feeling sick, shaking, coronary heart palpitations, and anxiety [1].', 'rejected': 'In modern times, there has been a rise in what is called heterophobia; the irrational fear of, discrimination against, or aversion to heterosexual people. [1][2] The word \"heterophobia\" is a play on the word \"homophobia,\" which describes the fear of homosexual people. [1] Like homophobia, heterophobia is promoted by those who wish to shame or bash heterosexuals, especially men who have sex with women. [2]'}\n"
          ]
        }
      ],
      "source": [
        "# Make ranking data to chosen, rejetced data for reward model dataset.\n",
        "total_data_ranking2chosen = []\n",
        "for tmp in data_list_ranking:\n",
        "    one_data_ranking2chosen = []\n",
        "\n",
        "    # data 1) 0 VS 1\n",
        "    data = {}\n",
        "    data[\"prompt\"] = tmp[\"prompt\"]\n",
        "    if tmp[\"ranking\"][0] < tmp[\"ranking\"][1]:\n",
        "        data[\"chosen\"] = tmp[\"completion_0\"]\n",
        "        data[\"rejected\"] = tmp[\"completion_1\"]\n",
        "    else:\n",
        "        data[\"chosen\"] = tmp[\"completion_1\"]\n",
        "        data[\"rejected\"] = tmp[\"completion_0\"]\n",
        "    one_data_ranking2chosen.append(data)\n",
        "\n",
        "    # # data 2) 0 VS 2\n",
        "    # data = {}\n",
        "    # data[\"prompt\"] = tmp[\"prompt\"]\n",
        "    # if tmp[\"ranking\"][0] < tmp[\"ranking\"][2]:\n",
        "    #     data[\"chosen\"] = tmp[\"completion_0\"]\n",
        "    #     data[\"rejected\"] = tmp[\"completion_2\"]\n",
        "    # else:\n",
        "    #     data[\"chosen\"] = tmp[\"completion_2\"]\n",
        "    #     data[\"rejected\"] = tmp[\"completion_0\"]\n",
        "    # one_data_ranking2chosen.append(data)\n",
        "\n",
        "    # # data 1) 1 VS 2\n",
        "    # data = {}\n",
        "    # data[\"prompt\"] = tmp[\"prompt\"]\n",
        "    # if tmp[\"ranking\"][1] < tmp[\"ranking\"][2]:\n",
        "    #     data[\"chosen\"] = tmp[\"completion_1\"]\n",
        "    #     data[\"rejected\"] = tmp[\"completion_2\"]\n",
        "    # else:\n",
        "    #     data[\"chosen\"] = tmp[\"completion_2\"]\n",
        "    #     data[\"rejected\"] = tmp[\"completion_1\"]\n",
        "    # one_data_ranking2chosen.append(data)\n",
        "\n",
        "\n",
        "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
        "\n",
        "\n",
        "print(\"before data num: %d\" % (len(data_list_ranking)))\n",
        "print(\"after data num: %d\" % (len(total_data_ranking2chosen)))\n",
        "print(\"data example: \\n%s\" % total_data_ranking2chosen[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1FpHo5jAf81",
        "outputId": "47f1f149-01da-480d-e15a-4faf7df6ef5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prompt': \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nHow does one juice a prune? If prunes are just dehydrated plums, shouldn't prune juice just be plum juice?\\n\\n### Response:\", 'chosen': 'You can juice dried prunes by steaming or simmering them to rehydrate them, running them through a strainer to remove the pits, seeds and skin, and then adding more water to the resulting pruney paste. [1] You don’t have to do that, though, because you could also just juice a fresh prune. Contrary to popular belief, prunes aren’t simply dried plums, but a group of cultivars, or varieties, of plum that are well suited to drying. [2][3] ', 'rejected': 'While prunes are not simply dried plums, they are a type of dried plum. [2][3]  To juice a prune, you must first steam or simmer them to rehydrate them, and then run them through a strainer to remove the pits, seeds, and skin. [2][3]'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 451.13it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 520.23it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 508.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "######################################################################\n",
            "## prompt ##\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Why do major cell phone carriers allow companies like MetroPCS, Cricket Wireless, Boost Mobile, etc. to resell their network?\n",
            "\n",
            "And for a cheaper price, too? I don't understand.\n",
            "\n",
            "### Response:\n",
            "######################################################################\n",
            "## chosen ##\n",
            "These companies known as mobile virtual network operators or MVNOs are able to resell network services in bulk from a regular carrier and then resell them to end-users, usually for cheaper prices than that carrier [2,3]. That’s still profitable for MVNOs because they don’t have to pay anything for the upkeep and modernization of the wireless network they’re using, therefore they can afford to lower the rates on voice calls, messages and data in order to attract more customers [3]. Usually, they usually offer quite affordable pre-pay rates for these services and that’s definitely worth remembering [2]. As for regular carriers, without getting into too much detail, they’re still happy to make a profit from selling their services to MVNOs, even though they’re basically creating competitors with more customer-friendly offers – in some markets, governments require carriers to support MVNOs in order to create a competitive environment in the local mobile business [3]. \n",
            "######################################################################\n",
            "## rejected ##\n",
            "The big cell phone carriers allow smaller companies, called MVNOs (Mobile Virtual Network Operators), to resell their network [3]. They don't own the cell phone towers or the network but instead lease the network for their customers to use [3]. They are able to offer cheaper cell phone plans because they don't have the huge overhead and advertising costs that the big guys do [2]. They also have better customer service because they are smaller and less complex [1].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Prepare for data and dataset.\n",
        "import random\n",
        "random.seed(230319)\n",
        "\n",
        "random.shuffle(total_data_ranking2chosen)\n",
        "print(total_data_ranking2chosen[1])\n",
        "\n",
        "# train_data = total_data_ranking2chosen[:-1000]\n",
        "# eval_data = total_data_ranking2chosen[-1000:0]\n",
        "# We just select very small set of data for a quicker training.\n",
        "train_data = total_data_ranking2chosen[:100]\n",
        "val_data = total_data_ranking2chosen[100:130]\n",
        "eval_data = total_data_ranking2chosen[130:160]\n",
        "\n",
        "train_dataset = RewardDataset(train_data, tokenizer, args.max_len)\n",
        "val_dataset = RewardDataset(val_data, tokenizer, args.max_len)\n",
        "eval_dataset = RewardDataset(eval_data, tokenizer, args.max_len)\n",
        "\n",
        "# Check\n",
        "idx = 10\n",
        "print(\"#\" * 70)\n",
        "print(\"## prompt ##\")\n",
        "print(train_data[idx][\"prompt\"])\n",
        "print(\"#\" * 70)\n",
        "print(\"## chosen ##\")\n",
        "print(train_data[idx][\"chosen\"])\n",
        "print(\"#\" * 70)\n",
        "print(\"## rejected ##\")\n",
        "print(train_data[idx][\"rejected\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tC6Sf3qMX2A-"
      },
      "outputs": [],
      "source": [
        "trainer = RewardModelTrainer(model=model,\n",
        "                             strategy=strategy,\n",
        "                             train_dataset=train_dataset,\n",
        "                             valid_dataset=val_dataset,\n",
        "                             eval_dataset=eval_dataset,\n",
        "                             batch_size=args.batch_size,\n",
        "                             max_epochs=args.max_epochs,\n",
        "                             loss_fn=args.loss_fn,\n",
        "                             lr=args.lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJO46OBraGOy",
        "outputId": "b515f2ae-0336-4c7a-f83b-a36caacd6a05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Train step of epoch 0:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Train step of epoch 0:   4%|▍         | 1/25 [00:00<00:08,  2.97it/s]\u001b[A\n",
            "Train step of epoch 0:   4%|▍         | 1/25 [00:00<00:08,  2.97it/s, loss=0.705, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:   8%|▊         | 2/25 [00:00<00:06,  3.83it/s, loss=0.705, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:   8%|▊         | 2/25 [00:00<00:06,  3.83it/s, loss=0.689, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  12%|█▏        | 3/25 [00:00<00:05,  4.12it/s, loss=0.689, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  12%|█▏        | 3/25 [00:00<00:05,  4.12it/s, loss=0.696, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  16%|█▌        | 4/25 [00:00<00:04,  4.30it/s, loss=0.696, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  16%|█▌        | 4/25 [00:01<00:04,  4.30it/s, loss=0.689, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  20%|██        | 5/25 [00:01<00:04,  4.40it/s, loss=0.689, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  20%|██        | 5/25 [00:01<00:04,  4.40it/s, loss=0.696, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  24%|██▍       | 6/25 [00:01<00:04,  4.45it/s, loss=0.696, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  24%|██▍       | 6/25 [00:01<00:04,  4.45it/s, loss=0.69, dist=0, acc=0] \u001b[A\n",
            "Train step of epoch 0:  28%|██▊       | 7/25 [00:01<00:04,  4.50it/s, loss=0.69, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  28%|██▊       | 7/25 [00:01<00:04,  4.50it/s, loss=0.695, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  32%|███▏      | 8/25 [00:01<00:03,  4.52it/s, loss=0.695, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  32%|███▏      | 8/25 [00:01<00:03,  4.52it/s, loss=0.693, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  36%|███▌      | 9/25 [00:02<00:03,  4.54it/s, loss=0.693, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  36%|███▌      | 9/25 [00:02<00:03,  4.54it/s, loss=0.696, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  40%|████      | 10/25 [00:02<00:03,  4.56it/s, loss=0.696, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  40%|████      | 10/25 [00:02<00:03,  4.56it/s, loss=0.697, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  44%|████▍     | 11/25 [00:02<00:03,  4.57it/s, loss=0.697, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  44%|████▍     | 11/25 [00:02<00:03,  4.57it/s, loss=0.688, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  48%|████▊     | 12/25 [00:02<00:02,  4.57it/s, loss=0.688, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  48%|████▊     | 12/25 [00:02<00:02,  4.57it/s, loss=0.692, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  52%|█████▏    | 13/25 [00:02<00:02,  4.57it/s, loss=0.692, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  52%|█████▏    | 13/25 [00:03<00:02,  4.57it/s, loss=0.69, dist=0, acc=0] \u001b[A\n",
            "Train step of epoch 0:  56%|█████▌    | 14/25 [00:03<00:02,  4.57it/s, loss=0.69, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  56%|█████▌    | 14/25 [00:03<00:02,  4.57it/s, loss=0.694, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  60%|██████    | 15/25 [00:03<00:02,  4.56it/s, loss=0.694, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  60%|██████    | 15/25 [00:03<00:02,  4.56it/s, loss=0.691, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  64%|██████▍   | 16/25 [00:03<00:01,  4.56it/s, loss=0.691, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  64%|██████▍   | 16/25 [00:03<00:01,  4.56it/s, loss=0.689, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  68%|██████▊   | 17/25 [00:03<00:01,  4.53it/s, loss=0.689, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  68%|██████▊   | 17/25 [00:03<00:01,  4.53it/s, loss=0.696, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  72%|███████▏  | 18/25 [00:04<00:01,  4.53it/s, loss=0.696, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  72%|███████▏  | 18/25 [00:04<00:01,  4.53it/s, loss=0.696, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  76%|███████▌  | 19/25 [00:04<00:01,  4.54it/s, loss=0.696, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  76%|███████▌  | 19/25 [00:04<00:01,  4.54it/s, loss=0.705, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  80%|████████  | 20/25 [00:04<00:01,  4.56it/s, loss=0.705, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  80%|████████  | 20/25 [00:04<00:01,  4.56it/s, loss=0.696, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  84%|████████▍ | 21/25 [00:04<00:00,  4.55it/s, loss=0.696, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  84%|████████▍ | 21/25 [00:04<00:00,  4.55it/s, loss=0.689, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  88%|████████▊ | 22/25 [00:04<00:00,  4.56it/s, loss=0.689, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  88%|████████▊ | 22/25 [00:05<00:00,  4.56it/s, loss=0.698, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  92%|█████████▏| 23/25 [00:05<00:00,  4.57it/s, loss=0.698, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  92%|█████████▏| 23/25 [00:05<00:00,  4.57it/s, loss=0.693, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  96%|█████████▌| 24/25 [00:05<00:00,  4.56it/s, loss=0.693, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  96%|█████████▌| 24/25 [00:05<00:00,  4.56it/s, loss=0.696, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0: 100%|██████████| 25/25 [00:05<00:00,  4.57it/s, loss=0.696, dist=0, acc=0]\u001b[A\n",
            "Train epoch:  33%|███▎      | 1/3 [00:06<00:12,  6.22s/it]\n",
            "Train step of epoch 0: 100%|██████████| 25/25 [00:06<00:00,  4.02it/s, dist=0.0051, acc=0.633]\n",
            "\n",
            "Train step of epoch 1:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Train step of epoch 1:   4%|▍         | 1/25 [00:00<00:03,  7.93it/s]\u001b[A\n",
            "Train step of epoch 1:   4%|▍         | 1/25 [00:00<00:03,  7.93it/s, loss=0.691, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:   8%|▊         | 2/25 [00:00<00:04,  5.52it/s, loss=0.691, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:   8%|▊         | 2/25 [00:00<00:04,  5.52it/s, loss=0.692, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  12%|█▏        | 3/25 [00:00<00:04,  5.07it/s, loss=0.692, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  12%|█▏        | 3/25 [00:00<00:04,  5.07it/s, loss=0.691, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  16%|█▌        | 4/25 [00:00<00:04,  4.88it/s, loss=0.691, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  16%|█▌        | 4/25 [00:00<00:04,  4.88it/s, loss=0.684, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  20%|██        | 5/25 [00:00<00:04,  4.75it/s, loss=0.684, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  20%|██        | 5/25 [00:01<00:04,  4.75it/s, loss=0.685, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  24%|██▍       | 6/25 [00:01<00:04,  4.70it/s, loss=0.685, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  24%|██▍       | 6/25 [00:01<00:04,  4.70it/s, loss=0.689, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  28%|██▊       | 7/25 [00:01<00:03,  4.67it/s, loss=0.689, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  28%|██▊       | 7/25 [00:01<00:03,  4.67it/s, loss=0.684, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  32%|███▏      | 8/25 [00:01<00:03,  4.63it/s, loss=0.684, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  32%|███▏      | 8/25 [00:01<00:03,  4.63it/s, loss=0.681, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  36%|███▌      | 9/25 [00:01<00:03,  4.62it/s, loss=0.681, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  36%|███▌      | 9/25 [00:01<00:03,  4.62it/s, loss=0.682, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  40%|████      | 10/25 [00:02<00:03,  4.60it/s, loss=0.682, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  40%|████      | 10/25 [00:02<00:03,  4.60it/s, loss=0.683, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  44%|████▍     | 11/25 [00:02<00:03,  4.59it/s, loss=0.683, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  44%|████▍     | 11/25 [00:02<00:03,  4.59it/s, loss=0.702, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  48%|████▊     | 12/25 [00:02<00:02,  4.59it/s, loss=0.702, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  48%|████▊     | 12/25 [00:02<00:02,  4.59it/s, loss=0.677, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  52%|█████▏    | 13/25 [00:02<00:02,  4.60it/s, loss=0.677, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  52%|█████▏    | 13/25 [00:02<00:02,  4.60it/s, loss=0.658, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  56%|█████▌    | 14/25 [00:02<00:02,  4.60it/s, loss=0.658, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  56%|█████▌    | 14/25 [00:03<00:02,  4.60it/s, loss=0.675, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  60%|██████    | 15/25 [00:03<00:02,  4.59it/s, loss=0.675, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  60%|██████    | 15/25 [00:03<00:02,  4.59it/s, loss=0.697, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  64%|██████▍   | 16/25 [00:03<00:01,  4.59it/s, loss=0.697, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  64%|██████▍   | 16/25 [00:03<00:01,  4.59it/s, loss=0.66, dist=0, acc=0] \u001b[A\n",
            "Train step of epoch 1:  68%|██████▊   | 17/25 [00:03<00:01,  4.59it/s, loss=0.66, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  68%|██████▊   | 17/25 [00:03<00:01,  4.59it/s, loss=0.655, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  72%|███████▏  | 18/25 [00:03<00:01,  4.58it/s, loss=0.655, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  72%|███████▏  | 18/25 [00:03<00:01,  4.58it/s, loss=0.685, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  76%|███████▌  | 19/25 [00:04<00:01,  4.59it/s, loss=0.685, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  76%|███████▌  | 19/25 [00:04<00:01,  4.59it/s, loss=0.644, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  80%|████████  | 20/25 [00:04<00:01,  4.57it/s, loss=0.644, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  80%|████████  | 20/25 [00:04<00:01,  4.57it/s, loss=0.666, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  84%|████████▍ | 21/25 [00:04<00:00,  4.58it/s, loss=0.666, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  84%|████████▍ | 21/25 [00:04<00:00,  4.58it/s, loss=0.669, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  88%|████████▊ | 22/25 [00:04<00:00,  4.58it/s, loss=0.669, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  88%|████████▊ | 22/25 [00:04<00:00,  4.58it/s, loss=0.646, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  92%|█████████▏| 23/25 [00:04<00:00,  4.58it/s, loss=0.646, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  92%|█████████▏| 23/25 [00:05<00:00,  4.58it/s, loss=0.644, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  96%|█████████▌| 24/25 [00:05<00:00,  4.58it/s, loss=0.644, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  96%|█████████▌| 24/25 [00:05<00:00,  4.58it/s, loss=0.679, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1: 100%|██████████| 25/25 [00:05<00:00,  4.59it/s, loss=0.679, dist=0, acc=0]\u001b[A\n",
            "Train epoch:  67%|██████▋   | 2/3 [00:12<00:06,  6.10s/it]\n",
            "Train step of epoch 1: 100%|██████████| 25/25 [00:06<00:00,  4.16it/s, dist=0.126, acc=0.7]\n",
            "\n",
            "Train step of epoch 2:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Train step of epoch 2:   4%|▍         | 1/25 [00:00<00:03,  7.86it/s]\u001b[A\n",
            "Train step of epoch 2:   4%|▍         | 1/25 [00:00<00:03,  7.86it/s, loss=0.653, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:   8%|▊         | 2/25 [00:00<00:04,  5.53it/s, loss=0.653, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:   8%|▊         | 2/25 [00:00<00:04,  5.53it/s, loss=0.558, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  12%|█▏        | 3/25 [00:00<00:04,  5.06it/s, loss=0.558, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  12%|█▏        | 3/25 [00:00<00:04,  5.06it/s, loss=0.593, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  16%|█▌        | 4/25 [00:00<00:04,  4.84it/s, loss=0.593, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  16%|█▌        | 4/25 [00:00<00:04,  4.84it/s, loss=0.688, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  20%|██        | 5/25 [00:01<00:04,  4.75it/s, loss=0.688, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  20%|██        | 5/25 [00:01<00:04,  4.75it/s, loss=0.677, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  24%|██▍       | 6/25 [00:01<00:04,  4.69it/s, loss=0.677, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  24%|██▍       | 6/25 [00:01<00:04,  4.69it/s, loss=0.633, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  28%|██▊       | 7/25 [00:01<00:03,  4.65it/s, loss=0.633, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  28%|██▊       | 7/25 [00:01<00:03,  4.65it/s, loss=0.582, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  32%|███▏      | 8/25 [00:01<00:03,  4.62it/s, loss=0.582, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  32%|███▏      | 8/25 [00:01<00:03,  4.62it/s, loss=0.428, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  36%|███▌      | 9/25 [00:01<00:03,  4.62it/s, loss=0.428, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  36%|███▌      | 9/25 [00:01<00:03,  4.62it/s, loss=0.516, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  40%|████      | 10/25 [00:02<00:03,  4.61it/s, loss=0.516, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  40%|████      | 10/25 [00:02<00:03,  4.61it/s, loss=0.478, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  44%|████▍     | 11/25 [00:02<00:03,  4.60it/s, loss=0.478, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  44%|████▍     | 11/25 [00:02<00:03,  4.60it/s, loss=0.689, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  48%|████▊     | 12/25 [00:02<00:02,  4.59it/s, loss=0.689, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  48%|████▊     | 12/25 [00:02<00:02,  4.59it/s, loss=0.713, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  52%|█████▏    | 13/25 [00:02<00:02,  4.59it/s, loss=0.713, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  52%|█████▏    | 13/25 [00:02<00:02,  4.59it/s, loss=0.555, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  56%|█████▌    | 14/25 [00:02<00:02,  4.59it/s, loss=0.555, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  56%|█████▌    | 14/25 [00:03<00:02,  4.59it/s, loss=0.525, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  60%|██████    | 15/25 [00:03<00:02,  4.59it/s, loss=0.525, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  60%|██████    | 15/25 [00:03<00:02,  4.59it/s, loss=0.658, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  64%|██████▍   | 16/25 [00:03<00:01,  4.58it/s, loss=0.658, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  64%|██████▍   | 16/25 [00:03<00:01,  4.58it/s, loss=0.456, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  68%|██████▊   | 17/25 [00:03<00:01,  4.59it/s, loss=0.456, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  68%|██████▊   | 17/25 [00:03<00:01,  4.59it/s, loss=0.438, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  72%|███████▏  | 18/25 [00:03<00:01,  4.58it/s, loss=0.438, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  72%|███████▏  | 18/25 [00:03<00:01,  4.58it/s, loss=0.69, dist=0, acc=0] \u001b[A\n",
            "Train step of epoch 2:  76%|███████▌  | 19/25 [00:04<00:01,  4.59it/s, loss=0.69, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  76%|███████▌  | 19/25 [00:04<00:01,  4.59it/s, loss=0.625, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  80%|████████  | 20/25 [00:04<00:01,  4.58it/s, loss=0.625, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  80%|████████  | 20/25 [00:04<00:01,  4.58it/s, loss=0.553, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  84%|████████▍ | 21/25 [00:04<00:00,  4.58it/s, loss=0.553, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  84%|████████▍ | 21/25 [00:04<00:00,  4.58it/s, loss=0.575, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  88%|████████▊ | 22/25 [00:04<00:00,  4.58it/s, loss=0.575, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  88%|████████▊ | 22/25 [00:04<00:00,  4.58it/s, loss=0.515, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  92%|█████████▏| 23/25 [00:04<00:00,  4.59it/s, loss=0.515, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  92%|█████████▏| 23/25 [00:05<00:00,  4.59it/s, loss=0.54, dist=0, acc=0] \u001b[A\n",
            "Train step of epoch 2:  96%|█████████▌| 24/25 [00:05<00:00,  4.59it/s, loss=0.54, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  96%|█████████▌| 24/25 [00:05<00:00,  4.59it/s, loss=0.599, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2: 100%|██████████| 25/25 [00:05<00:00,  4.58it/s, loss=0.599, dist=0, acc=0]\u001b[A\n",
            "Train epoch: 100%|██████████| 3/3 [00:18<00:00,  6.06s/it]\n",
            "Train step of epoch 2: 100%|██████████| 25/25 [00:06<00:00,  4.17it/s, dist=0.0589, acc=0.5]\n",
            "Train epoch: 100%|██████████| 3/3 [00:18<00:00,  6.08s/it]\n"
          ]
        }
      ],
      "source": [
        "# Train!!!\n",
        "trainer.fit()\n",
        "\n",
        "# Save model checkpoint after fitting on only rank0.\n",
        "# strategy.save_model(model, os.path.join(args.output_dir, \"rm.pt\"), only_rank0=True)\n",
        "trainer.save_model(path=os.path.join(args.output_dir, \"rm.pt\"), only_rank0=True)\n",
        "\n",
        "# Save optimizer checkpoint on all ranks.\n",
        "if args.need_optim_ckpt:\n",
        "    strategy.save_optimizer(trainer.optimizer,\n",
        "                            os.path.join(args.output_dir, \"rm_optim_checkpoint_%d.pt\" % (torch.cuda.current_device())),\n",
        "                            only_rank0=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EI7qm5N262EZ"
      },
      "source": [
        "# Step 3) PPO: Proximal Policy Optimization\n",
        "Further fine-tune the LLM from step 1 with the reward model and this dataset using RL (eg. PPO).\n",
        "\n",
        "- References\n",
        "    - [train_prompts.py](https://github.com/hpcaitech/ColossalAI/blob/main/applications/Chat/examples/train_prompts.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fiD6OSKpOrc"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "from copy import deepcopy\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from transformers import AutoTokenizer, BloomTokenizerFast\n",
        "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
        "\n",
        "from nextgpt.models.base import RewardModel\n",
        "from nextgpt.models.bloom import BLOOMActor, BLOOMCritic\n",
        "from nextgpt.models.gpt import GPTActor, GPTCritic\n",
        "from nextgpt.models.opt import OPTActor, OPTCritic\n",
        "from nextgpt.trainer import PPOTrainer\n",
        "from nextgpt.trainer.strategies import DDPStrategy, NaiveStrategy\n",
        "from nextgpt.dataset import PromptDataset, SupervisedDataset, DataCollatorForSupervisedDataset\n",
        "\n",
        "import json\n",
        "import os\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIs7m2m9pmWN",
        "outputId": "da58da84-bd83-40b8-fb2d-b09a3d6a1be5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(output_dir='./output_3_ppo', strategy='naive', model='gpt2', pretrain='./output_1_sft', rm_model='gpt2', rm_path='./output_2_rm/rm.pt', rm_pretrain='gpt2', need_optim_ckpt=False, num_episodes=1, max_timesteps=3, update_timesteps=3, max_epochs=1, train_batch_size=8, ptx_batch_size=1, experience_batch_size=8, lora_rank=0, kl_coef=0.1, ptx_coef=0.9)\n"
          ]
        }
      ],
      "source": [
        "# Define arguments.\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument(\"--output_dir\", type=str, default=\"./output_3_ppo\")\n",
        "parser.add_argument(\"--strategy\",\n",
        "                    type=str,\n",
        "                    choices=[\"naive\", \"ddp\"],\n",
        "                    default=\"naive\")\n",
        "parser.add_argument(\"--model\", \n",
        "                    type=str, \n",
        "                    choices=[\"gpt2\", \"bloom\", \"opt\"],\n",
        "                    default=\"gpt2\")\n",
        "parser.add_argument(\"--pretrain\", type=str, default=None)\n",
        "parser.add_argument(\"--rm_model\", \n",
        "                    type=str, \n",
        "                    choices=[\"gpt2\", \"bloom\", \"opt\"],\n",
        "                    default=\"gpt2\")\n",
        "parser.add_argument(\"--rm_path\", type=str, default=None)\n",
        "parser.add_argument(\"--rm_pretrain\", type=str, default=None)\n",
        "parser.add_argument(\"--need_optim_ckpt\", type=bool, default=False)\n",
        "parser.add_argument(\"--num_episodes\", type=int, default=10)\n",
        "parser.add_argument(\"--max_timesteps\", type=int, default=3)\n",
        "parser.add_argument(\"--update_timesteps\", type=int, default=3)\n",
        "parser.add_argument(\"--max_epochs\", type=int, default=5)\n",
        "parser.add_argument(\"--train_batch_size\", type=int, default=8)\n",
        "parser.add_argument(\"--ptx_batch_size\", type=int, default=1)\n",
        "parser.add_argument(\"--experience_batch_size\", type=int, default=8)\n",
        "parser.add_argument(\"--lora_rank\", type=int, default=0, help=\"low-rank adaptation matrices rank\")\n",
        "parser.add_argument(\"--kl_coef\", type=float, default=0.1)\n",
        "parser.add_argument(\"--ptx_coef\", type=float, default=0.9)\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "# For testing.\n",
        "# args.pretrain= \"gpt2\"\n",
        "args.pretrain= \"./output_1_sft\"\n",
        "args.rm_path = \"./output_2_rm/rm.pt\" # RM model path\n",
        "args.rm_pretrain= \"gpt2\"\n",
        "\n",
        "args.num_episodes = 1\n",
        "args.max_epochs = 1\n",
        "\n",
        "print(args)\n",
        "if not os.path.exists(args.output_dir):\n",
        "    os.makedirs(args.output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyHx9jXoq0yi"
      },
      "outputs": [],
      "source": [
        "# Configure strategy.\n",
        "if args.strategy == \"naive\":\n",
        "    strategy = NaiveStrategy()\n",
        "elif args.strategy == \"ddp\":\n",
        "    strategy = DDPStrategy()\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported strategy: {args.strategy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pH1wPr4Nq-bM"
      },
      "outputs": [],
      "source": [
        "if args.rm_path is not None:\n",
        "    rm_state_dict = torch.load(args.rm_path, map_location=\"cpu\")\n",
        "\n",
        "# Configure intial model.\n",
        "if args.model == \"gpt2\":\n",
        "    initial_model = GPTActor(pretrained=args.pretrain)\n",
        "elif args.model == \"bloom\":\n",
        "    initial_model = BLOOMActor(pretrained=args.pretrain)\n",
        "elif args.model == \"opt\":\n",
        "    initial_model = OPTActor(pretrained=args.pretrain)\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported actor model: {args.model}\")\n",
        "\n",
        "# Configure reward model.\n",
        "if args.rm_model == \"gpt2\":\n",
        "    reward_model = GPTRM(pretrained=args.rm_pretrain)\n",
        "elif args.rm_model == \"bloom\":\n",
        "    reward_model = BLOOMRM(pretrained=args.rm_pretrain)\n",
        "elif args.rm_model == \"opt\":\n",
        "    reward_model = OPTRM(pretrained=args.rm_pretrain)\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported reward model: {args.rm_model}\")\n",
        "\n",
        "if args.rm_path is not None:\n",
        "    reward_model.load_state_dict(rm_state_dict)\n",
        "\n",
        "# initial_model.to(torch.float16).to(torch.cuda.current_device())\n",
        "# reward_model.to(torch.float16).to(torch.cuda.current_device())\n",
        "initial_model.to(torch.cuda.current_device())\n",
        "reward_model.to(torch.cuda.current_device())\n",
        "\n",
        "# Configure actor and critic.\n",
        "with strategy.model_init_context():\n",
        "    # Actor\n",
        "    if args.model == \"gpt2\":\n",
        "        actor = GPTActor(pretrained=args.pretrain, lora_rank=args.lora_rank)\n",
        "    elif args.model == \"bloom\":\n",
        "        actor = BLOOMActor(pretrained=args.pretrain_actor, lora_rank=args.lora_rank)\n",
        "    elif args.model == \"opt\":\n",
        "        actor = OPTActor(pretrained=args.pretrain, lora_rank=args.lora_rank)        \n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported actor model: {args.model}\")\n",
        "\n",
        "    # Critic\n",
        "    if args.rm_model == \"gpt2\":\n",
        "        critic = GPTCritic(pretrained=args.rm_pretrain, lora_rank=args.lora_rank)\n",
        "    elif args.rm_model == \"bloom\":\n",
        "        critic = BLOOMCritic(pretrained=args.rm_pretrain, lora_rank=args.lora_rank)\n",
        "    elif args.rm_model == \"opt\":\n",
        "        critic = OPTCritic(pretrained=args.rm_pretrain, lora_rank=args.lora_rank)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported reward model: {args.rm_model}\")\n",
        "\n",
        "    if args.rm_path is not None:\n",
        "        critic.load_state_dict(rm_state_dict)\n",
        "        del rm_state_dict\n",
        "\n",
        "# critic.to(torch.float16).to(torch.cuda.current_device())\n",
        "# actor.to(torch.float16).to(torch.cuda.current_device())\n",
        "critic.to(torch.cuda.current_device())\n",
        "actor.to(torch.cuda.current_device())\n",
        "\n",
        "# Configure tokenizer.\n",
        "if args.model == \"gpt2\":\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\n",
        "        \"gpt2\", \n",
        "        # bos_token=\"<|startoftext|>\",\n",
        "        # eos_token=\"<|endoftext|>\", \n",
        "        # pad_token=\"<|pad|>\",\n",
        "        # padding_side=\"right\", \n",
        "        model_max_length=512,\n",
        "        )\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "elif args.model == \"bloom\":\n",
        "    tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-560m\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token            \n",
        "elif args.model == \"opt\":\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-xAImuVqh46"
      },
      "outputs": [],
      "source": [
        "# Configure optimizer.\n",
        "actor_optim = Adam(actor.parameters(), lr=1e-7)\n",
        "critic_optim = Adam(critic.parameters(), lr=1e-7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKBzMklPqijJ"
      },
      "outputs": [],
      "source": [
        "# Setting the models.\n",
        "(actor, actor_optim), (critic, critic_optim) = strategy.prepare((actor, actor_optim), (critic, critic_optim))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzDpPXr7rRTL"
      },
      "outputs": [],
      "source": [
        "def tokenize_fn(texts):\n",
        "    # MUST padding to max length to ensure inputs of all ranks have the same length\n",
        "    # Different length may lead to hang when using gemini, as different generation steps\n",
        "    batch = tokenizer(texts, return_tensors=\"pt\", max_length=96, padding=\"max_length\", truncation=True)\n",
        "    return {k: v.to(torch.cuda.current_device()) for k, v in batch.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEMpasbTrgzF",
        "outputId": "25f91d77-ec64-46b0-db76-0c9bbb0a4b1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset webgpt_comparisons (/root/.cache/huggingface/datasets/openai___webgpt_comparisons/default/0.0.0/8b5d5879cdc98c4c0099af6053dffe8d504588d43d3b11f1b1ec223ab1e8db0a)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'instruction': 'Voiced by Harry Shearer, what Simpsons character was modeled after Ted Koppel?', 'completion': 'The Simpsons character that was possibly based on Ted Koppel is Kent Brockman.  He is a local news anchor in Springfield and is modeled after Ted Koppel. [1]'}]\n"
          ]
        }
      ],
      "source": [
        "# Prepare dataset.\n",
        "dataset_webgpt_comp = load_dataset(\"openai/webgpt_comparisons\", split=\"train[:20%]\")\n",
        "\n",
        "data_list = []\n",
        "for row in dataset_webgpt_comp:\n",
        "    question = row[\"question\"][\"full_text\"]\n",
        "    answer_0 = row[\"answer_0\"]\n",
        "    data_list.append({\n",
        "        \"instruction\": question,\n",
        "        \"completion\": answer_0\n",
        "    })\n",
        "\n",
        "print(data_list[:1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure dataloader.\n",
        "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
        "\n",
        "prompt_dataset = PromptDataset(\n",
        "    dataset=data_list, \n",
        "    tokenizer=tokenizer, \n",
        "    prompt_template=PROMPT_TEMPLATE, \n",
        "    max_datasets_size=10000)\n",
        "\n",
        "prompt_sampler = None\n",
        "if dist.is_initialized() and dist.get_world_size() > 1:\n",
        "    prompt_sampler = DistributedSampler(prompt_dataset, shuffle=True, seed=42, drop_last=True)\n",
        "\n",
        "prompt_dataloader = DataLoader(\n",
        "    prompt_dataset,\n",
        "    shuffle=(prompt_sampler is None),\n",
        "    sampler=prompt_sampler,\n",
        "    batch_size=args.train_batch_size)\n",
        "\n",
        "pretrain_dataset = SupervisedDataset(\n",
        "    dataset=data_list,\n",
        "    tokenizer=tokenizer, \n",
        "    prompt_template=PROMPT_TEMPLATE,\n",
        "    completion_field=\"completion\",\n",
        "    max_datasets_size=10000,\n",
        "    max_length=512,\n",
        "    verbose=True)\n",
        "\n",
        "pretrain_sampler = None\n",
        "if dist.is_initialized() and dist.get_world_size() > 1:\n",
        "    pretrain_sampler = DistributedSampler(pretrain_dataset, shuffle=True, seed=42, drop_last=True)\n",
        "\n",
        "pretrain_dataloader = DataLoader(\n",
        "    pretrain_dataset,\n",
        "    shuffle=(pretrain_sampler is None),\n",
        "    sampler=pretrain_sampler,\n",
        "    batch_size=args.ptx_batch_size,\n",
        "    collate_fn=data_collator)\n",
        "\n",
        "def tokenize_fn(texts):\n",
        "    # MUST padding to max length to ensure inputs of all ranks have the same length\n",
        "    # Different length may lead to hang when using gemini, as different generation steps\n",
        "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding='max_length', truncation=True)\n",
        "    return {k: v.to(torch.cuda.current_device()) for k, v in batch.items()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGmuTC7hIgWa",
        "outputId": "1a78c3de-6359-4ab7-9654-010b379d8901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Limiting dataset to 10000 examples.\n",
            "Loading data...\n",
            "Limiting dataset to 10000 examples.\n",
            "Formatting inputs...\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Voiced by Harry Shearer, what Simpsons character was modeled after Ted Koppel?\n",
            "\n",
            "### Response:\n",
            "The Simpsons character that was possibly based on Ted Koppel is Kent Brockman.  He is a local news anchor in Springfield and is modeled after Ted Koppel. [1]<|endoftext|>\n",
            "Tokenizing inputs... This may take some time...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MihLfe26u-4g",
        "outputId": "9de667b0-669b-4083-9215-38a7a69c629c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.17it/s]\n",
            "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, reward=-1.36]\u001b[A\n",
            "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:00,  4.04it/s, reward=-1.36]\u001b[A\n",
            "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:00,  4.04it/s, reward=-.964]\u001b[A\n",
            "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:00<00:00,  4.80it/s, reward=-.964]\u001b[A\n",
            "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:00<00:00,  4.80it/s, reward=-1.21]\u001b[A\n",
            "Train epoch [1/1]: 100%|██████████| 3/3 [00:00<00:00,  4.94it/s, reward=-1.21]\n",
            "Episode [1/1]: 100%|██████████| 3/3 [00:03<00:00,  1.13s/it]\n"
          ]
        }
      ],
      "source": [
        "# Configure trainer.\n",
        "trainer = PPOTrainer(\n",
        "    strategy,\n",
        "    actor,\n",
        "    critic,\n",
        "    reward_model,\n",
        "    initial_model,\n",
        "    actor_optim,\n",
        "    critic_optim,\n",
        "    kl_coef=args.kl_coef,\n",
        "    ptx_coef=args.ptx_coef,\n",
        "    max_epochs=args.max_epochs,\n",
        "    train_batch_size=args.train_batch_size,\n",
        "    experience_batch_size=args.experience_batch_size,\n",
        "    tokenizer=tokenize_fn,\n",
        "    max_length=128,\n",
        "    do_sample=True,\n",
        "    temperature=1.0,\n",
        "    top_k=50,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "trainer.fit(\n",
        "    prompt_dataloader=prompt_dataloader,\n",
        "    pretrain_dataloader=pretrain_dataloader,\n",
        "    num_episodes=args.num_episodes,\n",
        "    max_timesteps=args.max_timesteps,\n",
        "    update_timesteps=args.update_timesteps)\n",
        "\n",
        "# Save model checkpoint after fitting on only rank0.\n",
        "trainer.save_model(os.path.join(args.output_dir, \"actor.pt\"), only_rank0=True, tokenizer=tokenizer)\n",
        "# Save optimizer checkpoint on all ranks.\n",
        "strategy.save_optimizer(actor_optim,\n",
        "                        os.path.join(args.output_dir, \"actor_optim_checkpoint_%d.pt\" % (torch.cuda.current_device())),\n",
        "                        only_rank0=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecCC92TM7w3g",
        "outputId": "0adcc1b6-59d6-4694-f200-952dec1e4a9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "######################################################################\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Heterophobia is the irrational fear of what\n",
            "\n",
            "### Response:Heterophobia is the irrational fear of what will happen to you\n",
            "\n",
            "If you ever read an internet article that claims your friend's boyfriend has a mental disorder, the first thing you would do is get on the internet, and the person in question will think you\n"
          ]
        }
      ],
      "source": [
        "#  Inference test.\n",
        "def generation(input_text):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(torch.cuda.current_device())\n",
        "    outputs = actor.generate(input_ids,\n",
        "                             max_length=100,\n",
        "                             do_sample=True,\n",
        "                             top_k=50,\n",
        "                             top_p=0.95,\n",
        "                             num_return_sequences=1)\n",
        "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
        "    print(\"#\" * 70)\n",
        "    print(output)\n",
        "    return output\n",
        "\n",
        "\n",
        "test_isntruction_list = [\n",
        "    \"Heterophobia is the irrational fear of what\",\n",
        "    ]\n",
        "\n",
        "test_prompt_list = [PROMPT_TEMPLATE.format_map({\"instruction\": tmp}) for tmp in test_isntruction_list]\n",
        "\n",
        "for input_text in test_prompt_list:\n",
        "    output = generation(input_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2Ju8fgt_is2"
      },
      "source": [
        "# Inference by PPO actor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOiwh9lTBSMS"
      },
      "outputs": [],
      "source": [
        "import argparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vd5AIx3nAJ35"
      },
      "outputs": [],
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--model\", \n",
        "                    type=str, \n",
        "                    choices=[\"gpt2\", \"bloom\", \"opt\"],\n",
        "                    default=\"gpt2\")\n",
        "# We suggest to use the pretrained model from HuggingFace, use pretrain to configure model\n",
        "parser.add_argument(\"--pretrain\", type=str, default=None)\n",
        "parser.add_argument(\"--model_path\", type=str, default=None)\n",
        "parser.add_argument(\"--input\", type=str, default=\"Question: How are you ? Answer:\")\n",
        "parser.add_argument(\"--max_length\", type=int, default=100)\n",
        "args = parser.parse_args([])\n",
        "\n",
        "# args.pretrain= \"gpt2\"\n",
        "args.pretrain= \"./output_1_sft\"\n",
        "args.model_path = \"./output_3_ppo/actor.pt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wg_mvSAAfZu"
      },
      "outputs": [],
      "source": [
        "def eval(args):\n",
        "    # Configure model.\n",
        "    if args.model == \"gpt2\":\n",
        "        actor = GPTActor(pretrained=args.pretrain).to(torch.cuda.current_device())\n",
        "    elif args.model == \"bloom\":\n",
        "        actor = BLOOMActor(pretrained=args.pretrain).to(torch.cuda.current_device())\n",
        "    elif args.model == \"opt\":\n",
        "        actor = OPTActor(pretrained=args.pretrain).to(torch.cuda.current_device())\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported model: {args.model}\")\n",
        "\n",
        "    state_dict = torch.load(args.model_path)\n",
        "    # actor.model.load_state_dict(state_dict)\n",
        "    actor.load_state_dict(state_dict)\n",
        "\n",
        "    # Configure tokenizer.\n",
        "    if args.model == \"gpt2\":\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    elif args.model == \"bloom\":\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\")\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    elif args.model == \"opt\":\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported model: {args.model}\")\n",
        "\n",
        "    actor.eval()\n",
        "    input = args.input\n",
        "    input_ids = tokenizer.encode(input, return_tensors=\"pt\").to(torch.cuda.current_device())\n",
        "    outputs = actor.generate(input_ids,\n",
        "                             max_length=args.max_length,\n",
        "                             do_sample=True,\n",
        "                             top_k=10,\n",
        "                             top_p=0.95,\n",
        "                             num_return_sequences=1)\n",
        "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
        "    print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VY7IyQs3B7Gw",
        "outputId": "52ebc983-3b38-4166-8b68-7322969c48e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['model.transformer.wte.weight', 'model.transformer.wpe.weight', 'model.transformer.h.0.ln_1.weight', 'model.transformer.h.0.ln_1.bias', 'model.transformer.h.0.attn.bias', 'model.transformer.h.0.attn.masked_bias', 'model.transformer.h.0.attn.c_attn.weight', 'model.transformer.h.0.attn.c_attn.bias', 'model.transformer.h.0.attn.c_proj.weight', 'model.transformer.h.0.attn.c_proj.bias', 'model.transformer.h.0.ln_2.weight', 'model.transformer.h.0.ln_2.bias', 'model.transformer.h.0.mlp.c_fc.weight', 'model.transformer.h.0.mlp.c_fc.bias', 'model.transformer.h.0.mlp.c_proj.weight', 'model.transformer.h.0.mlp.c_proj.bias', 'model.transformer.h.1.ln_1.weight', 'model.transformer.h.1.ln_1.bias', 'model.transformer.h.1.attn.bias', 'model.transformer.h.1.attn.masked_bias', 'model.transformer.h.1.attn.c_attn.weight', 'model.transformer.h.1.attn.c_attn.bias', 'model.transformer.h.1.attn.c_proj.weight', 'model.transformer.h.1.attn.c_proj.bias', 'model.transformer.h.1.ln_2.weight', 'model.transformer.h.1.ln_2.bias', 'model.transformer.h.1.mlp.c_fc.weight', 'model.transformer.h.1.mlp.c_fc.bias', 'model.transformer.h.1.mlp.c_proj.weight', 'model.transformer.h.1.mlp.c_proj.bias', 'model.transformer.h.2.ln_1.weight', 'model.transformer.h.2.ln_1.bias', 'model.transformer.h.2.attn.bias', 'model.transformer.h.2.attn.masked_bias', 'model.transformer.h.2.attn.c_attn.weight', 'model.transformer.h.2.attn.c_attn.bias', 'model.transformer.h.2.attn.c_proj.weight', 'model.transformer.h.2.attn.c_proj.bias', 'model.transformer.h.2.ln_2.weight', 'model.transformer.h.2.ln_2.bias', 'model.transformer.h.2.mlp.c_fc.weight', 'model.transformer.h.2.mlp.c_fc.bias', 'model.transformer.h.2.mlp.c_proj.weight', 'model.transformer.h.2.mlp.c_proj.bias', 'model.transformer.h.3.ln_1.weight', 'model.transformer.h.3.ln_1.bias', 'model.transformer.h.3.attn.bias', 'model.transformer.h.3.attn.masked_bias', 'model.transformer.h.3.attn.c_attn.weight', 'model.transformer.h.3.attn.c_attn.bias', 'model.transformer.h.3.attn.c_proj.weight', 'model.transformer.h.3.attn.c_proj.bias', 'model.transformer.h.3.ln_2.weight', 'model.transformer.h.3.ln_2.bias', 'model.transformer.h.3.mlp.c_fc.weight', 'model.transformer.h.3.mlp.c_fc.bias', 'model.transformer.h.3.mlp.c_proj.weight', 'model.transformer.h.3.mlp.c_proj.bias', 'model.transformer.h.4.ln_1.weight', 'model.transformer.h.4.ln_1.bias', 'model.transformer.h.4.attn.bias', 'model.transformer.h.4.attn.masked_bias', 'model.transformer.h.4.attn.c_attn.weight', 'model.transformer.h.4.attn.c_attn.bias', 'model.transformer.h.4.attn.c_proj.weight', 'model.transformer.h.4.attn.c_proj.bias', 'model.transformer.h.4.ln_2.weight', 'model.transformer.h.4.ln_2.bias', 'model.transformer.h.4.mlp.c_fc.weight', 'model.transformer.h.4.mlp.c_fc.bias', 'model.transformer.h.4.mlp.c_proj.weight', 'model.transformer.h.4.mlp.c_proj.bias', 'model.transformer.h.5.ln_1.weight', 'model.transformer.h.5.ln_1.bias', 'model.transformer.h.5.attn.bias', 'model.transformer.h.5.attn.masked_bias', 'model.transformer.h.5.attn.c_attn.weight', 'model.transformer.h.5.attn.c_attn.bias', 'model.transformer.h.5.attn.c_proj.weight', 'model.transformer.h.5.attn.c_proj.bias', 'model.transformer.h.5.ln_2.weight', 'model.transformer.h.5.ln_2.bias', 'model.transformer.h.5.mlp.c_fc.weight', 'model.transformer.h.5.mlp.c_fc.bias', 'model.transformer.h.5.mlp.c_proj.weight', 'model.transformer.h.5.mlp.c_proj.bias', 'model.transformer.h.6.ln_1.weight', 'model.transformer.h.6.ln_1.bias', 'model.transformer.h.6.attn.bias', 'model.transformer.h.6.attn.masked_bias', 'model.transformer.h.6.attn.c_attn.weight', 'model.transformer.h.6.attn.c_attn.bias', 'model.transformer.h.6.attn.c_proj.weight', 'model.transformer.h.6.attn.c_proj.bias', 'model.transformer.h.6.ln_2.weight', 'model.transformer.h.6.ln_2.bias', 'model.transformer.h.6.mlp.c_fc.weight', 'model.transformer.h.6.mlp.c_fc.bias', 'model.transformer.h.6.mlp.c_proj.weight', 'model.transformer.h.6.mlp.c_proj.bias', 'model.transformer.h.7.ln_1.weight', 'model.transformer.h.7.ln_1.bias', 'model.transformer.h.7.attn.bias', 'model.transformer.h.7.attn.masked_bias', 'model.transformer.h.7.attn.c_attn.weight', 'model.transformer.h.7.attn.c_attn.bias', 'model.transformer.h.7.attn.c_proj.weight', 'model.transformer.h.7.attn.c_proj.bias', 'model.transformer.h.7.ln_2.weight', 'model.transformer.h.7.ln_2.bias', 'model.transformer.h.7.mlp.c_fc.weight', 'model.transformer.h.7.mlp.c_fc.bias', 'model.transformer.h.7.mlp.c_proj.weight', 'model.transformer.h.7.mlp.c_proj.bias', 'model.transformer.h.8.ln_1.weight', 'model.transformer.h.8.ln_1.bias', 'model.transformer.h.8.attn.bias', 'model.transformer.h.8.attn.masked_bias', 'model.transformer.h.8.attn.c_attn.weight', 'model.transformer.h.8.attn.c_attn.bias', 'model.transformer.h.8.attn.c_proj.weight', 'model.transformer.h.8.attn.c_proj.bias', 'model.transformer.h.8.ln_2.weight', 'model.transformer.h.8.ln_2.bias', 'model.transformer.h.8.mlp.c_fc.weight', 'model.transformer.h.8.mlp.c_fc.bias', 'model.transformer.h.8.mlp.c_proj.weight', 'model.transformer.h.8.mlp.c_proj.bias', 'model.transformer.h.9.ln_1.weight', 'model.transformer.h.9.ln_1.bias', 'model.transformer.h.9.attn.bias', 'model.transformer.h.9.attn.masked_bias', 'model.transformer.h.9.attn.c_attn.weight', 'model.transformer.h.9.attn.c_attn.bias', 'model.transformer.h.9.attn.c_proj.weight', 'model.transformer.h.9.attn.c_proj.bias', 'model.transformer.h.9.ln_2.weight', 'model.transformer.h.9.ln_2.bias', 'model.transformer.h.9.mlp.c_fc.weight', 'model.transformer.h.9.mlp.c_fc.bias', 'model.transformer.h.9.mlp.c_proj.weight', 'model.transformer.h.9.mlp.c_proj.bias', 'model.transformer.h.10.ln_1.weight', 'model.transformer.h.10.ln_1.bias', 'model.transformer.h.10.attn.bias', 'model.transformer.h.10.attn.masked_bias', 'model.transformer.h.10.attn.c_attn.weight', 'model.transformer.h.10.attn.c_attn.bias', 'model.transformer.h.10.attn.c_proj.weight', 'model.transformer.h.10.attn.c_proj.bias', 'model.transformer.h.10.ln_2.weight', 'model.transformer.h.10.ln_2.bias', 'model.transformer.h.10.mlp.c_fc.weight', 'model.transformer.h.10.mlp.c_fc.bias', 'model.transformer.h.10.mlp.c_proj.weight', 'model.transformer.h.10.mlp.c_proj.bias', 'model.transformer.h.11.ln_1.weight', 'model.transformer.h.11.ln_1.bias', 'model.transformer.h.11.attn.bias', 'model.transformer.h.11.attn.masked_bias', 'model.transformer.h.11.attn.c_attn.weight', 'model.transformer.h.11.attn.c_attn.bias', 'model.transformer.h.11.attn.c_proj.weight', 'model.transformer.h.11.attn.c_proj.bias', 'model.transformer.h.11.ln_2.weight', 'model.transformer.h.11.ln_2.bias', 'model.transformer.h.11.mlp.c_fc.weight', 'model.transformer.h.11.mlp.c_fc.bias', 'model.transformer.h.11.mlp.c_proj.weight', 'model.transformer.h.11.mlp.c_proj.bias', 'model.transformer.ln_f.weight', 'model.transformer.ln_f.bias', 'model.lm_head.weight'])\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Heterophobia is the irrational fear of what?\n",
            "\n",
            "### Response:Heterophobia is the irrational fear of what? [1, 2] The fear of being perceived as a different kind of person, a person with a different kind of personality [2]. [2] This fear is triggered by perceived differences in appearance between different\n"
          ]
        }
      ],
      "source": [
        "input_text = \"Heterophobia is the irrational fear of what?\"\n",
        "args.input = PROMPT_TEMPLATE.format_map({\"instruction\": input_text})\n",
        "eval(args)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b3c0f8e79f5e49ee89ca7814ed67867d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3766fafa089c4969a39e6137a8dfaef7",
              "IPY_MODEL_76ed9ac281114e4894690cb459450d5b",
              "IPY_MODEL_ca52f099b62f476fa97a69f01eed54f9"
            ],
            "layout": "IPY_MODEL_6a99bea8b0bd42af9e329e5681a04e86"
          }
        },
        "3766fafa089c4969a39e6137a8dfaef7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89b3b0d550da4f6ea2739ccd82eaaba6",
            "placeholder": "​",
            "style": "IPY_MODEL_da419feb0219474b9d4ab528a367f3d7",
            "value": "Downloading (…)/main/tokenizer.json: 100%"
          }
        },
        "76ed9ac281114e4894690cb459450d5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d64734e05c14085975cfb0afe0e7129",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9810ead6985147b184a04a4c9b458011",
            "value": 1355256
          }
        },
        "ca52f099b62f476fa97a69f01eed54f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f3a6bd610ad4092b0de5b0519d6c7ad",
            "placeholder": "​",
            "style": "IPY_MODEL_8b2c028b27bd4b599b7670cdbd7ad235",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 3.23MB/s]"
          }
        },
        "6a99bea8b0bd42af9e329e5681a04e86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89b3b0d550da4f6ea2739ccd82eaaba6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da419feb0219474b9d4ab528a367f3d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d64734e05c14085975cfb0afe0e7129": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9810ead6985147b184a04a4c9b458011": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f3a6bd610ad4092b0de5b0519d6c7ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b2c028b27bd4b599b7670cdbd7ad235": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}