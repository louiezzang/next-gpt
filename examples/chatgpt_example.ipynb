{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/louiezzang/next-gpt/blob/main/examples/chatgpt_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o5OIi7jDa9K"
      },
      "source": [
        "# Overview\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TMz3aiLECTk"
      },
      "source": [
        "What is RLHF? <br>\n",
        "See [this link](https://gist.github.com/JoaoLages/c6f2dfd13d2484aa8bb0b2d567fbf093).\n",
        "\n",
        "<br>\n",
        "\n",
        "**Example of RLHF dataset**:\n",
        "\n",
        "Total 3 datasets are needed for training the 3 steps(SFT, RM and PPO)\n",
        "- [Example of dataset](https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/chatllama#dataset-preparation)\n",
        "- [Example of dataset 1](https://huggingface.co/datasets/stanfordnlp/SHP)\n",
        "- [Example of dataset 2](https://huggingface.co/datasets/Anthropic/hh-rlhf)\n",
        "\n",
        "step1) Dataset for SFT(Supervised Fine-tuning training)\n",
        "```json\n",
        "[\n",
        "    {\n",
        "        \"prompt\": \"\",\n",
        "        \"completion\": \"\"        \n",
        "    }, ...\n",
        "]\n",
        "```\n",
        "\n",
        "step2) Dataset for RM(Reward Model) training: There are multiple completetions with human rated ranking score for one prompt.\n",
        "```json\n",
        "[\n",
        "    {\n",
        "        \"prompt\": \"\",\n",
        "        \"completion_1\": \"\",\n",
        "        \"completion_2\": \"\",\n",
        "        \"completion_3\": \"\",            \n",
        "        \"ranking\": [1, 0, 2]\n",
        "    }, ...\n",
        "]\n",
        "```\n",
        "    \n",
        "step3) Dataset for PPO(RLHF) training: It only consists of prompt.\n",
        "```json\n",
        "[\n",
        "    {\n",
        "        \"prompt\": \"\"\n",
        "    }, ...\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YG_5Fvkx9fH0"
      },
      "source": [
        "# Environment setup\n",
        "\n",
        "#### Installation (python>=3.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdbMmQE5Zsjw"
      },
      "outputs": [],
      "source": [
        "# Install next-gpt lib.\n",
        "!rm -rf ./next-gpt/\n",
        "!git clone https://github.com/louiezzang/next-gpt.git\n",
        "%cd next-gpt/\n",
        "!pip install .\n",
        "%cd ../"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JezJ8wz3_A7B"
      },
      "source": [
        "# Step 1) SFT: Surpervised Fine-tuning\n",
        "Build a Supervised Fine-tuning model to answer well to the question.\n",
        "\n",
        "- Refereneces\n",
        "  - [fine tuning code_1](https://github.com/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb)\n",
        "  - [fine tuning code_2](https://github.com/Beomi/KoAlpaca/blob/main/train.py)\n",
        "\n",
        "\n",
        "- SFT(Supervised Fine Tuning)\n",
        "- Fine-tune a pretrained LLM on a specific domain or corpus of instructions and human demonstrations\n",
        "\n",
        "- Dataset example\n",
        "```json\n",
        "[\n",
        "    {\n",
        "        \"prompt\": \"\",\n",
        "        \"completion\": \"\"        \n",
        "    }, ...\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpoPqaBfAkqW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "import json\n",
        "import yaml\n",
        "import argparse\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import loralib as lora\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, BloomTokenizerFast\n",
        "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "from nextgpt.dataset import SupervisedDataset, DataCollatorForSupervisedDataset\n",
        "from nextgpt.trainer import SFTTrainer\n",
        "from nextgpt.trainer.strategies import DDPStrategy, NaiveStrategy\n",
        "from nextgpt.models.bloom import BLOOMLM\n",
        "from nextgpt.models.gpt import GPTLM\n",
        "from nextgpt.models.opt import OPTLM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT_TEMPLATE = (\n",
        "  \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "  \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "  \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
        ")"
      ],
      "metadata": {
        "id": "ZhTIRVuOjjNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLJdNVzEsdJT"
      },
      "outputs": [],
      "source": [
        "# Define arguments.\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--strategy\",\n",
        "                    choices=[\"naive\", \"ddp\"],\n",
        "                    default=\"naive\")\n",
        "parser.add_argument(\"--model\", choices=[\"gpt2\", \"bloom\", \"opt\"], default=\"gpt2\")\n",
        "parser.add_argument(\"--pretrain\", type=str, default=None)\n",
        "parser.add_argument(\"--max_datasets_size\", type=int, default=None)\n",
        "parser.add_argument(\"--need_optim_ckpt\", type=bool, default=False)\n",
        "parser.add_argument(\"--max_epochs\", type=int, default=3)\n",
        "parser.add_argument(\"--batch_size\", type=int, default=4)\n",
        "parser.add_argument(\"--max_len\", type=int, default=512)\n",
        "parser.add_argument(\"--lora_rank\", type=int, default=0, help=\"low-rank adaptation matrices rank. 0 means LoRA is not applied.\")\n",
        "parser.add_argument(\"--log_interval\", type=int, default=100, help=\"how many steps to log\")\n",
        "parser.add_argument(\"--lr\", type=float, default=5e-6)\n",
        "parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=8)\n",
        "parser.add_argument(\"--output_dir\", type=str, default=\"./output_1_sft\")\n",
        "\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "# For testing.\n",
        "args.pretrain = \"gpt2\"\n",
        "args.max_datasets_size = 10000\n",
        "args.max_epochs = 1\n",
        "\n",
        "print(args)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure strategy.\n",
        "if args.strategy == \"naive\":\n",
        "    strategy = NaiveStrategy()\n",
        "elif args.strategy == \"ddp\":\n",
        "    strategy = DDPStrategy()\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported strategy: {args.strategy}\")\n",
        "\n",
        "# Configure model.\n",
        "with strategy.model_init_context():\n",
        "    if args.model == \"bloom\":\n",
        "        model = BLOOMLM(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
        "    elif args.model == \"opt\":\n",
        "        model = OPTLM(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
        "    elif args.model == \"gpt2\":\n",
        "        model = GPTLM(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported model: {args.model}\")\n",
        "\n",
        "# Configure tokenizer.\n",
        "if args.model == \"gpt2\":\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "elif args.model == \"bloom\":\n",
        "    tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-560m\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "elif args.model == \"opt\":\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")  \n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported model: {args.model}\")"
      ],
      "metadata": {
        "id": "jUPPbygUgtlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure dataset.\n",
        "dataset_webgpt_comp = load_dataset(\"openai/webgpt_comparisons\", split=\"train[:20%]\")\n",
        "\n",
        "data_list = []\n",
        "for row in dataset_webgpt_comp:\n",
        "    question = row[\"question\"][\"full_text\"]\n",
        "    answer_0 = row[\"answer_0\"]\n",
        "    data_list.append({\n",
        "        \"instruction\": question,\n",
        "        \"completion\": answer_0\n",
        "    })\n",
        "\n",
        "dataset = SupervisedDataset(\n",
        "    dataset=data_list,\n",
        "    tokenizer=tokenizer, \n",
        "    prompt_template=PROMPT_TEMPLATE,\n",
        "    completion_field=\"completion\",\n",
        "    max_datasets_size=args.max_datasets_size,\n",
        "    max_length=args.max_len,\n",
        "    verbose=True)\n",
        "\n",
        "# Split train and eval dataset.\n",
        "train_size = int(0.8 * len(dataset))\n",
        "eval_size = len(dataset) - train_size\n",
        "train_dataset, eval_dataset = torch.utils.data.random_split(dataset, [train_size, eval_size])\n",
        "\n",
        "# Data collator.\n",
        "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "62HB_iNujVAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train!!!\n",
        "trainer = SFTTrainer(model=model,\n",
        "                     strategy=strategy,\n",
        "                     data_collator=data_collator,\n",
        "                     train_dataset=train_dataset,\n",
        "                     eval_dataset=eval_dataset,\n",
        "                     batch_size=args.batch_size,\n",
        "                     max_epochs=args.max_epochs,\n",
        "                     gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
        "                     lr=args.lr)\n",
        "\n",
        "trainer.fit()\n",
        "\n",
        "# Save model checkpoint after fitting on only rank0.\n",
        "trainer.save_model(path=args.output_dir, only_rank0=True, tokenizer=tokenizer)\n",
        "# Save optimizer checkpoint on all ranks.\n",
        "if args.need_optim_ckpt:\n",
        "    strategy.save_optimizer(trainer.optimizer,\n",
        "                            \"sft_optim_checkpoint_%d.pt\" % (torch.cuda.current_device()),\n",
        "                            only_rank0=False)"
      ],
      "metadata": {
        "id": "VutB4LDdkdRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la ./output_1_sft"
      ],
      "metadata": {
        "id": "s1HboVoaCdMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rnWI0iWwLb1"
      },
      "outputs": [],
      "source": [
        "# Inference test.\n",
        "generator = transformers.pipeline(\"text-generation\", model=args.output_dir, tokenizer=tokenizer)\n",
        "\n",
        "generation_args = dict(\n",
        "    num_beams=4,\n",
        "    repetition_penalty=2.0,\n",
        "    no_repeat_ngram_size=4,\n",
        "    max_new_tokens=64,\n",
        "    do_sample=True,\n",
        "    top_k=30,\n",
        "    top_p=0.95,\n",
        "    temperature=1.9, \n",
        "    #max_length=300, \n",
        "    #num_return_sequences=20\n",
        "    early_stopping=True,\n",
        ")\n",
        "\n",
        "test_list = data_list[-5:]\n",
        "\n",
        "test_prompt_list = []\n",
        "actual_completion_list = []\n",
        "for row in test_list:\n",
        "    text_input = row\n",
        "    prompt = PROMPT_TEMPLATE.format_map(text_input)\n",
        "    test_prompt_list.append(prompt)\n",
        "    actual_completion_list.append(text_input[\"completion\"])\n",
        "\n",
        "result_list = generator(test_prompt_list, **generation_args)\n",
        "for prompt, result, actual_response in zip(test_prompt_list, result_list, actual_completion_list):\n",
        "    print(\"\")\n",
        "    print(\"-\" * 70)\n",
        "    print((\"completion: %s\" % (result[0][\"generated_text\"])))\n",
        "    print(f\"\\n### Actual answer:\\n{actual_response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JoJpv-wAvY6"
      },
      "source": [
        "# Step 2) RM: Reward Model\n",
        "Train Reward Model to generate the better answer by giving a reward to the better answer.\n",
        "- Dataset example\n",
        "```json\n",
        "[\n",
        "    {\n",
        "        \"prompt\": \"\",\n",
        "        \"completion_1\": \"\",\n",
        "        \"completion_2\": \"\",\n",
        "        \"completion_3\": \"\",            \n",
        "        \"ranking\": [1, 0, 2]\n",
        "    }, ...\n",
        "]\n",
        "```\n",
        "- Dataset sources\n",
        "  - [Dahoas/rm-static](https://huggingface.co/datasets/Dahoas/rm-static)\n",
        "  - [openai/webgpt_comparisons](https://huggingface.co/datasets/openai/webgpt_comparisons)\n",
        "  - [openai/summarize_from_feedback](https://huggingface.co/datasets/openai/summarize_from_feedback)\n",
        "  - [Dahoas/instruct-synthetic-prompt-responses](https://huggingface.co/datasets/Dahoas/synthetic-instruct-gptj-pairwise)\n",
        "\n",
        "- References\n",
        "    - [train_reward_model.py](https://github.com/hpcaitech/ColossalAI/blob/main/applications/Chat/examples/train_reward_model.py)\n",
        "    - [train_prompts.py](https://github.com/hpcaitech/ColossalAI/blob/main/applications/Chat/examples/train_prompts.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esYui6hPCuF-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, BloomTokenizerFast\n",
        "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
        "import loralib as lora\n",
        "\n",
        "from nextgpt.dataset import RewardDataset\n",
        "from nextgpt.models.base import RewardModel\n",
        "from nextgpt.models.bloom import BLOOMRM\n",
        "from nextgpt.models.gpt import GPTRM\n",
        "from nextgpt.models.opt import OPTRM\n",
        "from nextgpt.trainer import RewardModelTrainer\n",
        "from nextgpt.trainer.strategies import DDPStrategy, NaiveStrategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGnVyFvLSAjR"
      },
      "outputs": [],
      "source": [
        "# Define arguments.\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--output_dir\", type=str, default=\"./output_2_rm\")\n",
        "parser.add_argument(\"--strategy\",\n",
        "                    type=str, \n",
        "                    choices=[\"naive\", \"ddp\"],\n",
        "                    default=\"naive\")\n",
        "parser.add_argument(\"--model\", \n",
        "                    type=str, \n",
        "                    choices=[\"gpt2\", \"bloom\", \"opt\"], \n",
        "                    default=\"gpt2\")\n",
        "parser.add_argument(\"--pretrain\", type=str, default=\"gpt2\")\n",
        "parser.add_argument(\"--model_path\", type=str, default=None)\n",
        "parser.add_argument(\"--need_optim_ckpt\", type=bool, default=False)\n",
        "parser.add_argument(\"--max_epochs\", type=int, default=10)\n",
        "parser.add_argument(\"--batch_size\", type=int, default=4)\n",
        "parser.add_argument(\"--lora_rank\", type=int, default=0, help=\"low-rank adaptation matrices rank\")\n",
        "parser.add_argument(\"--loss_fn\", \n",
        "                    type=str, \n",
        "                    choices=[\"log_sig\", \"log_exp\"],\n",
        "                    default=\"log_sig\")\n",
        "parser.add_argument(\"--lr\", type=float, default=5e-5)\n",
        "parser.add_argument(\"--max_len\", type=int, default=512)\n",
        "\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "# For testing.\n",
        "args.max_epochs = 3\n",
        "args.pretrain = \"gpt2\" # pretrained initial model.\n",
        "args.verbose = True\n",
        "\n",
        "print(args)\n",
        "if not os.path.exists(args.output_dir):\n",
        "    os.makedirs(args.output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHEdwfvCwZTi"
      },
      "outputs": [],
      "source": [
        "# Configure strategy.\n",
        "if args.strategy == \"naive\":\n",
        "    strategy = NaiveStrategy()\n",
        "elif args.strategy == \"ddp\":\n",
        "    strategy = DDPStrategy()\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported strategy: {args.strategy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnR1YKD2Zuyo"
      },
      "outputs": [],
      "source": [
        "# Configure model.\n",
        "with strategy.model_init_context():\n",
        "    if args.model == \"gpt2\":\n",
        "        model = GPTRM(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
        "    elif args.model == \"bloom\":\n",
        "        model = BLOOMRM(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
        "    elif args.model == \"opt\":\n",
        "        model = OPTRM(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device()) \n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported model: {args.model}\")\n",
        "\n",
        "    # Load the supervised finetuning model state dict if it is specified.\n",
        "    # However, we will train the reward model from the initial language model instead of supervised finetuning model.\n",
        "    if args.model_path is not None:\n",
        "        state_dict = torch.load(args.model_path)\n",
        "        model.model.load_state_dict(state_dict)\n",
        "\n",
        "# This float16 or `model.half()` might cause loss NaN issue!!!\n",
        "# See:\n",
        "#   https://stackoverflow.com/questions/65332165/loss-is-nan-when-fine-tuning-huggingface-nli-model-both-roberta-bart\n",
        "#   https://github.com/huggingface/transformers/issues/9160\n",
        "# model = model.to(torch.float16)\n",
        "\n",
        "# Configure tokenizer.\n",
        "if args.model == \"gpt2\":\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        \"gpt2\", \n",
        "        # bos_token=\"<|startoftext|>\",\n",
        "        # eos_token=\"<|endoftext|>\", \n",
        "        # pad_token=\"<|pad|>\",\n",
        "        # padding_side=\"right\", \n",
        "        model_max_length=args.max_len,\n",
        "        )\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(tokenizer)\n",
        "    # model.resize_token_embeddings(len(tokenizer)) \n",
        "elif args.model == \"bloom\":\n",
        "    tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-560m\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "elif args.model == \"opt\":\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")  \n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported model: {args.model}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQU-3QqH65FE"
      },
      "outputs": [],
      "source": [
        "# Get the dataset.\n",
        "dataset_webgpt_comp = load_dataset(\"openai/webgpt_comparisons\", split=\"train[:20%]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2H18pYX7akp"
      },
      "outputs": [],
      "source": [
        "# Convert data into ranking format.\n",
        "data_list_ranking = []\n",
        "for row in dataset_webgpt_comp:\n",
        "    question = row[\"question\"][\"full_text\"]\n",
        "    answer_0 = row[\"answer_0\"]\n",
        "    answer_1 = row[\"answer_1\"]\n",
        "    score_0 = row[\"score_0\"]\n",
        "    score_1 = row[\"score_1\"]\n",
        "    if answer_0 == \"\" or answer_1 == \"\" or (score_0 == score_1):\n",
        "        continue\n",
        "\n",
        "    ranking = [0 if score_0 > score_1 else 1, 0 if score_0 < score_1 else 1]\n",
        "    data_list_ranking.append({\n",
        "        \"prompt\": PROMPT_TEMPLATE.format_map({\"instruction\": question}),\n",
        "        \"completion_0\": answer_0,\n",
        "        \"completion_1\": answer_1,\n",
        "        \"ranking\": ranking\n",
        "    })\n",
        "\n",
        "data_list_ranking[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeTW6fIE_c2L"
      },
      "outputs": [],
      "source": [
        "# Make ranking data to chosen, rejetced data for reward model dataset.\n",
        "total_data_ranking2chosen = []\n",
        "for tmp in data_list_ranking:\n",
        "    one_data_ranking2chosen = []\n",
        "\n",
        "    # data 1) 0 VS 1\n",
        "    data = {}\n",
        "    data[\"prompt\"] = tmp[\"prompt\"]\n",
        "    if tmp[\"ranking\"][0] < tmp[\"ranking\"][1]:\n",
        "        data[\"chosen\"] = tmp[\"completion_0\"]\n",
        "        data[\"rejected\"] = tmp[\"completion_1\"]\n",
        "    else:\n",
        "        data[\"chosen\"] = tmp[\"completion_1\"]\n",
        "        data[\"rejected\"] = tmp[\"completion_0\"]\n",
        "    one_data_ranking2chosen.append(data)\n",
        "\n",
        "    # # data 2) 0 VS 2\n",
        "    # data = {}\n",
        "    # data[\"prompt\"] = tmp[\"prompt\"]\n",
        "    # if tmp[\"ranking\"][0] < tmp[\"ranking\"][2]:\n",
        "    #     data[\"chosen\"] = tmp[\"completion_0\"]\n",
        "    #     data[\"rejected\"] = tmp[\"completion_2\"]\n",
        "    # else:\n",
        "    #     data[\"chosen\"] = tmp[\"completion_2\"]\n",
        "    #     data[\"rejected\"] = tmp[\"completion_0\"]\n",
        "    # one_data_ranking2chosen.append(data)\n",
        "\n",
        "    # # data 1) 1 VS 2\n",
        "    # data = {}\n",
        "    # data[\"prompt\"] = tmp[\"prompt\"]\n",
        "    # if tmp[\"ranking\"][1] < tmp[\"ranking\"][2]:\n",
        "    #     data[\"chosen\"] = tmp[\"completion_1\"]\n",
        "    #     data[\"rejected\"] = tmp[\"completion_2\"]\n",
        "    # else:\n",
        "    #     data[\"chosen\"] = tmp[\"completion_2\"]\n",
        "    #     data[\"rejected\"] = tmp[\"completion_1\"]\n",
        "    # one_data_ranking2chosen.append(data)\n",
        "\n",
        "\n",
        "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
        "\n",
        "\n",
        "print(\"before data num: %d\" % (len(data_list_ranking)))\n",
        "print(\"after data num: %d\" % (len(total_data_ranking2chosen)))\n",
        "print(\"data example: \\n%s\" % total_data_ranking2chosen[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1FpHo5jAf81"
      },
      "outputs": [],
      "source": [
        "# Prepare for data and dataset.\n",
        "import random\n",
        "random.seed(230319)\n",
        "\n",
        "random.shuffle(total_data_ranking2chosen)\n",
        "print(total_data_ranking2chosen[1])\n",
        "\n",
        "# train_data = total_data_ranking2chosen[:-1000]\n",
        "# eval_data = total_data_ranking2chosen[-1000:0]\n",
        "# We just select very small set of data for a quicker training.\n",
        "train_data = total_data_ranking2chosen[:100]\n",
        "val_data = total_data_ranking2chosen[100:130]\n",
        "eval_data = total_data_ranking2chosen[130:160]\n",
        "\n",
        "train_dataset = RewardDataset(train_data, tokenizer, args.max_len)\n",
        "val_dataset = RewardDataset(val_data, tokenizer, args.max_len)\n",
        "eval_dataset = RewardDataset(eval_data, tokenizer, args.max_len)\n",
        "\n",
        "# Check\n",
        "idx = 10\n",
        "print(\"#\" * 70)\n",
        "print(\"## prompt ##\")\n",
        "print(train_data[idx][\"prompt\"])\n",
        "print(\"#\" * 70)\n",
        "print(\"## chosen ##\")\n",
        "print(train_data[idx][\"chosen\"])\n",
        "print(\"#\" * 70)\n",
        "print(\"## rejected ##\")\n",
        "print(train_data[idx][\"rejected\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tC6Sf3qMX2A-"
      },
      "outputs": [],
      "source": [
        "trainer = RewardModelTrainer(model=model,\n",
        "                             strategy=strategy,\n",
        "                             train_dataset=train_dataset,\n",
        "                             valid_dataset=val_dataset,\n",
        "                             eval_dataset=eval_dataset,\n",
        "                             batch_size=args.batch_size,\n",
        "                             max_epochs=args.max_epochs,\n",
        "                             loss_fn=args.loss_fn,\n",
        "                             lr=args.lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJO46OBraGOy"
      },
      "outputs": [],
      "source": [
        "# Train!!!\n",
        "trainer.fit()\n",
        "\n",
        "# Save model checkpoint after fitting on only rank0.\n",
        "# strategy.save_model(model, os.path.join(args.output_dir, \"rm.pt\"), only_rank0=True)\n",
        "trainer.save_model(path=os.path.join(args.output_dir, \"rm.pt\"), only_rank0=True)\n",
        "\n",
        "# Save optimizer checkpoint on all ranks.\n",
        "if args.need_optim_ckpt:\n",
        "    strategy.save_optimizer(trainer.optimizer,\n",
        "                            os.path.join(args.output_dir, \"rm_optim_checkpoint_%d.pt\" % (torch.cuda.current_device())),\n",
        "                            only_rank0=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EI7qm5N262EZ"
      },
      "source": [
        "# Step 3) PPO: Proximal Policy Optimization\n",
        "Further fine-tune the LLM from step 1 with the reward model and this dataset using RL (eg. PPO).\n",
        "\n",
        "- References\n",
        "    - [train_prompts.py](https://github.com/hpcaitech/ColossalAI/blob/main/applications/Chat/examples/train_prompts.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fiD6OSKpOrc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import argparse\n",
        "from copy import deepcopy\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "from transformers import AutoTokenizer, BloomTokenizerFast\n",
        "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
        "\n",
        "from nextgpt.models.base import RewardModel\n",
        "from nextgpt.models.bloom import BLOOMActor, BLOOMCritic\n",
        "from nextgpt.models.gpt import GPTActor, GPTCritic\n",
        "from nextgpt.models.opt import OPTActor, OPTCritic\n",
        "from nextgpt.trainer import PPOTrainer\n",
        "from nextgpt.trainer.strategies import DDPStrategy, NaiveStrategy\n",
        "from nextgpt.dataset import PromptDataset, SupervisedDataset, DataCollatorForSupervisedDataset\n",
        "\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIs7m2m9pmWN"
      },
      "outputs": [],
      "source": [
        "# Define arguments.\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument(\"--output_dir\", type=str, default=\"./output_3_ppo\")\n",
        "parser.add_argument(\"--strategy\",\n",
        "                    type=str,\n",
        "                    choices=[\"naive\", \"ddp\"],\n",
        "                    default=\"naive\")\n",
        "parser.add_argument(\"--model\", \n",
        "                    type=str, \n",
        "                    choices=[\"gpt2\", \"bloom\", \"opt\"],\n",
        "                    default=\"gpt2\")\n",
        "parser.add_argument(\"--pretrain\", type=str, default=None)\n",
        "parser.add_argument(\"--rm_model\", \n",
        "                    type=str, \n",
        "                    choices=[\"gpt2\", \"bloom\", \"opt\"],\n",
        "                    default=\"gpt2\")\n",
        "parser.add_argument(\"--rm_path\", type=str, default=None)\n",
        "parser.add_argument(\"--rm_pretrain\", type=str, default=None)\n",
        "parser.add_argument(\"--need_optim_ckpt\", type=bool, default=False)\n",
        "parser.add_argument(\"--num_episodes\", type=int, default=10)\n",
        "parser.add_argument(\"--max_timesteps\", type=int, default=3)\n",
        "parser.add_argument(\"--update_timesteps\", type=int, default=3)\n",
        "parser.add_argument(\"--max_epochs\", type=int, default=5)\n",
        "parser.add_argument(\"--train_batch_size\", type=int, default=8)\n",
        "parser.add_argument(\"--ptx_batch_size\", type=int, default=1)\n",
        "parser.add_argument(\"--experience_batch_size\", type=int, default=8)\n",
        "parser.add_argument(\"--lora_rank\", type=int, default=0, help=\"low-rank adaptation matrices rank\")\n",
        "parser.add_argument(\"--kl_coef\", type=float, default=0.1)\n",
        "parser.add_argument(\"--ptx_coef\", type=float, default=0.9)\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "# For testing.\n",
        "# args.pretrain= \"gpt2\"\n",
        "args.pretrain= \"./output_1_sft\"\n",
        "args.rm_path = \"./output_2_rm/rm.pt\" # RM model path\n",
        "args.rm_pretrain= \"gpt2\"\n",
        "\n",
        "args.num_episodes = 1\n",
        "args.max_epochs = 1\n",
        "\n",
        "print(args)\n",
        "if not os.path.exists(args.output_dir):\n",
        "    os.makedirs(args.output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyHx9jXoq0yi"
      },
      "outputs": [],
      "source": [
        "# Configure strategy.\n",
        "if args.strategy == \"naive\":\n",
        "    strategy = NaiveStrategy()\n",
        "elif args.strategy == \"ddp\":\n",
        "    strategy = DDPStrategy()\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported strategy: {args.strategy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pH1wPr4Nq-bM"
      },
      "outputs": [],
      "source": [
        "if args.rm_path is not None:\n",
        "    rm_state_dict = torch.load(args.rm_path, map_location=\"cpu\")\n",
        "\n",
        "# Configure intial model.\n",
        "if args.model == \"gpt2\":\n",
        "    initial_model = GPTActor(pretrained=args.pretrain)\n",
        "elif args.model == \"bloom\":\n",
        "    initial_model = BLOOMActor(pretrained=args.pretrain)\n",
        "elif args.model == \"opt\":\n",
        "    initial_model = OPTActor(pretrained=args.pretrain)\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported actor model: {args.model}\")\n",
        "\n",
        "# Configure reward model.\n",
        "if args.rm_model == \"gpt2\":\n",
        "    reward_model = GPTRM(pretrained=args.rm_pretrain)\n",
        "elif args.rm_model == \"bloom\":\n",
        "    reward_model = BLOOMRM(pretrained=args.rm_pretrain)\n",
        "elif args.rm_model == \"opt\":\n",
        "    reward_model = OPTRM(pretrained=args.rm_pretrain)\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported reward model: {args.rm_model}\")\n",
        "\n",
        "if args.rm_path is not None:\n",
        "    reward_model.load_state_dict(rm_state_dict)\n",
        "\n",
        "# initial_model.to(torch.float16).to(torch.cuda.current_device())\n",
        "# reward_model.to(torch.float16).to(torch.cuda.current_device())\n",
        "initial_model.to(torch.cuda.current_device())\n",
        "reward_model.to(torch.cuda.current_device())\n",
        "\n",
        "# Configure actor and critic.\n",
        "with strategy.model_init_context():\n",
        "    # Actor\n",
        "    if args.model == \"gpt2\":\n",
        "        actor = GPTActor(pretrained=args.pretrain, lora_rank=args.lora_rank)\n",
        "    elif args.model == \"bloom\":\n",
        "        actor = BLOOMActor(pretrained=args.pretrain_actor, lora_rank=args.lora_rank)\n",
        "    elif args.model == \"opt\":\n",
        "        actor = OPTActor(pretrained=args.pretrain, lora_rank=args.lora_rank)        \n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported actor model: {args.model}\")\n",
        "\n",
        "    # Critic\n",
        "    if args.rm_model == \"gpt2\":\n",
        "        critic = GPTCritic(pretrained=args.rm_pretrain, lora_rank=args.lora_rank)\n",
        "    elif args.rm_model == \"bloom\":\n",
        "        critic = BLOOMCritic(pretrained=args.rm_pretrain, lora_rank=args.lora_rank)\n",
        "    elif args.rm_model == \"opt\":\n",
        "        critic = OPTCritic(pretrained=args.rm_pretrain, lora_rank=args.lora_rank)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported reward model: {args.rm_model}\")\n",
        "\n",
        "    if args.rm_path is not None:\n",
        "        critic.load_state_dict(rm_state_dict)\n",
        "        del rm_state_dict\n",
        "\n",
        "# critic.to(torch.float16).to(torch.cuda.current_device())\n",
        "# actor.to(torch.float16).to(torch.cuda.current_device())\n",
        "critic.to(torch.cuda.current_device())\n",
        "actor.to(torch.cuda.current_device())\n",
        "\n",
        "# Configure tokenizer.\n",
        "if args.model == \"gpt2\":\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\n",
        "        \"gpt2\", \n",
        "        # bos_token=\"<|startoftext|>\",\n",
        "        # eos_token=\"<|endoftext|>\", \n",
        "        # pad_token=\"<|pad|>\",\n",
        "        # padding_side=\"right\", \n",
        "        model_max_length=512,\n",
        "        )\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "elif args.model == \"bloom\":\n",
        "    tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-560m\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token            \n",
        "elif args.model == \"opt\":\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzDpPXr7rRTL"
      },
      "outputs": [],
      "source": [
        "def tokenize_fn(texts):\n",
        "    # MUST padding to max length to ensure inputs of all ranks have the same length\n",
        "    # Different length may lead to hang when using gemini, as different generation steps\n",
        "    batch = tokenizer(texts, return_tensors=\"pt\", max_length=96, padding=\"max_length\", truncation=True)\n",
        "    return {k: v.to(torch.cuda.current_device()) for k, v in batch.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEMpasbTrgzF"
      },
      "outputs": [],
      "source": [
        "# Prepare dataset.\n",
        "dataset_webgpt_comp = load_dataset(\"openai/webgpt_comparisons\", split=\"train[:20%]\")\n",
        "\n",
        "data_list = []\n",
        "for row in dataset_webgpt_comp:\n",
        "    question = row[\"question\"][\"full_text\"]\n",
        "    answer_0 = row[\"answer_0\"]\n",
        "    data_list.append({\n",
        "        \"instruction\": question,\n",
        "        \"completion\": answer_0\n",
        "    })\n",
        "\n",
        "print(data_list[:1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure dataloader.\n",
        "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
        "\n",
        "prompt_dataset = PromptDataset(\n",
        "    dataset=data_list, \n",
        "    tokenizer=tokenizer, \n",
        "    prompt_template=PROMPT_TEMPLATE, \n",
        "    max_datasets_size=10000)\n",
        "\n",
        "prompt_sampler = None\n",
        "if dist.is_initialized() and dist.get_world_size() > 1:\n",
        "    prompt_sampler = DistributedSampler(prompt_dataset, shuffle=True, seed=42, drop_last=True)\n",
        "\n",
        "prompt_dataloader = DataLoader(\n",
        "    prompt_dataset,\n",
        "    shuffle=(prompt_sampler is None),\n",
        "    sampler=prompt_sampler,\n",
        "    batch_size=args.train_batch_size)\n",
        "\n",
        "pretrain_dataset = SupervisedDataset(\n",
        "    dataset=data_list,\n",
        "    tokenizer=tokenizer, \n",
        "    prompt_template=PROMPT_TEMPLATE,\n",
        "    completion_field=\"completion\",\n",
        "    max_datasets_size=10000,\n",
        "    max_length=512,\n",
        "    verbose=True)\n",
        "\n",
        "pretrain_sampler = None\n",
        "if dist.is_initialized() and dist.get_world_size() > 1:\n",
        "    pretrain_sampler = DistributedSampler(pretrain_dataset, shuffle=True, seed=42, drop_last=True)\n",
        "\n",
        "pretrain_dataloader = DataLoader(\n",
        "    pretrain_dataset,\n",
        "    shuffle=(pretrain_sampler is None),\n",
        "    sampler=pretrain_sampler,\n",
        "    batch_size=args.ptx_batch_size,\n",
        "    collate_fn=data_collator)"
      ],
      "metadata": {
        "id": "VGmuTC7hIgWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MihLfe26u-4g"
      },
      "outputs": [],
      "source": [
        "# Configure trainer.\n",
        "trainer = PPOTrainer(\n",
        "    strategy,\n",
        "    actor,\n",
        "    critic,\n",
        "    reward_model,\n",
        "    initial_model,\n",
        "    kl_coef=args.kl_coef,\n",
        "    ptx_coef=args.ptx_coef,\n",
        "    max_epochs=args.max_epochs,\n",
        "    train_batch_size=args.train_batch_size,\n",
        "    experience_batch_size=args.experience_batch_size,\n",
        "    tokenizer=tokenize_fn,\n",
        "    max_length=128,\n",
        "    do_sample=True,\n",
        "    temperature=1.0,\n",
        "    top_k=50,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "trainer.fit(\n",
        "    prompt_dataloader=prompt_dataloader,\n",
        "    pretrain_dataloader=pretrain_dataloader,\n",
        "    num_episodes=args.num_episodes,\n",
        "    max_timesteps=args.max_timesteps,\n",
        "    update_timesteps=args.update_timesteps)\n",
        "\n",
        "# Save model checkpoint after fitting on only rank0.\n",
        "trainer.save_model(os.path.join(args.output_dir, \"actor.pt\"), only_rank0=True, tokenizer=tokenizer)\n",
        "# Save optimizer checkpoint on all ranks.\n",
        "strategy.save_optimizer(trainer.actor_optim,\n",
        "                        os.path.join(args.output_dir, \"actor_optim_checkpoint_%d.pt\" % (torch.cuda.current_device())),\n",
        "                        only_rank0=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecCC92TM7w3g"
      },
      "outputs": [],
      "source": [
        "#  Inference test.\n",
        "def generation(input_text):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(torch.cuda.current_device())\n",
        "    outputs = actor.generate(input_ids,\n",
        "                             max_length=100,\n",
        "                             do_sample=True,\n",
        "                             top_k=50,\n",
        "                             top_p=0.95,\n",
        "                             num_return_sequences=1)\n",
        "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
        "    print(\"#\" * 70)\n",
        "    print(output)\n",
        "    return output\n",
        "\n",
        "\n",
        "test_isntruction_list = [\n",
        "    \"Heterophobia is the irrational fear of what\",\n",
        "    ]\n",
        "\n",
        "test_prompt_list = [PROMPT_TEMPLATE.format_map({\"instruction\": tmp}) for tmp in test_isntruction_list]\n",
        "\n",
        "for input_text in test_prompt_list:\n",
        "    output = generation(input_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2Ju8fgt_is2"
      },
      "source": [
        "# Inference by PPO actor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOiwh9lTBSMS"
      },
      "outputs": [],
      "source": [
        "import argparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vd5AIx3nAJ35"
      },
      "outputs": [],
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--model\", \n",
        "                    type=str, \n",
        "                    choices=[\"gpt2\", \"bloom\", \"opt\"],\n",
        "                    default=\"gpt2\")\n",
        "# We suggest to use the pretrained model from HuggingFace, use pretrain to configure model\n",
        "parser.add_argument(\"--pretrain\", type=str, default=None)\n",
        "parser.add_argument(\"--model_path\", type=str, default=None)\n",
        "parser.add_argument(\"--input\", type=str, default=\"Question: How are you ? Answer:\")\n",
        "parser.add_argument(\"--max_length\", type=int, default=100)\n",
        "args = parser.parse_args([])\n",
        "\n",
        "# args.pretrain= \"gpt2\"\n",
        "args.pretrain= \"./output_1_sft\"\n",
        "args.model_path = \"./output_3_ppo/actor.pt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wg_mvSAAfZu"
      },
      "outputs": [],
      "source": [
        "def eval(args):\n",
        "    # Configure model.\n",
        "    if args.model == \"gpt2\":\n",
        "        actor = GPTActor(pretrained=args.pretrain).to(torch.cuda.current_device())\n",
        "    elif args.model == \"bloom\":\n",
        "        actor = BLOOMActor(pretrained=args.pretrain).to(torch.cuda.current_device())\n",
        "    elif args.model == \"opt\":\n",
        "        actor = OPTActor(pretrained=args.pretrain).to(torch.cuda.current_device())\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported model: {args.model}\")\n",
        "\n",
        "    state_dict = torch.load(args.model_path)\n",
        "    # actor.model.load_state_dict(state_dict)\n",
        "    actor.load_state_dict(state_dict)\n",
        "\n",
        "    # Configure tokenizer.\n",
        "    if args.model == \"gpt2\":\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    elif args.model == \"bloom\":\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\")\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    elif args.model == \"opt\":\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported model: {args.model}\")\n",
        "\n",
        "    actor.eval()\n",
        "    input = args.input\n",
        "    input_ids = tokenizer.encode(input, return_tensors=\"pt\").to(torch.cuda.current_device())\n",
        "    outputs = actor.generate(input_ids,\n",
        "                             max_length=args.max_length,\n",
        "                             do_sample=True,\n",
        "                             top_k=10,\n",
        "                             top_p=0.95,\n",
        "                             num_return_sequences=1)\n",
        "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
        "    print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VY7IyQs3B7Gw"
      },
      "outputs": [],
      "source": [
        "input_text = \"Heterophobia is the irrational fear of what?\"\n",
        "args.input = PROMPT_TEMPLATE.format_map({\"instruction\": input_text})\n",
        "eval(args)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}