{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/louiezzang/next-gpt/blob/main/examples/chatgpt_replica_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o5OIi7jDa9K"
      },
      "source": [
        "# chatGPT replica\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TMz3aiLECTk"
      },
      "source": [
        "What is RLHF? <br>\n",
        "See [this link](https://gist.github.com/JoaoLages/c6f2dfd13d2484aa8bb0b2d567fbf093).\n",
        "\n",
        "<br>\n",
        "\n",
        "**Example of RLHF dataset**:\n",
        "\n",
        "Total 3 datasets are needed for training the 3 steps(SFT, RM and PPO)\n",
        "- [Example of dataset](https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/chatllama#dataset-preparation)\n",
        "- [Example of dataset 1](https://huggingface.co/datasets/stanfordnlp/SHP)\n",
        "- [Example of dataset 2](https://huggingface.co/datasets/Anthropic/hh-rlhf)\n",
        "\n",
        "step1) Dataset for SFT(Supervised Fine-tuning training)\n",
        "```json\n",
        "[\n",
        "    {\n",
        "        \"prompt\": \"\",\n",
        "        \"completion\": \"\"        \n",
        "    }, ...\n",
        "]\n",
        "```\n",
        "\n",
        "step2) Dataset for RM(Reward Model) training: There are multiple completetions with human rated ranking score for one prompt.\n",
        "```json\n",
        "[\n",
        "    {\n",
        "        \"prompt\": \"\",\n",
        "        \"completion_1\": \"\",\n",
        "        \"completion_2\": \"\",\n",
        "        \"completion_3\": \"\",            \n",
        "        \"ranking\": [1, 0, 2]\n",
        "    }, ...\n",
        "]\n",
        "```\n",
        "    \n",
        "step3) Dataset for PPO(RLHF) training: It only consists of prompt.\n",
        "```json\n",
        "[\n",
        "    {\n",
        "        \"prompt\": \"\"\n",
        "    }, ...\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YG_5Fvkx9fH0"
      },
      "source": [
        "# Colab environment setup\n",
        "\n",
        "#### Installation (python>=3.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdbMmQE5Zsjw"
      },
      "outputs": [],
      "source": [
        "# Install next-gpt lib.\n",
        "!rm -rf ./next-gpt/\n",
        "!git clone https://github.com/louiezzang/next-gpt.git\n",
        "%cd next-gpt/\n",
        "!pip install .\n",
        "%cd ../"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JezJ8wz3_A7B"
      },
      "source": [
        "# Step 1) SFT: Surpervised Fine-tuning\n",
        "Build a Supervised Fine-tuning model to answer well to the question.\n",
        "\n",
        "- Refereneces\n",
        "  - [fine tuning code_1](https://github.com/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb)\n",
        "  - [fine tuning code_2](https://github.com/Beomi/KoAlpaca/blob/main/train.py)\n",
        "\n",
        "\n",
        "- SFT(Supervised Fine Tuning)\n",
        "- Fine-tune a pretrained LLM on a specific domain or corpus of instructions and human demonstrations\n",
        "\n",
        "- Dataset example\n",
        "```json\n",
        "[\n",
        "    {\n",
        "        \"prompt\": \"\",\n",
        "        \"completion\": \"\"        \n",
        "    }, ...\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpoPqaBfAkqW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "import json\n",
        "import yaml\n",
        "import argparse\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "import transformers\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoConfig, AutoModelForCausalLM, pipeline, \n",
        "    TrainingArguments, AutoModelWithLMHead,\n",
        "    ProgressCallback\n",
        ")\n",
        "from nextgpt.finetuning import (\n",
        "    SupervisedDataset, DataCollatorForSupervisedDataset,\n",
        "    SupervisedTrainer, LoggingCallback\n",
        ")\n",
        "\n",
        "PT_MODEL_NAME = \"gpt2\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define arguments.\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--model_name\", type=str, default=\"gpt2\", choices=[\"gpt2'\", \"bloom\", \"opt\"])\n",
        "parser.add_argument(\"--max_epochs\", type=int, default=1)\n",
        "parser.add_argument(\"--train_batch_size\", type=int, default=4)\n",
        "parser.add_argument(\"--output_dir\", type=str, default=\"./output_1_sft\")\n",
        "\n",
        "args = parser.parse_args(args=[])\n",
        "print(args)"
      ],
      "metadata": {
        "id": "FVXvBXyhPxwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9M1fZh_A80h"
      },
      "outputs": [],
      "source": [
        "# Get the tokenizer.\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(args.model_name, \n",
        "                                          bos_token=\"<|startoftext|>\",\n",
        "                                          eos_token=\"<|endoftext|>\", \n",
        "                                          pad_token=\"<|pad|>\")\n",
        "print(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vji8AkHflB0b"
      },
      "outputs": [],
      "source": [
        "dataset_webgpt_comp = load_dataset(\"openai/webgpt_comparisons\", split=\"train[:20%]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azsnYbQtmQ3j"
      },
      "outputs": [],
      "source": [
        "data_list = []\n",
        "for row in dataset_webgpt_comp:\n",
        "  question = row[\"question\"][\"full_text\"]\n",
        "  answer_0 = row[\"answer_0\"]\n",
        "  data_list.append({\n",
        "      \"prompt\": question,\n",
        "      \"completion\": answer_0\n",
        "  })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4bLuGNRlh9-"
      },
      "outputs": [],
      "source": [
        "PROMPT_TEMPLATE = (\n",
        "  \"Below is an instruction that describes a task, paired with an input that provides further context.\\n\\n\"\n",
        "  \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "  \"### Instruction:\\n{prompt}\\n\\n### Response:\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NhIhWL0lAVp"
      },
      "outputs": [],
      "source": [
        "dataset = SupervisedDataset(\n",
        "    data=data_list,\n",
        "    tokenizer=tokenizer, \n",
        "    prompt_template=PROMPT_TEMPLATE,\n",
        "    prompt_fields=[\"prompt\"], \n",
        "    completion_field=\"completion\",\n",
        "    verbose=True)\n",
        "\n",
        "# Split train and val dataset.\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, eval_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Data collator.\n",
        "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_9-GV97sf9O"
      },
      "outputs": [],
      "source": [
        "# Load the pretrained model.\n",
        "model = AutoModelForCausalLM.from_pretrained(args.model_name)\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRbskFS7sq84"
      },
      "outputs": [],
      "source": [
        "# Train arguments.\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./train_1_sft\", # the output directory\n",
        "    overwrite_output_dir=True, # overwrite the content of the output directory\n",
        "    num_train_epochs=args.max_epochs, # number of training epochs\n",
        "    per_device_train_batch_size=args.train_batch_size, # batch size for training\n",
        "    per_device_eval_batch_size=4, # batch size for evaluation\n",
        "    eval_steps=3, # number of update steps between two evaluations.\n",
        "    save_steps=100, # after # steps model is saved \n",
        "    warmup_steps=5, # number of warmup steps for learning rate scheduler\n",
        "    prediction_loss_only=True,\n",
        ")\n",
        "\n",
        "# Train the model.\n",
        "trainer = SupervisedTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=None,\n",
        "    # callbacks=[ProgressCallback, LoggingCallback(logger=None)],\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_state()\n",
        "trainer.safe_save_model(output_dir=args.output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhp5IFF0vQ-m"
      },
      "outputs": [],
      "source": [
        "# Inference test.\n",
        "generator = pipeline(\"text-generation\", model=args.output_dir, tokenizer=tokenizer)\n",
        "\n",
        "generation_args = dict(\n",
        "    num_beams=4,\n",
        "    repetition_penalty=2.0,\n",
        "    no_repeat_ngram_size=4,\n",
        "    # bos_token=\"<|startoftext|>\",\n",
        "    # eos_token=\"<|endoftext|>\", \n",
        "    # pad_token=\"<|pad|>\",\n",
        "    max_new_tokens=64,\n",
        "    do_sample=True,\n",
        "    top_k=30,\n",
        "    top_p=0.95,\n",
        "    temperature=1.9, \n",
        "    #max_length=300, \n",
        "    #num_return_sequences=20\n",
        "    early_stopping=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rnWI0iWwLb1"
      },
      "outputs": [],
      "source": [
        "test_list = data_list[-5:]\n",
        "\n",
        "test_prompt_list = []\n",
        "actual_completion_list = []\n",
        "for row in test_list:\n",
        "    text_input = row\n",
        "    prompt = prompt_template.format_map(text_input)\n",
        "    test_prompt_list.append(prompt)\n",
        "    actual_completion_list.append(text_input[\"completion\"])\n",
        "\n",
        "result_list = generator(test_prompt_list, **generation_args)\n",
        "for prompt, result, actual_response in zip(test_prompt_list, result_list, actual_completion_list):\n",
        "    print(\"\")\n",
        "    print(\"-\" * 70)\n",
        "    print((\"completion: %s\" % (result[0][\"generated_text\"])))\n",
        "    print(f\"\\n### Actual answer:\\n{actual_response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2) RM: Reward Model\n",
        "Train Reward Model to generate the better answer by giving a reward to the better answer.\n",
        "- Dataset example\n",
        "```json\n",
        "[\n",
        "    {\n",
        "        \"prompt\": \"\",\n",
        "        \"completion_1\": \"\",\n",
        "        \"completion_2\": \"\",\n",
        "        \"completion_3\": \"\",            \n",
        "        \"ranking\": [1, 0, 2]\n",
        "    }, ...\n",
        "]\n",
        "```\n",
        "- Dataset sources\n",
        "  - [Dahoas/rm-static](https://huggingface.co/datasets/Dahoas/rm-static)\n",
        "  - [openai/webgpt_comparisons](https://huggingface.co/datasets/openai/webgpt_comparisons)\n",
        "  - [openai/summarize_from_feedback](https://huggingface.co/datasets/openai/summarize_from_feedback)\n",
        "  - [Dahoas/instruct-synthetic-prompt-responses](https://huggingface.co/datasets/Dahoas/synthetic-instruct-gptj-pairwise)\n",
        "\n",
        "- References\n",
        "    - [train_reward_model.py](https://github.com/hpcaitech/ColossalAI/blob/main/applications/Chat/examples/train_reward_model.py)\n",
        "    - [train_prompts.py](https://github.com/hpcaitech/ColossalAI/blob/main/applications/Chat/examples/train_prompts.py)"
      ],
      "metadata": {
        "id": "8JoJpv-wAvY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, BloomTokenizerFast\n",
        "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
        "import loralib as lora\n",
        "\n",
        "from nextgpt.rlhf.dataset import RewardDataset\n",
        "from nextgpt.rlhf.models.base import RewardModel\n",
        "from nextgpt.rlhf.models.bloom import BLOOMRM\n",
        "from nextgpt.rlhf.models.gpt import GPTRM\n",
        "from nextgpt.rlhf.models.opt import OPTRM\n",
        "from nextgpt.rlhf.models import LogExpLoss, LogSigLoss\n",
        "from nextgpt.rlhf.trainer import RewardModelTrainer\n",
        "from nextgpt.rlhf.trainer.strategies import DDPStrategy, NaiveStrategy"
      ],
      "metadata": {
        "id": "esYui6hPCuF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define arguments.\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--output_dir\", type=str, default=\"/output_2_rm\")\n",
        "parser.add_argument(\"--strategy\",\n",
        "                    choices=[\"naive\", \"ddp\"],\n",
        "                    default=\"naive\")\n",
        "parser.add_argument(\"--model_name\", type=str, default=\"gpt2\", choices=[\"gpt2\", \"bloom\", \"opt\"])\n",
        "parser.add_argument(\"--pretrain\", type=str, default=\"gpt2\")\n",
        "parser.add_argument(\"--model_path\", type=str, default=None)\n",
        "parser.add_argument('--need_optim_ckpt', type=bool, default=False)\n",
        "parser.add_argument(\"--max_epochs\", type=int, default=10)\n",
        "parser.add_argument(\"--batch_size\", type=int, default=4)\n",
        "parser.add_argument(\"--lora_rank\", type=int, default=0, help=\"low-rank adaptation matrices rank\")\n",
        "parser.add_argument(\"--loss_fn\", type=str, default=\"log_sig\", choices=[\"log_sig\", \"log_exp\"])\n",
        "parser.add_argument(\"--max_len\", type=int, default=512)\n",
        "\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "# For test.\n",
        "args.max_epochs = 3\n",
        "args.pretrain = \"gpt2\" # pretrained initial model.\n",
        "args.max_len = 1024\n",
        "args.verbose = True\n",
        "\n",
        "print(args)\n",
        "if not os.path.exists(args.output_dir):\n",
        "    os.makedirs(args.output_dir)"
      ],
      "metadata": {
        "id": "WGnVyFvLSAjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure strategy.\n",
        "if args.strategy == \"naive\":\n",
        "    strategy = NaiveStrategy()\n",
        "elif args.strategy == \"ddp\":\n",
        "    strategy = DDPStrategy()\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported strategy: {args.strategy}\")"
      ],
      "metadata": {
        "id": "KHEdwfvCwZTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the dataset.\n",
        "dataset_webgpt_comp = load_dataset(\"openai/webgpt_comparisons\", split=\"train[:20%]\")"
      ],
      "metadata": {
        "id": "JQU-3QqH65FE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure model, tokenizer.\n",
        "with strategy.model_init_context():\n",
        "    # Load pretrained gpt2.\n",
        "    if args.model_name == \"gpt2\":\n",
        "        # Get the tokenizer.\n",
        "        tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "            args.model_name, \n",
        "            bos_token=\"<|startoftext|>\",\n",
        "            eos_token=\"<|endoftext|>\", \n",
        "            pad_token=\"<|pad|>\",\n",
        "            padding_side=\"right\", \n",
        "            model_max_length=args.max_len,\n",
        "            )\n",
        "        print(tokenizer)\n",
        "        model = GPTRM(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
        "        model.resize_token_embeddings(len(tokenizer)) \n",
        "    elif args.model == \"bloom\":\n",
        "        model = BLOOMRM(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
        "        tokenizer = BloomTokenizerFast.from_pretrained(args.pretrain)\n",
        "    elif args.model == \"opt\":\n",
        "        model = OPTRM(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")      \n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported model: {args.model_name}\")\n",
        "\n",
        "    # Load the supervised finetuning model state dict if it is specified.\n",
        "    # However, we will train the reward model from the initial language model instead of supervised finetuning model.\n",
        "    if args.model_path is not None:\n",
        "        # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        state_dict = torch.load(args.model_path)\n",
        "        model.model.load_state_dict(state_dict)\n",
        "\n",
        "    # This float16 or `model.half()` might cause loss NaN issue. See:\n",
        "    #   https://stackoverflow.com/questions/65332165/loss-is-nan-when-fine-tuning-huggingface-nli-model-both-roberta-bart\n",
        "    #   https://github.com/huggingface/transformers/issues/9160\n",
        "    # model = model.to(torch.float16)"
      ],
      "metadata": {
        "id": "jnR1YKD2Zuyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data into ranking format.\n",
        "data_list_ranking = []\n",
        "for row in dataset_webgpt_comp:\n",
        "  question = row[\"question\"][\"full_text\"]\n",
        "  answer_0 = row[\"answer_0\"]\n",
        "  answer_1 = row[\"answer_1\"]\n",
        "  score_0 = row[\"score_0\"]\n",
        "  score_1 = row[\"score_1\"]\n",
        "  if answer_0 == \"\" or answer_1 == \"\" or (score_0 == score_1):\n",
        "      continue\n",
        "\n",
        "  ranking = [0 if score_0 > score_1 else 1, 0 if score_0 < score_1 else 1]\n",
        "  data_list_ranking.append({\n",
        "      \"prompt\": PROMPT_TEMPLATE.format_map({\"prompt\": question}),\n",
        "      \"completion_0\": answer_0,\n",
        "      \"completion_1\": answer_1,\n",
        "      \"ranking\": ranking\n",
        "  })\n",
        "\n",
        "data_list_ranking[:2]"
      ],
      "metadata": {
        "id": "x2H18pYX7akp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make ranking data to chosen, rejetced data for reward model dataset.\n",
        "total_data_ranking2chosen = []\n",
        "for tmp in data_list_ranking:\n",
        "    one_data_ranking2chosen = []\n",
        "\n",
        "    # data 1) 0 VS 1\n",
        "    data = {}\n",
        "    data[\"prompt\"] = tmp[\"prompt\"]\n",
        "    if tmp[\"ranking\"][0] < tmp[\"ranking\"][1]:\n",
        "        data[\"chosen\"] = tmp[\"completion_0\"]\n",
        "        data[\"rejected\"] = tmp[\"completion_1\"]\n",
        "    else:\n",
        "        data[\"chosen\"] = tmp[\"completion_1\"]\n",
        "        data[\"rejected\"] = tmp[\"completion_0\"]\n",
        "    one_data_ranking2chosen.append(data)\n",
        "\n",
        "    # # data 2) 0 VS 2\n",
        "    # data = {}\n",
        "    # data[\"prompt\"] = tmp[\"prompt\"]\n",
        "    # if tmp[\"ranking\"][0] < tmp[\"ranking\"][2]:\n",
        "    #     data[\"chosen\"] = tmp[\"completion_0\"]\n",
        "    #     data[\"rejected\"] = tmp[\"completion_2\"]\n",
        "    # else:\n",
        "    #     data[\"chosen\"] = tmp[\"completion_2\"]\n",
        "    #     data[\"rejected\"] = tmp[\"completion_0\"]\n",
        "    # one_data_ranking2chosen.append(data)\n",
        "\n",
        "    # # data 1) 1 VS 2\n",
        "    # data = {}\n",
        "    # data[\"prompt\"] = tmp[\"prompt\"]\n",
        "    # if tmp[\"ranking\"][1] < tmp[\"ranking\"][2]:\n",
        "    #     data[\"chosen\"] = tmp[\"completion_1\"]\n",
        "    #     data[\"rejected\"] = tmp[\"completion_2\"]\n",
        "    # else:\n",
        "    #     data[\"chosen\"] = tmp[\"completion_2\"]\n",
        "    #     data[\"rejected\"] = tmp[\"completion_1\"]\n",
        "    # one_data_ranking2chosen.append(data)\n",
        "\n",
        "\n",
        "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
        "\n",
        "\n",
        "print(\"before data num: %d\" % (len(data_list_ranking)))\n",
        "print(\"after data num: %d\" % (len(total_data_ranking2chosen)))\n",
        "print(\"data example: \\n%s\" % total_data_ranking2chosen[1])"
      ],
      "metadata": {
        "id": "DeTW6fIE_c2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare for data and dataset.\n",
        "import random\n",
        "random.seed(230319)\n",
        "\n",
        "random.shuffle(total_data_ranking2chosen)\n",
        "print(total_data_ranking2chosen[1])\n",
        "\n",
        "# train_data = total_data_ranking2chosen[:-1000]\n",
        "# eval_data = total_data_ranking2chosen[-1000:0]\n",
        "# We just select very small set of data for a quicker training.\n",
        "train_data = total_data_ranking2chosen[:100]\n",
        "val_data = total_data_ranking2chosen[100:130]\n",
        "eval_data = total_data_ranking2chosen[130:160]\n",
        "\n",
        "train_dataset = RewardDataset(train_data, tokenizer, args.max_len)\n",
        "val_dataset = RewardDataset(val_data, tokenizer, args.max_len)\n",
        "eval_dataset = RewardDataset(eval_data, tokenizer, args.max_len)\n",
        "\n",
        "# Check\n",
        "idx = 10\n",
        "print(\"#\" * 70)\n",
        "print(\"## prompt ##\")\n",
        "print(train_data[idx][\"prompt\"])\n",
        "print(\"#\" * 70)\n",
        "print(\"## chosen ##\")\n",
        "print(train_data[idx][\"chosen\"])\n",
        "print(\"#\" * 70)\n",
        "print(\"## rejected ##\")\n",
        "print(train_data[idx][\"rejected\"])"
      ],
      "metadata": {
        "id": "o1FpHo5jAf81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure optimizer.\n",
        "optim = Adam(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Configure loss function.\n",
        "if args.loss_fn == \"log_sig\":\n",
        "    loss_fn = LogSigLoss()\n",
        "elif args.loss_fn == \"log_exp\":\n",
        "    loss_fn = LogExpLoss()\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported loss function: {args.loss_fn}\")"
      ],
      "metadata": {
        "id": "JqCnhM_SXw9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = RewardModelTrainer(model=model,\n",
        "                            strategy=strategy,\n",
        "                            optim=optim,\n",
        "                            loss_fn=loss_fn,\n",
        "                            train_dataset=train_dataset,\n",
        "                            valid_dataset=val_dataset,\n",
        "                            eval_dataset=eval_dataset,\n",
        "                            batch_size=args.batch_size,\n",
        "                            max_epochs=args.max_epochs)"
      ],
      "metadata": {
        "id": "tC6Sf3qMX2A-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train!!!\n",
        "trainer.fit()\n",
        "\n",
        "# Save model checkpoint after fitting on only rank0.\n",
        "# strategy.save_model(model, os.path.join(args.output_dir, \"rm.pt\"), only_rank0=True)\n",
        "trainer.save_model(path=os.path.join(args.output_dir, \"rm.pt\"), only_rank0=True, tokenizer=tokenizer)\n",
        "\n",
        "# Save optimizer checkpoint on all ranks.\n",
        "if args.need_optim_ckpt:\n",
        "    strategy.save_optimizer(trainer.optimizer,\n",
        "                            os.path.join(args.output_dir, \"rm_optim_checkpoint_%d.pt\" % (torch.cuda.current_device())),\n",
        "                            only_rank0=False)"
      ],
      "metadata": {
        "id": "WJO46OBraGOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3) PPO: Proximal Policy Optimization\n",
        "Further fine-tune the LLM from step 1 with the reward model and this dataset using RL (eg. PPO).\n",
        "\n",
        "- References\n",
        "    - [train_prompts.py](https://github.com/hpcaitech/ColossalAI/blob/main/applications/Chat/examples/train_prompts.py)"
      ],
      "metadata": {
        "id": "EI7qm5N262EZ"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}