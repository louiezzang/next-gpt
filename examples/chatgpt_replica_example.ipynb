{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/louiezzang/next-gpt/blob/main/examples/chatgpt_replica_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o5OIi7jDa9K"
      },
      "source": [
        "# chatGPT replica\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TMz3aiLECTk"
      },
      "source": [
        "What is RLHF? <br>\n",
        "See [this link](https://gist.github.com/JoaoLages/c6f2dfd13d2484aa8bb0b2d567fbf093).\n",
        "\n",
        "<br>\n",
        "\n",
        "**Example of RLHF dataset**:\n",
        "\n",
        "Total 3 datasets are needed for training the 3 steps(SFT, RM and PPO)\n",
        "- [Example of dataset](https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/chatllama#dataset-preparation)\n",
        "- [Example of dataset 1](https://huggingface.co/datasets/stanfordnlp/SHP)\n",
        "- [Example of dataset 2](https://huggingface.co/datasets/Anthropic/hh-rlhf)\n",
        "\n",
        "step1) Dataset for SFT(Supervised Fine-tuning training)\n",
        "```json\n",
        "[\n",
        "    {\n",
        "        \"prompt\": \"\",\n",
        "        \"completion\": \"\"        \n",
        "    }, ...\n",
        "]\n",
        "```\n",
        "\n",
        "step2) Dataset for RM(Reward Model) training: There are multiple completetions with human rated ranking score for one prompt.\n",
        "```json\n",
        "[\n",
        "    {\n",
        "        \"prompt\": \"\",\n",
        "        \"completion_1\": \"\",\n",
        "        \"completion_2\": \"\",\n",
        "        \"completion_3\": \"\",            \n",
        "        \"ranking\": [1, 0, 2]\n",
        "    }, ...\n",
        "]\n",
        "```\n",
        "    \n",
        "step3) Dataset for PPO(RLHF) training: It only consists of prompt.\n",
        "```json\n",
        "[\n",
        "    {\n",
        "        \"prompt\": \"\"\n",
        "    }, ...\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YG_5Fvkx9fH0"
      },
      "source": [
        "# Colab environment setup\n",
        "\n",
        "#### Installation (python>=3.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdbMmQE5Zsjw",
        "outputId": "99b2f4cf-8edb-4de5-e01a-81a85eef82ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'next-gpt'...\n",
            "remote: Enumerating objects: 568, done.\u001b[K\n",
            "remote: Counting objects: 100% (220/220), done.\u001b[K\n",
            "remote: Compressing objects: 100% (134/134), done.\u001b[K\n",
            "remote: Total 568 (delta 106), reused 146 (delta 68), pack-reused 348\u001b[K\n",
            "Receiving objects: 100% (568/568), 197.32 KiB | 7.89 MiB/s, done.\n",
            "Resolving deltas: 100% (274/274), done.\n",
            "/content/next-gpt\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/next-gpt\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from next-gpt==1.0.0) (4.65.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (from next-gpt==1.0.0) (4.27.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (from next-gpt==1.0.0) (2.11.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.9/dist-packages (from next-gpt==1.0.0) (0.3.3)\n",
            "Requirement already satisfied: loralib in /usr/local/lib/python3.9/dist-packages (from next-gpt==1.0.0) (0.1.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.9/dist-packages (from next-gpt==1.0.0) (0.0.133)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets->next-gpt==1.0.0) (23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets->next-gpt==1.0.0) (1.22.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets->next-gpt==1.0.0) (3.8.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from datasets->next-gpt==1.0.0) (0.13.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets->next-gpt==1.0.0) (1.4.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets->next-gpt==1.0.0) (9.0.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets->next-gpt==1.0.0) (3.2.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets->next-gpt==1.0.0) (2023.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets->next-gpt==1.0.0) (6.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets->next-gpt==1.0.0) (0.3.6)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets->next-gpt==1.0.0) (0.70.14)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets->next-gpt==1.0.0) (2.27.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets->next-gpt==1.0.0) (0.18.0)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.9/dist-packages (from langchain->next-gpt==1.0.0) (1.2.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain->next-gpt==1.0.0) (1.10.7)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.9/dist-packages (from langchain->next-gpt==1.0.0) (0.5.7)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.9/dist-packages (from langchain->next-gpt==1.0.0) (8.2.2)\n",
            "Requirement already satisfied: SQLAlchemy<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain->next-gpt==1.0.0) (1.4.47)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.9/dist-packages (from tiktoken->next-gpt==1.0.0) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers->next-gpt==1.0.0) (0.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers->next-gpt==1.0.0) (3.10.7)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->next-gpt==1.0.0) (22.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->next-gpt==1.0.0) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->next-gpt==1.0.0) (1.3.3)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->next-gpt==1.0.0) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->next-gpt==1.0.0) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->next-gpt==1.0.0) (1.8.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->next-gpt==1.0.0) (4.0.2)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->next-gpt==1.0.0) (3.19.0)\n",
            "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->next-gpt==1.0.0) (1.5.1)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->next-gpt==1.0.0) (0.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets->next-gpt==1.0.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets->next-gpt==1.0.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets->next-gpt==1.0.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets->next-gpt==1.0.0) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from SQLAlchemy<2,>=1->langchain->next-gpt==1.0.0) (2.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets->next-gpt==1.0.0) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets->next-gpt==1.0.0) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->next-gpt==1.0.0) (1.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain->next-gpt==1.0.0) (1.0.0)\n",
            "Building wheels for collected packages: next-gpt\n",
            "  Building wheel for next-gpt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for next-gpt: filename=next_gpt-1.0.0-py3-none-any.whl size=58894 sha256=e4051b55711873734d036593e72db357ae760bc787fed2010b17089626302823\n",
            "  Stored in directory: /root/.cache/pip/wheels/f4/0d/a2/307ec9a6214260bfd63facee827d7bbef5c1ba9618277ea1b5\n",
            "Successfully built next-gpt\n",
            "Installing collected packages: next-gpt\n",
            "  Attempting uninstall: next-gpt\n",
            "    Found existing installation: next-gpt 1.0.0\n",
            "    Uninstalling next-gpt-1.0.0:\n",
            "      Successfully uninstalled next-gpt-1.0.0\n",
            "Successfully installed next-gpt-1.0.0\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# Install next-gpt lib.\n",
        "!rm -rf ./next-gpt/\n",
        "!git clone https://github.com/louiezzang/next-gpt.git\n",
        "%cd next-gpt/\n",
        "!pip install .\n",
        "%cd ../"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JezJ8wz3_A7B"
      },
      "source": [
        "# Step 1) SFT: Surpervised Fine-tuning\n",
        "Build a Supervised Fine-tuning model to answer well to the question.\n",
        "\n",
        "- Refereneces\n",
        "  - [fine tuning code_1](https://github.com/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb)\n",
        "  - [fine tuning code_2](https://github.com/Beomi/KoAlpaca/blob/main/train.py)\n",
        "\n",
        "\n",
        "- SFT(Supervised Fine Tuning)\n",
        "- Fine-tune a pretrained LLM on a specific domain or corpus of instructions and human demonstrations\n",
        "\n",
        "- Dataset example\n",
        "```json\n",
        "[\n",
        "    {\n",
        "        \"prompt\": \"\",\n",
        "        \"completion\": \"\"        \n",
        "    }, ...\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fpoPqaBfAkqW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "import json\n",
        "import yaml\n",
        "import argparse\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "import transformers\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoConfig, AutoModelForCausalLM, pipeline, \n",
        "    TrainingArguments, AutoModelWithLMHead,\n",
        "    ProgressCallback\n",
        ")\n",
        "from nextgpt.finetuning import (\n",
        "    SupervisedDataset, DataCollatorForSupervisedDataset,\n",
        "    SupervisedTrainer, LoggingCallback\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define arguments.\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--model\", type=str, default=\"gpt2\", choices=[\"gpt2'\", \"bloom\", \"opt\"])\n",
        "parser.add_argument(\"--max_epochs\", type=int, default=1)\n",
        "parser.add_argument(\"--train_batch_size\", type=int, default=4)\n",
        "parser.add_argument(\"--output_dir\", type=str, default=\"./output_1_sft\")\n",
        "\n",
        "args = parser.parse_args(args=[])\n",
        "print(args)"
      ],
      "metadata": {
        "id": "NLJdNVzEsdJT",
        "outputId": "ee23d833-4a43-446f-c01c-58f0e3b7f094",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(model='gpt2', max_epochs=1, train_batch_size=4, output_dir='./output_1_sft')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9M1fZh_A80h",
        "outputId": "ed2c1be3-de50-4e7e-98dc-825d39175e22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|pad|>'})\n"
          ]
        }
      ],
      "source": [
        "# Get the tokenizer.\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(args.model, \n",
        "                                          bos_token=\"<|startoftext|>\",\n",
        "                                          eos_token=\"<|endoftext|>\", \n",
        "                                          pad_token=\"<|pad|>\")\n",
        "print(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vji8AkHflB0b",
        "outputId": "cdb995e4-66ac-4b05-8a4e-dba309cea27b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset webgpt_comparisons (/root/.cache/huggingface/datasets/openai___webgpt_comparisons/default/0.0.0/8b5d5879cdc98c4c0099af6053dffe8d504588d43d3b11f1b1ec223ab1e8db0a)\n"
          ]
        }
      ],
      "source": [
        "dataset_webgpt_comp = load_dataset(\"openai/webgpt_comparisons\", split=\"train[:20%]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "azsnYbQtmQ3j"
      },
      "outputs": [],
      "source": [
        "data_list = []\n",
        "for row in dataset_webgpt_comp:\n",
        "  question = row[\"question\"][\"full_text\"]\n",
        "  answer_0 = row[\"answer_0\"]\n",
        "  data_list.append({\n",
        "      \"prompt\": question,\n",
        "      \"completion\": answer_0\n",
        "  })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "i4bLuGNRlh9-"
      },
      "outputs": [],
      "source": [
        "PROMPT_TEMPLATE = (\n",
        "  \"Below is an instruction that describes a task, paired with an input that provides further context.\\n\\n\"\n",
        "  \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "  \"### Instruction:\\n{prompt}\\n\\n### Response:\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NhIhWL0lAVp",
        "outputId": "0487ed39-b336-40d3-d2b5-323f53187563"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3916/3916 [00:00<00:00, 428356.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task, paired with an input that provides further context.\n",
            "\n",
            "Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Voiced by Harry Shearer, what Simpsons character was modeled after Ted Koppel?\n",
            "\n",
            "### Response:\n",
            "The Simpsons character that was possibly based on Ted Koppel is Kent Brockman.  He is a local news anchor in Springfield and is modeled after Ted Koppel. [1]<|endoftext|>\n",
            "Tokenizing inputs... This may take some time...\n",
            "Loading data done!!: 3916\n"
          ]
        }
      ],
      "source": [
        "dataset = SupervisedDataset(\n",
        "    data=data_list,\n",
        "    tokenizer=tokenizer, \n",
        "    prompt_template=PROMPT_TEMPLATE,\n",
        "    completion_field=\"completion\",\n",
        "    verbose=True)\n",
        "\n",
        "# Split train and val dataset.\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, eval_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Data collator.\n",
        "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_9-GV97sf9O",
        "outputId": "6858699e-d417-4635-9713-8680a5124a79"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(50259, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Load the pretrained model.\n",
        "model = AutoModelForCausalLM.from_pretrained(args.model)\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "XRbskFS7sq84",
        "outputId": "a283bfd4-868b-41d7-9c3b-ef8ffbcb48fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='783' max='783' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [783/783 01:40, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>3.891300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Train arguments.\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./train_1_sft\", # the output directory\n",
        "    overwrite_output_dir=True, # overwrite the content of the output directory\n",
        "    num_train_epochs=args.max_epochs, # number of training epochs\n",
        "    per_device_train_batch_size=args.train_batch_size, # batch size for training\n",
        "    per_device_eval_batch_size=4, # batch size for evaluation\n",
        "    eval_steps=3, # number of update steps between two evaluations.\n",
        "    save_steps=100, # after # steps model is saved \n",
        "    warmup_steps=5, # number of warmup steps for learning rate scheduler\n",
        "    prediction_loss_only=True,\n",
        ")\n",
        "\n",
        "# Train the model.\n",
        "trainer = SupervisedTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=None,\n",
        "    # callbacks=[ProgressCallback, LoggingCallback(logger=None)],\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_state()\n",
        "trainer.safe_save_model(output_dir=args.output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fhp5IFF0vQ-m"
      },
      "outputs": [],
      "source": [
        "# Inference test.\n",
        "generator = pipeline(\"text-generation\", model=args.output_dir, tokenizer=tokenizer)\n",
        "\n",
        "generation_args = dict(\n",
        "    num_beams=4,\n",
        "    repetition_penalty=2.0,\n",
        "    no_repeat_ngram_size=4,\n",
        "    # bos_token=\"<|startoftext|>\",\n",
        "    # eos_token=\"<|endoftext|>\", \n",
        "    # pad_token=\"<|pad|>\",\n",
        "    max_new_tokens=64,\n",
        "    do_sample=True,\n",
        "    top_k=30,\n",
        "    top_p=0.95,\n",
        "    temperature=1.9, \n",
        "    #max_length=300, \n",
        "    #num_return_sequences=20\n",
        "    early_stopping=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8rnWI0iWwLb1",
        "outputId": "cc6beabb-cc1e-48a7-fa00-73003629fed9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1201: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----------------------------------------------------------------------\n",
            "completion: Below is an instruction that describes a task, paired with an input that provides further context.\n",
            "\n",
            "Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "I've noticed when scanning my files for malware/viruses, the \"number of files scanned\" that pops up is almost always greater than the number of files I selected to scan. What is actually being scanned and why is it considered different files?\n",
            "\n",
            "### Response:In most cases there is no way to change the number of downloads on your computer without changing the software used to download the files. However, once you have installed the operating system, you can change this by installing a new operating system, such as Microsoft Windows 10, or installing a third party software. For example, if\n",
            "\n",
            "### Actual answer:\n",
            "Microsoft Defender Antivirus has multiple layers of protection to catch malware and viruses. These include quick scans, full scans, and on-access protection with cloud-delivered protection [1,2,3]. A quick scan checks the processes, memory, profiles, and certain locations on the device [1]. Real-time protection reviews files when they are opened and closed and whenever a user navigates to a folder [1,2]. On-access protection with cloud-delivered protection helps ensure that all the files accessed on the system are being scanned with the latest security intelligence and cloud machine learning models [3]. A full scan detects malware that was not detected by other scans, but it can take a while and use valuable system resources to complete [3]. It can also take longer to complete if the device is offline for an extended period of time [3].\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "completion: Below is an instruction that describes a task, paired with an input that provides further context.\n",
            "\n",
            "Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "How the Obama talk, which has 159,313 upvotes, is not higher all-time than \"Test Post Please Ignore\" with 26,753 upvotes.\n",
            "\n",
            "I'm assuming the difference is that the Reddit system accounts and tries to correct massive upvotes, but why is it being so much harsher to the Obama AMA? Shouldn't it level out over time, not decrease?\n",
            "\n",
            "### Response:There are two major factors that determine whether or not a politician gets elected in the Electoral College [1]. First is the length of the presidential campaign [2]. A president can only win a popular vote if he or she receives more than one million votes [3]. The second factor is the percentage of the electoral college divided\n",
            "\n",
            "### Actual answer:\n",
            "Reddit uses a story algorithm to rank posts. This means that the number of votes and the time a link has been posted have the biggest impact on where a story will rank. Additionally, Reddit also ranks items by the number of votes they accumulate, as well as the age of the post compared to others. [1, 3]\n",
            " sulphReddit also uses a logarithm function in its algorithm. This means that after the first few minutes of the post going live, the initial reactions are crucial to its future survival. If the post instantly achieves upvotes right after posting, more users will be likely to see the post. If it happens later, then the relevance of the post is diminished, so reactions hold weight. [2, 3] This means that newer posts generally rank higher than older posts. This keeps the front page fresh, and ensures that links with thousands of up-votes aren’t stuck on the front page for weeks or months at a time. Stories that get a more equal range of up-votes and down-votes will generally be ranked lower. [3]\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "completion: Below is an instruction that describes a task, paired with an input that provides further context.\n",
            "\n",
            "Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "if micro- and macro-physics equations are incompatible, at roughly what size does each start being descriptive? Is there overlap, and if not, are there \"in-between\" sizes that could be studied?\n",
            "\n",
            "Also, aside from string theory, what efforts are currently being made to bridge the gap?\n",
            "\n",
            "### Response:Microscopic models of the periodic table [2], such as the Earth's magnetic field, have been proposed for many years. However, they do not fit into our understanding of how the solar system was constructed [2]. The simplest possible explanation for this is that the sun had no atmosphere, but instead carried energy from\n",
            "\n",
            "### Actual answer:\n",
            "The laws of physics are considered fundamental, although many of them refer to idealized or theoretical systems that are hard to replicate in the real world [3]. The behavior of particles in the micro-physical world is so unpredictable from the known laws that operate in the macro-physical world that Niels Bohr reputedly said, “Anyone who is not shocked by quantum mechanics has not really understood it” [1]. Schrodinger – a pioneer in quantum mechanics - reputedly said, “I don’t like it and I wish I had had nothing to do with it” [1].\n",
            "\n",
            "String theory and quantum gravitation are attempts to reconcile the two – among them string theory and quantum gravitation [2]. In fact, the marriage of quantum mechanics – the physics of the very small – with general relativity – the physics of the very large – is believed by some to be the crucial step in formulating a general ‘theory of everything’ that will hopefully contain all the basic laws of nature in one package [5]. Quantum mechanics has had extraordinary successes in explaining everything from the state of the universe immediately after the big bang, to the structure of DNA, to the colour of your socks [6].\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "completion: Below is an instruction that describes a task, paired with an input that provides further context.\n",
            "\n",
            "Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "- You know how you can flick a switch on a rearview mirror and it reduces glare while still being reflective? How does that work?\n",
            "\n",
            "### Response:A light emitting surface (IR) emits light from its outermost layer [2]. It then travels through the air and bounces off of surfaces [2]. The reflected light is then reflected by the lens of your car or other body part [2]. When you turn the light on in front of your car, the reflection\n",
            "\n",
            "### Actual answer:\n",
            "Rearview mirrors have two reflective surfaces, a regular mirror on the back, and a glass wedge in front of it that reflects only about four percent of the incoming light [1]. When you flip the switch on the bottom of the mirror, the glass wedge moves, changing the way light passes through it and how it's reflected [2][3]. In daytime driving mode, the back surface of the mirror reflects light and images [3]. When you flip the switch and change the orientation of the mirror glass, the front section is responsible for what you see [3]. Because the light and images must first travel through the back side of the glass before hitting the front and bouncing back to you, the image is dimmer and the glare of headlights behind you is greatly reduced [3][4].\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "completion: Below is an instruction that describes a task, paired with an input that provides further context.\n",
            "\n",
            "Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "How does negaive mass work in physics?\n",
            "\n",
            "I saw the article below and wondered how it could be possible to have a negative mass. \n",
            "\n",
            "https://www.reddit.com/r/space/comments/a3a33c/scientists_may_have_solved_one_of_the_biggest/?utm_source=reddit-android\n",
            "\n",
            "How will it behave in a classical situation? How would gravity interact with a negative mass? \n",
            "\n",
            "### Response:The gravitational force of a positively massed object is said to be constant [1]. The gravitational forces on a positive massed object are determined by the mass of the object, so they must be exactly zero [1]. This means that if the weight of the object was approximately zero, then the energy of the object would\n",
            "\n",
            "### Actual answer:\n",
            "In theoretical physics, negative mass is a type of matter whose mass is of opposite sign to the mass of normal matter [1]. The mass of normal matter is defined as positive mass [1] so therefore negative mass is mass that is actually negative. To create negative mass, the research team at Washington State University cooled rubidium atoms to just a hair above absolute zero, creating what is known as a Bose-Einstein condensate [3]. In this state, particles move extremely slowly and, following the principles of quantum mechanics, behave like waves [3]. They also synchronize and move in unison as what is known as a superfluid, which flows without losing energy [3]. To create the negative mass, the researchers applied a second set of lasers that kicked the atoms back and forth and changed the way they spin [3].\n"
          ]
        }
      ],
      "source": [
        "test_list = data_list[-5:]\n",
        "\n",
        "test_prompt_list = []\n",
        "actual_completion_list = []\n",
        "for row in test_list:\n",
        "    text_input = row\n",
        "    prompt = PROMPT_TEMPLATE.format_map(text_input)\n",
        "    test_prompt_list.append(prompt)\n",
        "    actual_completion_list.append(text_input[\"completion\"])\n",
        "\n",
        "result_list = generator(test_prompt_list, **generation_args)\n",
        "for prompt, result, actual_response in zip(test_prompt_list, result_list, actual_completion_list):\n",
        "    print(\"\")\n",
        "    print(\"-\" * 70)\n",
        "    print((\"completion: %s\" % (result[0][\"generated_text\"])))\n",
        "    print(f\"\\n### Actual answer:\\n{actual_response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2) RM: Reward Model\n",
        "Train Reward Model to generate the better answer by giving a reward to the better answer.\n",
        "- Dataset example\n",
        "```json\n",
        "[\n",
        "    {\n",
        "        \"prompt\": \"\",\n",
        "        \"completion_1\": \"\",\n",
        "        \"completion_2\": \"\",\n",
        "        \"completion_3\": \"\",            \n",
        "        \"ranking\": [1, 0, 2]\n",
        "    }, ...\n",
        "]\n",
        "```\n",
        "- Dataset sources\n",
        "  - [Dahoas/rm-static](https://huggingface.co/datasets/Dahoas/rm-static)\n",
        "  - [openai/webgpt_comparisons](https://huggingface.co/datasets/openai/webgpt_comparisons)\n",
        "  - [openai/summarize_from_feedback](https://huggingface.co/datasets/openai/summarize_from_feedback)\n",
        "  - [Dahoas/instruct-synthetic-prompt-responses](https://huggingface.co/datasets/Dahoas/synthetic-instruct-gptj-pairwise)\n",
        "\n",
        "- References\n",
        "    - [train_reward_model.py](https://github.com/hpcaitech/ColossalAI/blob/main/applications/Chat/examples/train_reward_model.py)\n",
        "    - [train_prompts.py](https://github.com/hpcaitech/ColossalAI/blob/main/applications/Chat/examples/train_prompts.py)"
      ],
      "metadata": {
        "id": "8JoJpv-wAvY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, BloomTokenizerFast\n",
        "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
        "import loralib as lora\n",
        "\n",
        "from nextgpt.rlhf.dataset import RewardDataset\n",
        "from nextgpt.rlhf.models.base import RewardModel\n",
        "from nextgpt.rlhf.models.bloom import BLOOMRM\n",
        "from nextgpt.rlhf.models.gpt import GPTRM\n",
        "from nextgpt.rlhf.models.opt import OPTRM\n",
        "from nextgpt.rlhf.models import LogExpLoss, LogSigLoss\n",
        "from nextgpt.rlhf.trainer import RewardModelTrainer\n",
        "from nextgpt.rlhf.trainer.strategies import DDPStrategy, NaiveStrategy"
      ],
      "metadata": {
        "id": "esYui6hPCuF-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define arguments.\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--output_dir\", type=str, default=\"./output_2_rm\")\n",
        "parser.add_argument(\"--strategy\",\n",
        "                    choices=[\"naive\", \"ddp\"],\n",
        "                    default=\"naive\")\n",
        "parser.add_argument(\"--model\", type=str, default=\"gpt2\", choices=[\"gpt2\", \"bloom\", \"opt\"])\n",
        "parser.add_argument(\"--pretrain\", type=str, default=\"gpt2\")\n",
        "parser.add_argument(\"--model_path\", type=str, default=None)\n",
        "parser.add_argument('--need_optim_ckpt', type=bool, default=False)\n",
        "parser.add_argument(\"--max_epochs\", type=int, default=10)\n",
        "parser.add_argument(\"--batch_size\", type=int, default=4)\n",
        "parser.add_argument(\"--lora_rank\", type=int, default=0, help=\"low-rank adaptation matrices rank\")\n",
        "parser.add_argument(\"--loss_fn\", type=str, default=\"log_sig\", choices=[\"log_sig\", \"log_exp\"])\n",
        "parser.add_argument(\"--max_len\", type=int, default=512)\n",
        "\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "# For test.\n",
        "args.max_epochs = 3\n",
        "args.pretrain = \"gpt2\" # pretrained initial model.\n",
        "args.max_len = 1024\n",
        "args.verbose = True\n",
        "\n",
        "print(args)\n",
        "if not os.path.exists(args.output_dir):\n",
        "    os.makedirs(args.output_dir)"
      ],
      "metadata": {
        "id": "WGnVyFvLSAjR",
        "outputId": "a9b1d67d-0a00-4930-aaef-35af8da4cb90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(output_dir='./output_2_rm', strategy='naive', model='gpt2', pretrain='gpt2', model_path=None, need_optim_ckpt=False, max_epochs=3, batch_size=4, lora_rank=0, loss_fn='log_sig', max_len=1024, verbose=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure strategy.\n",
        "if args.strategy == \"naive\":\n",
        "    strategy = NaiveStrategy()\n",
        "elif args.strategy == \"ddp\":\n",
        "    strategy = DDPStrategy()\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported strategy: {args.strategy}\")"
      ],
      "metadata": {
        "id": "KHEdwfvCwZTi"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure model, tokenizer.\n",
        "with strategy.model_init_context():\n",
        "    # Load pretrained gpt2.\n",
        "    if args.model == \"gpt2\":\n",
        "        # Get the tokenizer.\n",
        "        tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "            args.model, \n",
        "            bos_token=\"<|startoftext|>\",\n",
        "            eos_token=\"<|endoftext|>\", \n",
        "            pad_token=\"<|pad|>\",\n",
        "            padding_side=\"right\", \n",
        "            model_max_length=args.max_len,\n",
        "            )\n",
        "        print(tokenizer)\n",
        "        model = GPTRM(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
        "        model.resize_token_embeddings(len(tokenizer)) \n",
        "    elif args.model == \"bloom\":\n",
        "        model = BLOOMRM(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
        "        tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-560m\")\n",
        "    elif args.model == \"opt\":\n",
        "        model = OPTRM(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")      \n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported model: {args.model}\")\n",
        "\n",
        "    # Load the supervised finetuning model state dict if it is specified.\n",
        "    # However, we will train the reward model from the initial language model instead of supervised finetuning model.\n",
        "    if args.model_path is not None:\n",
        "        # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        state_dict = torch.load(args.model_path)\n",
        "        model.model.load_state_dict(state_dict)\n",
        "\n",
        "    # This float16 or `model.half()` might cause loss NaN issue!!!\n",
        "    # See:\n",
        "    #   https://stackoverflow.com/questions/65332165/loss-is-nan-when-fine-tuning-huggingface-nli-model-both-roberta-bart\n",
        "    #   https://github.com/huggingface/transformers/issues/9160\n",
        "    model = model.to(torch.float16)"
      ],
      "metadata": {
        "id": "jnR1YKD2Zuyo",
        "outputId": "dbe8e232-dcdc-44ad-eb7b-a09b7a36f62b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|pad|>'})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the dataset.\n",
        "dataset_webgpt_comp = load_dataset(\"openai/webgpt_comparisons\", split=\"train[:20%]\")"
      ],
      "metadata": {
        "id": "JQU-3QqH65FE",
        "outputId": "22c5605e-4a45-4ec6-d5c6-8cc4f3c62554",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset webgpt_comparisons (/root/.cache/huggingface/datasets/openai___webgpt_comparisons/default/0.0.0/8b5d5879cdc98c4c0099af6053dffe8d504588d43d3b11f1b1ec223ab1e8db0a)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data into ranking format.\n",
        "data_list_ranking = []\n",
        "for row in dataset_webgpt_comp:\n",
        "    question = row[\"question\"][\"full_text\"]\n",
        "    answer_0 = row[\"answer_0\"]\n",
        "    answer_1 = row[\"answer_1\"]\n",
        "    score_0 = row[\"score_0\"]\n",
        "    score_1 = row[\"score_1\"]\n",
        "    if answer_0 == \"\" or answer_1 == \"\" or (score_0 == score_1):\n",
        "        continue\n",
        "\n",
        "    ranking = [0 if score_0 > score_1 else 1, 0 if score_0 < score_1 else 1]\n",
        "    data_list_ranking.append({\n",
        "        \"prompt\": PROMPT_TEMPLATE.format_map({\"prompt\": question}),\n",
        "        \"completion_0\": answer_0,\n",
        "        \"completion_1\": answer_1,\n",
        "        \"ranking\": ranking\n",
        "    })\n",
        "\n",
        "data_list_ranking[:2]"
      ],
      "metadata": {
        "id": "x2H18pYX7akp",
        "outputId": "83503c10-a443-4ced-adce-12d2b1b8fa8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'prompt': 'Below is an instruction that describes a task, paired with an input that provides further context.\\n\\nWrite a response that appropriately completes the request.\\n\\n### Instruction:\\nVoiced by Harry Shearer, what Simpsons character was modeled after Ted Koppel?\\n\\n### Response:',\n",
              "  'completion_0': 'The Simpsons character that was possibly based on Ted Koppel is Kent Brockman.  He is a local news anchor in Springfield and is modeled after Ted Koppel. [1]',\n",
              "  'completion_1': \"Apu Nahasapeemapetilon is a recurring character in the American animated television series The Simpsons. He is an Indian immigrant proprietor who runs the Kwik-E-Mart, a popular convenience store in Springfield. [1] He was based on Peter Seller's character in the film The Party. [2]\",\n",
              "  'ranking': [0, 1]},\n",
              " {'prompt': 'Below is an instruction that describes a task, paired with an input that provides further context.\\n\\nWrite a response that appropriately completes the request.\\n\\n### Instruction:\\nHeterophobia is the irrational fear of what\\n\\n### Response:',\n",
              "  'completion_0': ' Heterophobia is the irrational fear of the opposite sex, coined as Sexophobia [1]. This phobia can be caused by genetics, heredity, negative experiences with the opposite sex, or a combination of these [1].  Symptoms may result from encountering people of the opposite sex, including breathlessness, dizziness, excessive sweating, nausea, dry mouth, feeling sick, shaking, coronary heart palpitations, and anxiety [1].',\n",
              "  'completion_1': 'In modern times, there has been a rise in what is called heterophobia; the irrational fear of, discrimination against, or aversion to heterosexual people. [1][2] The word \"heterophobia\" is a play on the word \"homophobia,\" which describes the fear of homosexual people. [1] Like homophobia, heterophobia is promoted by those who wish to shame or bash heterosexuals, especially men who have sex with women. [2]',\n",
              "  'ranking': [0, 1]}]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make ranking data to chosen, rejetced data for reward model dataset.\n",
        "total_data_ranking2chosen = []\n",
        "for tmp in data_list_ranking:\n",
        "    one_data_ranking2chosen = []\n",
        "\n",
        "    # data 1) 0 VS 1\n",
        "    data = {}\n",
        "    data[\"prompt\"] = tmp[\"prompt\"]\n",
        "    if tmp[\"ranking\"][0] < tmp[\"ranking\"][1]:\n",
        "        data[\"chosen\"] = tmp[\"completion_0\"]\n",
        "        data[\"rejected\"] = tmp[\"completion_1\"]\n",
        "    else:\n",
        "        data[\"chosen\"] = tmp[\"completion_1\"]\n",
        "        data[\"rejected\"] = tmp[\"completion_0\"]\n",
        "    one_data_ranking2chosen.append(data)\n",
        "\n",
        "    # # data 2) 0 VS 2\n",
        "    # data = {}\n",
        "    # data[\"prompt\"] = tmp[\"prompt\"]\n",
        "    # if tmp[\"ranking\"][0] < tmp[\"ranking\"][2]:\n",
        "    #     data[\"chosen\"] = tmp[\"completion_0\"]\n",
        "    #     data[\"rejected\"] = tmp[\"completion_2\"]\n",
        "    # else:\n",
        "    #     data[\"chosen\"] = tmp[\"completion_2\"]\n",
        "    #     data[\"rejected\"] = tmp[\"completion_0\"]\n",
        "    # one_data_ranking2chosen.append(data)\n",
        "\n",
        "    # # data 1) 1 VS 2\n",
        "    # data = {}\n",
        "    # data[\"prompt\"] = tmp[\"prompt\"]\n",
        "    # if tmp[\"ranking\"][1] < tmp[\"ranking\"][2]:\n",
        "    #     data[\"chosen\"] = tmp[\"completion_1\"]\n",
        "    #     data[\"rejected\"] = tmp[\"completion_2\"]\n",
        "    # else:\n",
        "    #     data[\"chosen\"] = tmp[\"completion_2\"]\n",
        "    #     data[\"rejected\"] = tmp[\"completion_1\"]\n",
        "    # one_data_ranking2chosen.append(data)\n",
        "\n",
        "\n",
        "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
        "\n",
        "\n",
        "print(\"before data num: %d\" % (len(data_list_ranking)))\n",
        "print(\"after data num: %d\" % (len(total_data_ranking2chosen)))\n",
        "print(\"data example: \\n%s\" % total_data_ranking2chosen[1])"
      ],
      "metadata": {
        "id": "DeTW6fIE_c2L",
        "outputId": "b6b21f07-a307-4cfd-9d0a-3d1e8273650b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before data num: 2747\n",
            "after data num: 2747\n",
            "data example: \n",
            "{'prompt': 'Below is an instruction that describes a task, paired with an input that provides further context.\\n\\nWrite a response that appropriately completes the request.\\n\\n### Instruction:\\nHeterophobia is the irrational fear of what\\n\\n### Response:', 'chosen': ' Heterophobia is the irrational fear of the opposite sex, coined as Sexophobia [1]. This phobia can be caused by genetics, heredity, negative experiences with the opposite sex, or a combination of these [1].  Symptoms may result from encountering people of the opposite sex, including breathlessness, dizziness, excessive sweating, nausea, dry mouth, feeling sick, shaking, coronary heart palpitations, and anxiety [1].', 'rejected': 'In modern times, there has been a rise in what is called heterophobia; the irrational fear of, discrimination against, or aversion to heterosexual people. [1][2] The word \"heterophobia\" is a play on the word \"homophobia,\" which describes the fear of homosexual people. [1] Like homophobia, heterophobia is promoted by those who wish to shame or bash heterosexuals, especially men who have sex with women. [2]'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare for data and dataset.\n",
        "import random\n",
        "random.seed(230319)\n",
        "\n",
        "random.shuffle(total_data_ranking2chosen)\n",
        "print(total_data_ranking2chosen[1])\n",
        "\n",
        "# train_data = total_data_ranking2chosen[:-1000]\n",
        "# eval_data = total_data_ranking2chosen[-1000:0]\n",
        "# We just select very small set of data for a quicker training.\n",
        "train_data = total_data_ranking2chosen[:100]\n",
        "val_data = total_data_ranking2chosen[100:130]\n",
        "eval_data = total_data_ranking2chosen[130:160]\n",
        "\n",
        "train_dataset = RewardDataset(train_data, tokenizer, args.max_len)\n",
        "val_dataset = RewardDataset(val_data, tokenizer, args.max_len)\n",
        "eval_dataset = RewardDataset(eval_data, tokenizer, args.max_len)\n",
        "\n",
        "# Check\n",
        "idx = 10\n",
        "print(\"#\" * 70)\n",
        "print(\"## prompt ##\")\n",
        "print(train_data[idx][\"prompt\"])\n",
        "print(\"#\" * 70)\n",
        "print(\"## chosen ##\")\n",
        "print(train_data[idx][\"chosen\"])\n",
        "print(\"#\" * 70)\n",
        "print(\"## rejected ##\")\n",
        "print(train_data[idx][\"rejected\"])"
      ],
      "metadata": {
        "id": "o1FpHo5jAf81",
        "outputId": "1451c56d-4a2c-4350-d8ec-178d590f6eca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prompt': \"Below is an instruction that describes a task, paired with an input that provides further context.\\n\\nWrite a response that appropriately completes the request.\\n\\n### Instruction:\\nHow does one juice a prune? If prunes are just dehydrated plums, shouldn't prune juice just be plum juice?\\n\\n### Response:\", 'chosen': 'You can juice dried prunes by steaming or simmering them to rehydrate them, running them through a strainer to remove the pits, seeds and skin, and then adding more water to the resulting pruney paste. [1] You don’t have to do that, though, because you could also just juice a fresh prune. Contrary to popular belief, prunes aren’t simply dried plums, but a group of cultivars, or varieties, of plum that are well suited to drying. [2][3] ', 'rejected': 'While prunes are not simply dried plums, they are a type of dried plum. [2][3]  To juice a prune, you must first steam or simmer them to rehydrate them, and then run them through a strainer to remove the pits, seeds, and skin. [2][3]'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 403.87it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 434.78it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 434.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "######################################################################\n",
            "## prompt ##\n",
            "Below is an instruction that describes a task, paired with an input that provides further context.\n",
            "\n",
            "Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Why do major cell phone carriers allow companies like MetroPCS, Cricket Wireless, Boost Mobile, etc. to resell their network?\n",
            "\n",
            "And for a cheaper price, too? I don't understand.\n",
            "\n",
            "### Response:\n",
            "######################################################################\n",
            "## chosen ##\n",
            "These companies known as mobile virtual network operators or MVNOs are able to resell network services in bulk from a regular carrier and then resell them to end-users, usually for cheaper prices than that carrier [2,3]. That’s still profitable for MVNOs because they don’t have to pay anything for the upkeep and modernization of the wireless network they’re using, therefore they can afford to lower the rates on voice calls, messages and data in order to attract more customers [3]. Usually, they usually offer quite affordable pre-pay rates for these services and that’s definitely worth remembering [2]. As for regular carriers, without getting into too much detail, they’re still happy to make a profit from selling their services to MVNOs, even though they’re basically creating competitors with more customer-friendly offers – in some markets, governments require carriers to support MVNOs in order to create a competitive environment in the local mobile business [3]. \n",
            "######################################################################\n",
            "## rejected ##\n",
            "The big cell phone carriers allow smaller companies, called MVNOs (Mobile Virtual Network Operators), to resell their network [3]. They don't own the cell phone towers or the network but instead lease the network for their customers to use [3]. They are able to offer cheaper cell phone plans because they don't have the huge overhead and advertising costs that the big guys do [2]. They also have better customer service because they are smaller and less complex [1].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure optimizer.\n",
        "optim = Adam(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Configure loss function.\n",
        "if args.loss_fn == \"log_sig\":\n",
        "    loss_fn = LogSigLoss()\n",
        "elif args.loss_fn == \"log_exp\":\n",
        "    loss_fn = LogExpLoss()\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported loss function: {args.loss_fn}\")"
      ],
      "metadata": {
        "id": "JqCnhM_SXw9Z"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = RewardModelTrainer(model=model,\n",
        "                            strategy=strategy,\n",
        "                            optim=optim,\n",
        "                            loss_fn=loss_fn,\n",
        "                            train_dataset=train_dataset,\n",
        "                            valid_dataset=val_dataset,\n",
        "                            eval_dataset=eval_dataset,\n",
        "                            batch_size=args.batch_size,\n",
        "                            max_epochs=args.max_epochs)"
      ],
      "metadata": {
        "id": "tC6Sf3qMX2A-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train!!!\n",
        "trainer.fit()\n",
        "\n",
        "# Save model checkpoint after fitting on only rank0.\n",
        "# strategy.save_model(model, os.path.join(args.output_dir, \"rm.pt\"), only_rank0=True)\n",
        "trainer.save_model(path=os.path.join(args.output_dir, \"rm.pt\"), only_rank0=True)\n",
        "\n",
        "# Save optimizer checkpoint on all ranks.\n",
        "if args.need_optim_ckpt:\n",
        "    strategy.save_optimizer(trainer.optimizer,\n",
        "                            os.path.join(args.output_dir, \"rm_optim_checkpoint_%d.pt\" % (torch.cuda.current_device())),\n",
        "                            only_rank0=False)"
      ],
      "metadata": {
        "id": "WJO46OBraGOy",
        "outputId": "1db3e673-dc5b-41e4-bbe9-aeb1c0177805",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Train step of epoch 0:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Train step of epoch 0:   4%|▍         | 1/25 [00:00<00:04,  5.95it/s]\u001b[A\n",
            "Train step of epoch 0:   4%|▍         | 1/25 [00:00<00:04,  5.95it/s, loss=0.694, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:   8%|▊         | 2/25 [00:00<00:03,  6.51it/s, loss=0.694, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:   8%|▊         | 2/25 [00:00<00:03,  6.51it/s, loss=nan, dist=0, acc=0]  \u001b[A\n",
            "Train step of epoch 0:  12%|█▏        | 3/25 [00:00<00:03,  6.72it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  12%|█▏        | 3/25 [00:00<00:03,  6.72it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  16%|█▌        | 4/25 [00:00<00:03,  6.79it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  16%|█▌        | 4/25 [00:00<00:03,  6.79it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  20%|██        | 5/25 [00:00<00:02,  6.85it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  20%|██        | 5/25 [00:00<00:02,  6.85it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  24%|██▍       | 6/25 [00:00<00:02,  6.86it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  24%|██▍       | 6/25 [00:00<00:02,  6.86it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  28%|██▊       | 7/25 [00:01<00:02,  6.88it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  28%|██▊       | 7/25 [00:01<00:02,  6.88it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  32%|███▏      | 8/25 [00:01<00:02,  6.91it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  32%|███▏      | 8/25 [00:01<00:02,  6.91it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  36%|███▌      | 9/25 [00:01<00:02,  6.90it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  36%|███▌      | 9/25 [00:01<00:02,  6.90it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  40%|████      | 10/25 [00:01<00:02,  6.91it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  40%|████      | 10/25 [00:01<00:02,  6.91it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  44%|████▍     | 11/25 [00:01<00:02,  6.93it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  44%|████▍     | 11/25 [00:01<00:02,  6.93it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  48%|████▊     | 12/25 [00:01<00:01,  6.95it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  48%|████▊     | 12/25 [00:01<00:01,  6.95it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  52%|█████▏    | 13/25 [00:01<00:01,  6.96it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  52%|█████▏    | 13/25 [00:01<00:01,  6.96it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  56%|█████▌    | 14/25 [00:02<00:01,  6.96it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  56%|█████▌    | 14/25 [00:02<00:01,  6.96it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  60%|██████    | 15/25 [00:02<00:01,  6.96it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  60%|██████    | 15/25 [00:02<00:01,  6.96it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  64%|██████▍   | 16/25 [00:02<00:01,  6.96it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  64%|██████▍   | 16/25 [00:02<00:01,  6.96it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  68%|██████▊   | 17/25 [00:02<00:01,  6.97it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  68%|██████▊   | 17/25 [00:02<00:01,  6.97it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  72%|███████▏  | 18/25 [00:02<00:01,  6.96it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  72%|███████▏  | 18/25 [00:02<00:01,  6.96it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  76%|███████▌  | 19/25 [00:02<00:00,  6.94it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  76%|███████▌  | 19/25 [00:02<00:00,  6.94it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  80%|████████  | 20/25 [00:02<00:00,  6.92it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  80%|████████  | 20/25 [00:02<00:00,  6.92it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  84%|████████▍ | 21/25 [00:03<00:00,  6.93it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  84%|████████▍ | 21/25 [00:03<00:00,  6.93it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  88%|████████▊ | 22/25 [00:03<00:00,  6.93it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  88%|████████▊ | 22/25 [00:03<00:00,  6.93it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  92%|█████████▏| 23/25 [00:03<00:00,  6.93it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  92%|█████████▏| 23/25 [00:03<00:00,  6.93it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  96%|█████████▌| 24/25 [00:03<00:00,  6.92it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0:  96%|█████████▌| 24/25 [00:03<00:00,  6.92it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 0: 100%|██████████| 25/25 [00:03<00:00,  6.90it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train epoch:  33%|███▎      | 1/3 [00:04<00:08,  4.02s/it]\n",
            "Train step of epoch 0: 100%|██████████| 25/25 [00:04<00:00,  6.22it/s, dist=nan, acc=0]\n",
            "\n",
            "Train step of epoch 1:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Train step of epoch 1:   4%|▍         | 1/25 [00:00<00:03,  7.25it/s]\u001b[A\n",
            "Train step of epoch 1:   4%|▍         | 1/25 [00:00<00:03,  7.25it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:   8%|▊         | 2/25 [00:00<00:03,  7.05it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:   8%|▊         | 2/25 [00:00<00:03,  7.05it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  12%|█▏        | 3/25 [00:00<00:03,  6.94it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  12%|█▏        | 3/25 [00:00<00:03,  6.94it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  16%|█▌        | 4/25 [00:00<00:03,  6.89it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  16%|█▌        | 4/25 [00:00<00:03,  6.89it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  20%|██        | 5/25 [00:00<00:02,  6.88it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  20%|██        | 5/25 [00:00<00:02,  6.88it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  24%|██▍       | 6/25 [00:00<00:02,  6.87it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  24%|██▍       | 6/25 [00:00<00:02,  6.87it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  28%|██▊       | 7/25 [00:01<00:02,  6.87it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  28%|██▊       | 7/25 [00:01<00:02,  6.87it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  32%|███▏      | 8/25 [00:01<00:02,  6.90it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  32%|███▏      | 8/25 [00:01<00:02,  6.90it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  36%|███▌      | 9/25 [00:01<00:02,  6.91it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  36%|███▌      | 9/25 [00:01<00:02,  6.91it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  40%|████      | 10/25 [00:01<00:02,  6.93it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  40%|████      | 10/25 [00:01<00:02,  6.93it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  44%|████▍     | 11/25 [00:01<00:02,  6.94it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  44%|████▍     | 11/25 [00:01<00:02,  6.94it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  48%|████▊     | 12/25 [00:01<00:01,  6.92it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  48%|████▊     | 12/25 [00:01<00:01,  6.92it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  52%|█████▏    | 13/25 [00:01<00:01,  6.92it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  52%|█████▏    | 13/25 [00:01<00:01,  6.92it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  56%|█████▌    | 14/25 [00:02<00:01,  6.95it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  56%|█████▌    | 14/25 [00:02<00:01,  6.95it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  60%|██████    | 15/25 [00:02<00:01,  6.97it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  60%|██████    | 15/25 [00:02<00:01,  6.97it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  64%|██████▍   | 16/25 [00:02<00:01,  6.98it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  64%|██████▍   | 16/25 [00:02<00:01,  6.98it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  68%|██████▊   | 17/25 [00:02<00:01,  6.99it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  68%|██████▊   | 17/25 [00:02<00:01,  6.99it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  72%|███████▏  | 18/25 [00:02<00:01,  6.97it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  72%|███████▏  | 18/25 [00:02<00:01,  6.97it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  76%|███████▌  | 19/25 [00:02<00:00,  6.91it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  76%|███████▌  | 19/25 [00:02<00:00,  6.91it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  80%|████████  | 20/25 [00:02<00:00,  6.89it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  80%|████████  | 20/25 [00:02<00:00,  6.89it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  84%|████████▍ | 21/25 [00:03<00:00,  6.87it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  84%|████████▍ | 21/25 [00:03<00:00,  6.87it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  88%|████████▊ | 22/25 [00:03<00:00,  6.86it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  88%|████████▊ | 22/25 [00:03<00:00,  6.86it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  92%|█████████▏| 23/25 [00:03<00:00,  6.87it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  92%|█████████▏| 23/25 [00:03<00:00,  6.87it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  96%|█████████▌| 24/25 [00:03<00:00,  6.85it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1:  96%|█████████▌| 24/25 [00:03<00:00,  6.85it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 1: 100%|██████████| 25/25 [00:03<00:00,  6.84it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train epoch:  67%|██████▋   | 2/3 [00:08<00:04,  4.02s/it]\n",
            "Train step of epoch 1: 100%|██████████| 25/25 [00:04<00:00,  6.24it/s, dist=nan, acc=0]\n",
            "\n",
            "Train step of epoch 2:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "Train step of epoch 2:   4%|▍         | 1/25 [00:00<00:03,  7.06it/s]\u001b[A\n",
            "Train step of epoch 2:   4%|▍         | 1/25 [00:00<00:03,  7.06it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:   8%|▊         | 2/25 [00:00<00:03,  6.93it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:   8%|▊         | 2/25 [00:00<00:03,  6.93it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  12%|█▏        | 3/25 [00:00<00:03,  6.90it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  12%|█▏        | 3/25 [00:00<00:03,  6.90it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  16%|█▌        | 4/25 [00:00<00:03,  6.85it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  16%|█▌        | 4/25 [00:00<00:03,  6.85it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  20%|██        | 5/25 [00:00<00:02,  6.82it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  20%|██        | 5/25 [00:00<00:02,  6.82it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  24%|██▍       | 6/25 [00:00<00:02,  6.81it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  24%|██▍       | 6/25 [00:00<00:02,  6.81it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  28%|██▊       | 7/25 [00:01<00:02,  6.86it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  28%|██▊       | 7/25 [00:01<00:02,  6.86it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  32%|███▏      | 8/25 [00:01<00:02,  6.89it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  32%|███▏      | 8/25 [00:01<00:02,  6.89it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  36%|███▌      | 9/25 [00:01<00:02,  6.92it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  36%|███▌      | 9/25 [00:01<00:02,  6.92it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  40%|████      | 10/25 [00:01<00:02,  6.94it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  40%|████      | 10/25 [00:01<00:02,  6.94it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  44%|████▍     | 11/25 [00:01<00:02,  6.95it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  44%|████▍     | 11/25 [00:01<00:02,  6.95it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  48%|████▊     | 12/25 [00:01<00:01,  6.92it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  48%|████▊     | 12/25 [00:01<00:01,  6.92it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  52%|█████▏    | 13/25 [00:01<00:01,  6.88it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  52%|█████▏    | 13/25 [00:01<00:01,  6.88it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  56%|█████▌    | 14/25 [00:02<00:01,  6.84it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  56%|█████▌    | 14/25 [00:02<00:01,  6.84it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  60%|██████    | 15/25 [00:02<00:01,  6.80it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  60%|██████    | 15/25 [00:02<00:01,  6.80it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  64%|██████▍   | 16/25 [00:02<00:01,  6.77it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  64%|██████▍   | 16/25 [00:02<00:01,  6.77it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  68%|██████▊   | 17/25 [00:02<00:01,  6.76it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  68%|██████▊   | 17/25 [00:02<00:01,  6.76it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  72%|███████▏  | 18/25 [00:02<00:01,  6.75it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  72%|███████▏  | 18/25 [00:02<00:01,  6.75it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  76%|███████▌  | 19/25 [00:02<00:00,  6.75it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  76%|███████▌  | 19/25 [00:02<00:00,  6.75it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  80%|████████  | 20/25 [00:02<00:00,  6.75it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  80%|████████  | 20/25 [00:02<00:00,  6.75it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  84%|████████▍ | 21/25 [00:03<00:00,  6.73it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  84%|████████▍ | 21/25 [00:03<00:00,  6.73it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  88%|████████▊ | 22/25 [00:03<00:00,  6.76it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  88%|████████▊ | 22/25 [00:03<00:00,  6.76it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  92%|█████████▏| 23/25 [00:03<00:00,  6.78it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  92%|█████████▏| 23/25 [00:03<00:00,  6.78it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  96%|█████████▌| 24/25 [00:03<00:00,  6.79it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2:  96%|█████████▌| 24/25 [00:03<00:00,  6.79it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train step of epoch 2: 100%|██████████| 25/25 [00:03<00:00,  6.80it/s, loss=nan, dist=0, acc=0]\u001b[A\n",
            "Train epoch: 100%|██████████| 3/3 [00:12<00:00,  4.04s/it]\n",
            "Train step of epoch 2: 100%|██████████| 25/25 [00:04<00:00,  6.16it/s, dist=nan, acc=0]\n",
            "Train epoch: 100%|██████████| 3/3 [00:12<00:00,  4.04s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3) PPO: Proximal Policy Optimization\n",
        "Further fine-tune the LLM from step 1 with the reward model and this dataset using RL (eg. PPO).\n",
        "\n",
        "- References\n",
        "    - [train_prompts.py](https://github.com/hpcaitech/ColossalAI/blob/main/applications/Chat/examples/train_prompts.py)"
      ],
      "metadata": {
        "id": "EI7qm5N262EZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "from copy import deepcopy\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from transformers import AutoTokenizer, BloomTokenizerFast\n",
        "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
        "\n",
        "from nextgpt.rlhf.models.base import RewardModel\n",
        "from nextgpt.rlhf.models.bloom import BLOOMActor, BLOOMCritic\n",
        "from nextgpt.rlhf.models.gpt import GPTActor, GPTCritic\n",
        "from nextgpt.rlhf.models.opt import OPTActor, OPTCritic\n",
        "from nextgpt.rlhf.trainer import PPOTrainer\n",
        "from nextgpt.rlhf.trainer.strategies import DDPStrategy, NaiveStrategy\n",
        "\n",
        "import json\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
      ],
      "metadata": {
        "id": "0fiD6OSKpOrc"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define arguments.\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument(\"--output_dir\", type=str, default=\"./output_3_ppo\")\n",
        "parser.add_argument(\"--strategy\",\n",
        "                    choices=[\"naive\", \"ddp\"],\n",
        "                    default=\"naive\")\n",
        "parser.add_argument(\"--model\", type=str, default=\"gpt2\", choices=[\"gpt2\", \"bloom\", \"opt\"])\n",
        "parser.add_argument(\"--pretrain_actor\", type=str, default=None)\n",
        "parser.add_argument(\"--pretrain_critic\", type=str, default=None)\n",
        "parser.add_argument(\"--num_episodes\", type=int, default=10)\n",
        "parser.add_argument(\"--max_timesteps\", type=int, default=3)\n",
        "parser.add_argument(\"--update_timesteps\", type=int, default=3)\n",
        "parser.add_argument(\"--max_epochs\", type=int, default=5)\n",
        "parser.add_argument(\"--train_batch_size\", type=int, default=8)\n",
        "parser.add_argument(\"--lora_rank\", type=int, default=0, help=\"low-rank adaptation matrices rank\")\n",
        "parser.add_argument(\"--max_len\", type=int, default=250)\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "# For test\n",
        "args.pretrain_actor = \"./output_1_sft\" # SFT model\n",
        "args.pretrain_critic = \"./output_2_rm/rm.pt\" # RM model\n",
        "\n",
        "args.num_episodes = 1\n",
        "args.max_epochs   = 1\n",
        "\n",
        "print(args)\n",
        "if not os.path.exists(args.output_dir):\n",
        "    os.makedirs(args.output_dir)"
      ],
      "metadata": {
        "id": "VIs7m2m9pmWN",
        "outputId": "128129a5-e7b2-4476-de00-5e3f32a21310",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(output_dir='./output_3_ppo', strategy='naive', model='gpt2', pretrain_actor='./output_1_sft', pretrain_critic='./output_2_rm/rm.pt', num_episodes=1, max_timesteps=3, update_timesteps=3, max_epochs=1, train_batch_size=8, lora_rank=0, max_len=250)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure strategy.\n",
        "if args.strategy == \"naive\":\n",
        "    strategy = NaiveStrategy()\n",
        "elif args.strategy == \"ddp\":\n",
        "    strategy = DDPStrategy()\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported strategy: {args.strategy}\")"
      ],
      "metadata": {
        "id": "PyHx9jXoq0yi"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure model, tokenizer.\n",
        "with strategy.model_init_context():\n",
        "    if args.model == \"gpt2\":\n",
        "        actor = GPTActor(pretrained=args.pretrain_actor, lora_rank=args.lora_rank)\n",
        "        critic = GPTCritic(pretrained=args.pretrain_critic, lora_rank=args.lora_rank)\n",
        "        # Get the tokenizer.\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            args.model, \n",
        "            bos_token=\"<|startoftext|>\",\n",
        "            eos_token=\"<|endoftext|>\", \n",
        "            pad_token=\"<|pad|>\",\n",
        "            padding_side=\"right\", \n",
        "            model_max_length=args.max_len,\n",
        "            )\n",
        "        # tokenizer.pad_token = tokenizer.eos_token\n",
        "    elif args.model == \"bloom\":\n",
        "        actor = BLOOMActor(pretrained=args.pretrain_actor, lora_rank=args.lora_rank)\n",
        "        critic = BLOOMCritic(pretrained=args.pretrain_critic, lora_rank=args.lora_rank)\n",
        "        tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-560m\")\n",
        "        tokenizer.pad_token = tokenizer.eos_token            \n",
        "    elif args.model == \"opt\":\n",
        "        actor = OPTActor(pretrained=args.pretrain_actor, lora_rank=args.lora_rank)\n",
        "        critic = OPTCritic(pretrained=args.pretrain_critic, lora_rank=args.lora_rank)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")            \n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported model: {args.model}\")\n",
        "\n",
        "    critic.to(torch.float16).to(torch.cuda.current_device())\n",
        "    actor.to(torch.float16).to(torch.cuda.current_device())\n",
        "\n",
        "    initial_model = deepcopy(actor)\n",
        "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())"
      ],
      "metadata": {
        "id": "pH1wPr4Nq-bM",
        "outputId": "2003afa5-fef0-44bd-e311-1773f77454a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0;31m# Load config dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m             \u001b[0mconfig_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dict_from_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_config_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m             \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommit_hash\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_dict_from_json_file\u001b[0;34m(cls, json_file)\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 64: invalid start byte",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-a71173a73ae2>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"gpt2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mactor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPTActor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrain_actor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlora_rank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlora_rank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mcritic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPTCritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrain_critic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlora_rank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlora_rank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;31m# Get the tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         tokenizer = AutoTokenizer.from_pretrained(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nextgpt/rlhf/models/gpt/gpt_critic.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, pretrained, config, checkpoint, lora_rank, lora_train_bias)\u001b[0m\n\u001b[1;32m     27\u001b[0m                  lora_train_bias: str = 'none') -> None:\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpretrained\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2Model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2174\u001b[0m             \u001b[0mconfig_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2175\u001b[0;31m             config, model_kwargs = cls.config_class.from_pretrained(\n\u001b[0m\u001b[1;32m   2176\u001b[0m                 \u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2177\u001b[0m                 \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"foo\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         ```\"\"\"\n\u001b[0;32m--> 546\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             logger.warning(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0moriginal_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    659\u001b[0m             \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommit_hash\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnicodeDecodeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m             raise EnvironmentError(\n\u001b[0m\u001b[1;32m    662\u001b[0m                 \u001b[0;34mf\"It looks like the config file at '{resolved_config_file}' is not a valid JSON file.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             )\n",
            "\u001b[0;31mOSError\u001b[0m: It looks like the config file at './output_2_rm/rm.pt' is not a valid JSON file."
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}